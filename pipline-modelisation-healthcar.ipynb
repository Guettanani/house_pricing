{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:06:02.210216Z","iopub.execute_input":"2025-06-21T18:06:02.210436Z","iopub.status.idle":"2025-06-21T18:06:04.611781Z","shell.execute_reply.started":"2025-06-21T18:06:02.210415Z","shell.execute_reply":"2025-06-21T18:06:04.610743Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# 📦 INSTALLATION ET IMPORTS\n# =============================================================================\n\n# Installation des dépendances critiques avec versions fixes\n# Dans un environnement Kaggle/Colab, la cellule suivante doit être exécutée avec '%%capture'\n# =============================================================================\n# ✅ Installation des dépendances avec versions stables et compatibles\n# =============================================================================\n\n%pip install -q numpy==1.24.3 pandas==2.0.3 scikit-learn==1.2.2\n%pip install -q xgboost==1.7.6 lightgbm==4.0.0 catboost==1.2\n%pip install -q shap==0.42.1 imbalanced-learn==0.10.1\n%pip install -q optuna==3.3.0 plotly==5.15.0 seaborn==0.12.2\n%pip install -q evidently==0.4.2 pydantic==1.10.11\n%pip install -q mlflow==2.5.0 feature-engine==1.6.2 joblib\n%pip install -q pyarrow==14.0.1\n\n\n\n# =============================================================================\n# ✅ Installation des dépendances avec versions stables et compatibles\n# =============================================================================\n\n%pip install -q numpy==1.24.3 pandas==2.0.3 scikit-learn==1.2.2\n%pip install -q xgboost==1.7.6 lightgbm==4.0.0 catboost==1.2\n%pip install -q shap==0.42.1 imbalanced-learn==0.10.1\n%pip install -q optuna==3.3.0 plotly==5.15.0 seaborn==0.12.2\n%pip install -q evidently==0.4.2 pydantic==1.10.11\n%pip install -q mlflow==2.5.0 feature-engine==1.6.2 joblib\n%pip install -q pyarrow==14.0.1\n\n\n\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n\nprint(\"✅ Tout est importé avec succès !\")\n\n# Imports essentiels\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport joblib\n\n# ML Core\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold, cross_val_score\n)\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score,\n    precision_recall_curve, average_precision_score,\n    f1_score, fbeta_score, matthews_corrcoef, roc_curve\n)\nfrom sklearn.preprocessing import (\n    StandardScaler, RobustScaler, LabelEncoder,\n    QuantileTransformer, PowerTransformer\n)\nfrom sklearn.feature_selection import (\n    SelectKBest, f_classif, VarianceThreshold,\n    mutual_info_classif\n)\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier,\n    StackingClassifier, IsolationForest\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# ML Spécialisé\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.combine import SMOTEENN\nimport shap\nimport optuna\n\n# Validation et monitoring\nfrom pydantic import BaseModel, validator\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n\n# Configuration globale\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# Seed pour reproductibilité\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"✅ Environnement configuré avec succès!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:06:04.614065Z","iopub.execute_input":"2025-06-21T18:06:04.614479Z","iopub.status.idle":"2025-06-21T18:08:55.748932Z","shell.execute_reply.started":"2025-06-21T18:06:04.614453Z","shell.execute_reply":"2025-06-21T18:08:55.747540Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\nmizani 0.13.2 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\nxarray 2025.1.2 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.0/548.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsigstore 3.6.2 requires pydantic<3,>=2, but you have pydantic 1.10.11 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.11 which is incompatible.\nydata-profiling 4.16.1 requires pydantic>=2, but you have pydantic 1.10.11 which is incompatible.\nlangchain-core 0.3.50 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\nlangchain-core 0.3.50 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.11 which is incompatible.\nlangchain 0.3.22 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.11 which is incompatible.\ngoogle-genai 1.9.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.11 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\nalbumentations 2.0.5 requires pydantic>=2.9.2, but you have pydantic 1.10.11 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\ndistributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\ndask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 12.0.1 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 12.0.1 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 12.0.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 12.0.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.16.1 requires pydantic>=2, but you have pydantic 1.10.11 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\nmizani 0.13.2 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nlangchain-core 0.3.50 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.11 which is incompatible.\nbigframes 1.42.0 requires pyarrow>=15.0.2, but you have pyarrow 12.0.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nlangchain 0.3.22 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.11 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\nxarray 2025.1.2 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlflow 2.5.0 requires pyarrow<13,>=4.0.0, but you have pyarrow 14.0.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\nbigframes 1.42.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 12.0.1 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 12.0.1 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 12.0.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 12.0.1 which is incompatible.\nbigframes 1.42.0 requires pyarrow>=15.0.2, but you have pyarrow 12.0.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlflow 2.5.0 requires pyarrow<13,>=4.0.0, but you have pyarrow 14.0.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\nbigframes 1.42.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n✅ Tout est importé avec succès !\n","output_type":"stream"},{"name":"stderr","text":"<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.NativeFile size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n","output_type":"stream"},{"name":"stdout","text":"✅ Environnement configuré avec succès!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom pydantic import BaseModel, validator\n# =============================================================================\n# 🏗️ ARCHITECTURE DE DONNÉES ET VALIDATION\n# =============================================================================\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration centralisée du modèle\"\"\"\n    random_state: int = 42\n    test_size: float = 0.2\n    cv_folds: int = 5\n    optuna_trials: int = 30  # Ajusté pour une exécution rapide\n    model_version: str = \"2.0.0\"\n\nclass HealthDataValidator(BaseModel):\n    \"\"\"Validateur Pydantic pour les données de santé\"\"\"\n    patient_id: str\n    age: int\n    heart_rate: float\n    systolic_bp: float\n    diastolic_bp: float\n    temperature: float\n    activity_level: float\n\n    @validator('age')\n    def validate_age(cls, v):\n        if not 60 <= v <= 120:\n            raise ValueError('Âge doit être entre 60 et 120 ans')\n        return v\n\n    @validator('heart_rate')\n    def validate_heart_rate(cls, v):\n        if not 40 <= v <= 180:\n            raise ValueError('Fréquence cardiaque non physiologique')\n        return v\n\n    @validator('systolic_bp', 'diastolic_bp')\n    def validate_blood_pressure(cls, v, field):\n        if field.name == 'systolic_bp' and not 80 <= v <= 220:\n            raise ValueError('Pression systolique anormale')\n        if field.name == 'diastolic_bp' and not 40 <= v <= 140:\n            raise ValueError('Pression diastolique anormale')\n        return v\n\nconfig = ModelConfig()\nprint(f\"📋 Configuration du modèle v{config.model_version} initialisée\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:08:55.750060Z","iopub.execute_input":"2025-06-21T18:08:55.750843Z","iopub.status.idle":"2025-06-21T18:08:55.764205Z","shell.execute_reply.started":"2025-06-21T18:08:55.750807Z","shell.execute_reply":"2025-06-21T18:08:55.763268Z"}},"outputs":[{"name":"stdout","text":"📋 Configuration du modèle v2.0.0 initialisée\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# 📊 GÉNÉRATION DE DONNÉES SYNTHÉTIQUES RÉALISTES\n# =============================================================================\n\ndef generate_synthetic_frailty_data(n_samples: int = 5000) -> pd.DataFrame:\n    \"\"\"Génère des données synthétiques réalistes pour la détection de fragilité\"\"\"\n    np.random.seed(RANDOM_STATE)\n    \n    patient_ids = [f\"P_{i:05d}\" for i in range(1, n_samples + 1)]\n    ages = np.random.normal(75, 8, n_samples).clip(65, 95).astype(int)\n    genders = np.random.choice(['M', 'F'], n_samples, p=[0.45, 0.55])\n    \n    base_heart_rate = np.random.normal(72, 12, n_samples)\n    heart_rate_variability = np.random.exponential(25, n_samples)\n    \n    age_factor = (ages - 65) / 30\n    systolic_bp = 120 + age_factor * 30 + np.random.normal(0, 15, n_samples)\n    diastolic_bp = 80 + age_factor * 10 + np.random.normal(0, 10, n_samples)\n    \n    temperature = np.random.normal(36.6, 0.4, n_samples).clip(35.5, 38.5)\n    activity_base = np.random.gamma(2, 2, n_samples)\n    mobility_score = np.random.beta(2, 3, n_samples) * 10\n    bmi = np.random.normal(26, 4, n_samples).clip(18, 40)\n    grip_strength = 35 - (ages - 65) * 0.5 + np.random.normal(0, 8, n_samples)\n    grip_strength = grip_strength.clip(10, 50)\n    \n    diabetes = np.random.binomial(1, 0.25, n_samples)\n    hypertension = np.random.binomial(1, 0.4, n_samples)\n    heart_disease = np.random.binomial(1, 0.2, n_samples)\n    \n    smoking_history = np.random.choice([0, 1, 2], n_samples, p=[0.6, 0.3, 0.1])\n    alcohol_consumption = np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.4, 0.2, 0.1])\n    \n    timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=n_samples, freq='H'))\n    pulse_pressure = systolic_bp - diastolic_bp\n    mean_arterial_pressure = diastolic_bp + pulse_pressure / 3\n    \n    frailty_score = (\n        0.3 * (ages > 80).astype(int) +\n        0.2 * (grip_strength < 20).astype(int) +\n        0.2 * (mobility_score < 3).astype(int) +\n        0.1 * (bmi < 20).astype(int) +\n        0.1 * diabetes +\n        0.1 * heart_disease +\n        np.random.normal(0, 0.15, n_samples)\n    )\n    is_frail = (frailty_score > 0.6).astype(int)\n    \n    data = pd.DataFrame({\n        'patient_id': patient_ids, 'timestamp': timestamps, 'age': ages, 'gender': genders,\n        'heart_rate': base_heart_rate.clip(45, 120), 'heart_rate_variability': heart_rate_variability.clip(5, 100),\n        'systolic_bp': systolic_bp.clip(90, 200), 'diastolic_bp': diastolic_bp.clip(50, 110),\n        'pulse_pressure': pulse_pressure, 'mean_arterial_pressure': mean_arterial_pressure,\n        'temperature': temperature, 'bmi': bmi, 'grip_strength': grip_strength, 'mobility_score': mobility_score,\n        'activity_level': activity_base.clip(0, 10), 'diabetes': diabetes, 'hypertension': hypertension,\n        'heart_disease': heart_disease, 'smoking_history': smoking_history, 'alcohol_consumption': alcohol_consumption,\n        'frailty_score': frailty_score.clip(0, 1), 'is_frail': is_frail\n    })\n    \n    missing_cols = ['heart_rate_variability', 'grip_strength', 'activity_level']\n    for col in missing_cols:\n        missing_mask = np.random.random(n_samples) < 0.07\n        data.loc[missing_mask, col] = np.nan\n    \n    return data\n\nprint(\"🔄 Génération des données synthétiques...\")\ndf = generate_synthetic_frailty_data(n_samples=8000)\nprint(f\"📊 Dataset généré: {df.shape[0]} patients, {df.shape[1]} variables\")\nprint(f\"🎯 Distribution de la cible: {df['is_frail'].value_counts().to_dict()}\")\nprint(f\"📈 Pourcentage de fragilité: {df['is_frail'].mean():.1%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:08:55.765424Z","iopub.execute_input":"2025-06-21T18:08:55.765780Z","iopub.status.idle":"2025-06-21T18:08:55.830860Z","shell.execute_reply.started":"2025-06-21T18:08:55.765727Z","shell.execute_reply":"2025-06-21T18:08:55.829882Z"}},"outputs":[{"name":"stdout","text":"🔄 Génération des données synthétiques...\n📊 Dataset généré: 8000 patients, 22 variables\n🎯 Distribution de la cible: {0: 7424, 1: 576}\n📈 Pourcentage de fragilité: 7.2%\n","output_type":"stream"},{"name":"stderr","text":"'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# a supprimer peut etre\n\ndef generate_synthetic_data():\n    from sklearn.datasets import make_classification\n    print(\"📊 Génération de données synthiques...\")\n    X, y = make_classification(n_samples=300, n_features=8, random_state=42)\n    df = pd.DataFrame(X, columns=[f\"feat_{i}\" for i in range(8)])\n\n    # Ajout de colonnes biomédicales simulées\n    df['timestamp'] = pd.date_range(start='2023-01-01', periods=300, freq='H')\n    df['bmi'] = np.random.uniform(18, 35, size=300)\n    df['age'] = np.random.randint(20, 70, size=300)\n    df['pulse_pressure'] = np.random.uniform(30, 60, size=300)\n    df['systolic_bp'] = np.random.uniform(110, 140, size=300)\n    df['diabetes'] = np.random.randint(0, 2, size=300)\n    df['hypertension'] = np.random.randint(0, 2, size=300)\n    df['heart_disease'] = np.random.randint(0, 2, size=300)\n    df['gender'] = np.random.choice(['M', 'F'], size=300)\n    \n    print(\"✅ Données synthétiques créées avec succès\")\n    return df, y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        print(\"🛠️ Fit du FeatureEngineer - aucune opération nécessaire ici.\")\n        return self\n\n    def transform(self, X):\n        print(\"🛠️ Transformation : ajout de nouvelles variables explicatives...\")\n        df = X.copy()\n        df['hour'] = df['timestamp'].dt.hour\n        df['day_of_week'] = df['timestamp'].dt.dayofweek\n        df['bmi_age_ratio'] = df['bmi'] / (df['age'] / 70)\n        df['pulse_pressure_norm'] = df['pulse_pressure'] / df['systolic_bp']\n        df['total_comorbidities'] = df[['diabetes', 'hypertension', 'heart_disease']].sum(axis=1)\n        print(\"✅ 5 nouvelles variables créées.\")\n        return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MedicalDataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\"🧹 Initialisation de l’imputeur avec ExtraTreesClassifier\")\n        self.imputer = IterativeImputer(\n            estimator=ExtraTreesClassifier(n_estimators=10, random_state=42),\n            max_iter=5,\n            random_state=42\n        )\n        self.numeric_cols = None\n\n    def fit(self, X, y=None):\n        print(\"🧹 Fit de l’imputeur sur les colonnes numériques...\")\n        self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n        self.imputer.fit(X[self.numeric_cols])\n        print(f\"✅ Colonnes numériques détectées : {self.numeric_cols}\")\n        return self\n\n    def transform(self, X):\n        print(\"🧹 Application de l’imputation et encodage binaire du genre...\")\n        df = X.copy()\n        df[self.numeric_cols] = self.imputer.transform(df[self.numeric_cols])\n        if 'gender' in df.columns:\n            df['gender'] = df['gender'].apply(lambda x: 1 if x == 'M' else 0)\n        print(\"✅ Nettoyage terminé.\")\n        return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}