{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:06:02.210216Z","iopub.execute_input":"2025-06-21T18:06:02.210436Z","iopub.status.idle":"2025-06-21T18:06:04.611781Z","shell.execute_reply.started":"2025-06-21T18:06:02.210415Z","shell.execute_reply":"2025-06-21T18:06:04.610743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üì¶ INSTALLATION ET IMPORTS\n# =============================================================================\n\n# Installation des d√©pendances critiques avec versions fixes\n# Dans un environnement Kaggle/Colab, la cellule suivante doit √™tre ex√©cut√©e avec '%%capture'\n# =============================================================================\n# ‚úÖ Installation des d√©pendances avec versions stables et compatibles\n# =============================================================================\n\n%pip install -q numpy==1.24.3 pandas==2.0.3 scikit-learn==1.2.2\n%pip install -q xgboost==1.7.6 lightgbm==4.0.0 catboost==1.2\n%pip install -q shap==0.42.1 imbalanced-learn==0.10.1\n%pip install -q optuna==3.3.0 plotly==5.15.0 seaborn==0.12.2\n%pip install -q evidently==0.4.2 pydantic==1.10.11\n%pip install -q mlflow==2.5.0 feature-engine==1.6.2 joblib\n%pip install -q pyarrow==14.0.1\n\n\n\n# =============================================================================\n# ‚úÖ Installation des d√©pendances avec versions stables et compatibles\n# =============================================================================\n\n%pip install -q numpy==1.24.3 pandas==2.0.3 scikit-learn==1.2.2\n%pip install -q xgboost==1.7.6 lightgbm==4.0.0 catboost==1.2\n%pip install -q shap==0.42.1 imbalanced-learn==0.10.1\n%pip install -q optuna==3.3.0 plotly==5.15.0 seaborn==0.12.2\n%pip install -q evidently==0.4.2 pydantic==1.10.11\n%pip install -q mlflow==2.5.0 feature-engine==1.6.2 joblib\n%pip install -q pyarrow==14.0.1\n\n\n\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n\nprint(\"‚úÖ Tout est import√© avec succ√®s !\")\n\n# Imports essentiels\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport joblib\n\n# ML Core\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold, cross_val_score\n)\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score,\n    precision_recall_curve, average_precision_score,\n    f1_score, fbeta_score, matthews_corrcoef, roc_curve\n)\nfrom sklearn.preprocessing import (\n    StandardScaler, RobustScaler, LabelEncoder,\n    QuantileTransformer, PowerTransformer\n)\nfrom sklearn.feature_selection import (\n    SelectKBest, f_classif, VarianceThreshold,\n    mutual_info_classif\n)\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier,\n    StackingClassifier, IsolationForest\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# ML Sp√©cialis√©\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.combine import SMOTEENN\nimport shap\nimport optuna\n\n# Validation et monitoring\nfrom pydantic import BaseModel, validator\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n\n# Configuration globale\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# Seed pour reproductibilit√©\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"‚úÖ Environnement configur√© avec succ√®s!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:06:04.614065Z","iopub.execute_input":"2025-06-21T18:06:04.614479Z","iopub.status.idle":"2025-06-21T18:08:55.748932Z","shell.execute_reply.started":"2025-06-21T18:06:04.614453Z","shell.execute_reply":"2025-06-21T18:08:55.747540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom pydantic import BaseModel, validator\n# =============================================================================\n# üèóÔ∏è ARCHITECTURE DE DONN√âES ET VALIDATION\n# =============================================================================\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration centralis√©e du mod√®le\"\"\"\n    random_state: int = 42\n    test_size: float = 0.2\n    cv_folds: int = 5\n    optuna_trials: int = 30  # Ajust√© pour une ex√©cution rapide\n    model_version: str = \"2.0.0\"\n\nclass HealthDataValidator(BaseModel):\n    \"\"\"Validateur Pydantic pour les donn√©es de sant√©\"\"\"\n    patient_id: str\n    age: int\n    heart_rate: float\n    systolic_bp: float\n    diastolic_bp: float\n    temperature: float\n    activity_level: float\n\n    @validator('age')\n    def validate_age(cls, v):\n        if not 60 <= v <= 120:\n            raise ValueError('√Çge doit √™tre entre 60 et 120 ans')\n        return v\n\n    @validator('heart_rate')\n    def validate_heart_rate(cls, v):\n        if not 40 <= v <= 180:\n            raise ValueError('Fr√©quence cardiaque non physiologique')\n        return v\n\n    @validator('systolic_bp', 'diastolic_bp')\n    def validate_blood_pressure(cls, v, field):\n        if field.name == 'systolic_bp' and not 80 <= v <= 220:\n            raise ValueError('Pression systolique anormale')\n        if field.name == 'diastolic_bp' and not 40 <= v <= 140:\n            raise ValueError('Pression diastolique anormale')\n        return v\n\nconfig = ModelConfig()\nprint(f\"üìã Configuration du mod√®le v{config.model_version} initialis√©e\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:08:55.750060Z","iopub.execute_input":"2025-06-21T18:08:55.750843Z","iopub.status.idle":"2025-06-21T18:08:55.764205Z","shell.execute_reply.started":"2025-06-21T18:08:55.750807Z","shell.execute_reply":"2025-06-21T18:08:55.763268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üìä G√âN√âRATION DE DONN√âES SYNTH√âTIQUES R√âALISTES\n# =============================================================================\n\ndef generate_synthetic_frailty_data(n_samples: int = 5000) -> pd.DataFrame:\n    \"\"\"G√©n√®re des donn√©es synth√©tiques r√©alistes pour la d√©tection de fragilit√©\"\"\"\n    np.random.seed(RANDOM_STATE)\n    \n    patient_ids = [f\"P_{i:05d}\" for i in range(1, n_samples + 1)]\n    ages = np.random.normal(75, 8, n_samples).clip(65, 95).astype(int)\n    genders = np.random.choice(['M', 'F'], n_samples, p=[0.45, 0.55])\n    \n    base_heart_rate = np.random.normal(72, 12, n_samples)\n    heart_rate_variability = np.random.exponential(25, n_samples)\n    \n    age_factor = (ages - 65) / 30\n    systolic_bp = 120 + age_factor * 30 + np.random.normal(0, 15, n_samples)\n    diastolic_bp = 80 + age_factor * 10 + np.random.normal(0, 10, n_samples)\n    \n    temperature = np.random.normal(36.6, 0.4, n_samples).clip(35.5, 38.5)\n    activity_base = np.random.gamma(2, 2, n_samples)\n    mobility_score = np.random.beta(2, 3, n_samples) * 10\n    bmi = np.random.normal(26, 4, n_samples).clip(18, 40)\n    grip_strength = 35 - (ages - 65) * 0.5 + np.random.normal(0, 8, n_samples)\n    grip_strength = grip_strength.clip(10, 50)\n    \n    diabetes = np.random.binomial(1, 0.25, n_samples)\n    hypertension = np.random.binomial(1, 0.4, n_samples)\n    heart_disease = np.random.binomial(1, 0.2, n_samples)\n    \n    smoking_history = np.random.choice([0, 1, 2], n_samples, p=[0.6, 0.3, 0.1])\n    alcohol_consumption = np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.4, 0.2, 0.1])\n    \n    timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=n_samples, freq='H'))\n    pulse_pressure = systolic_bp - diastolic_bp\n    mean_arterial_pressure = diastolic_bp + pulse_pressure / 3\n    \n    frailty_score = (\n        0.3 * (ages > 80).astype(int) +\n        0.2 * (grip_strength < 20).astype(int) +\n        0.2 * (mobility_score < 3).astype(int) +\n        0.1 * (bmi < 20).astype(int) +\n        0.1 * diabetes +\n        0.1 * heart_disease +\n        np.random.normal(0, 0.15, n_samples)\n    )\n    is_frail = (frailty_score > 0.6).astype(int)\n    \n    data = pd.DataFrame({\n        'patient_id': patient_ids, 'timestamp': timestamps, 'age': ages, 'gender': genders,\n        'heart_rate': base_heart_rate.clip(45, 120), 'heart_rate_variability': heart_rate_variability.clip(5, 100),\n        'systolic_bp': systolic_bp.clip(90, 200), 'diastolic_bp': diastolic_bp.clip(50, 110),\n        'pulse_pressure': pulse_pressure, 'mean_arterial_pressure': mean_arterial_pressure,\n        'temperature': temperature, 'bmi': bmi, 'grip_strength': grip_strength, 'mobility_score': mobility_score,\n        'activity_level': activity_base.clip(0, 10), 'diabetes': diabetes, 'hypertension': hypertension,\n        'heart_disease': heart_disease, 'smoking_history': smoking_history, 'alcohol_consumption': alcohol_consumption,\n        'frailty_score': frailty_score.clip(0, 1), 'is_frail': is_frail\n    })\n    \n    missing_cols = ['heart_rate_variability', 'grip_strength', 'activity_level']\n    for col in missing_cols:\n        missing_mask = np.random.random(n_samples) < 0.07\n        data.loc[missing_mask, col] = np.nan\n    \n    return data\n\nprint(\"üîÑ G√©n√©ration des donn√©es synth√©tiques...\")\ndf = generate_synthetic_frailty_data(n_samples=8000)\nprint(f\"üìä Dataset g√©n√©r√©: {df.shape[0]} patients, {df.shape[1]} variables\")\nprint(f\"üéØ Distribution de la cible: {df['is_frail'].value_counts().to_dict()}\")\nprint(f\"üìà Pourcentage de fragilit√©: {df['is_frail'].mean():.1%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:08:55.765424Z","iopub.execute_input":"2025-06-21T18:08:55.765780Z","iopub.status.idle":"2025-06-21T18:08:55.830860Z","shell.execute_reply.started":"2025-06-21T18:08:55.765727Z","shell.execute_reply":"2025-06-21T18:08:55.829882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# a supprimer peut etre\n\ndef generate_synthetic_data():\n    from sklearn.datasets import make_classification\n    print(\"üìä G√©n√©ration de donn√©es synthiques...\")\n    X, y = make_classification(n_samples=300, n_features=8, random_state=42)\n    df = pd.DataFrame(X, columns=[f\"feat_{i}\" for i in range(8)])\n\n    # Ajout de colonnes biom√©dicales simul√©es\n    df['timestamp'] = pd.date_range(start='2023-01-01', periods=300, freq='H')\n    df['bmi'] = np.random.uniform(18, 35, size=300)\n    df['age'] = np.random.randint(20, 70, size=300)\n    df['pulse_pressure'] = np.random.uniform(30, 60, size=300)\n    df['systolic_bp'] = np.random.uniform(110, 140, size=300)\n    df['diabetes'] = np.random.randint(0, 2, size=300)\n    df['hypertension'] = np.random.randint(0, 2, size=300)\n    df['heart_disease'] = np.random.randint(0, 2, size=300)\n    df['gender'] = np.random.choice(['M', 'F'], size=300)\n    \n    print(\"‚úÖ Donn√©es synth√©tiques cr√©√©es avec succ√®s\")\n    return df, y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        print(\"üõ†Ô∏è Fit du FeatureEngineer - aucune op√©ration n√©cessaire ici.\")\n        return self\n\n    def transform(self, X):\n        print(\"üõ†Ô∏è Transformation : ajout de nouvelles variables explicatives...\")\n        df = X.copy()\n        df['hour'] = df['timestamp'].dt.hour\n        df['day_of_week'] = df['timestamp'].dt.dayofweek\n        df['bmi_age_ratio'] = df['bmi'] / (df['age'] / 70)\n        df['pulse_pressure_norm'] = df['pulse_pressure'] / df['systolic_bp']\n        df['total_comorbidities'] = df[['diabetes', 'hypertension', 'heart_disease']].sum(axis=1)\n        print(\"‚úÖ 5 nouvelles variables cr√©√©es.\")\n        return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MedicalDataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\"üßπ Initialisation de l‚Äôimputeur avec ExtraTreesClassifier\")\n        self.imputer = IterativeImputer(\n            estimator=ExtraTreesClassifier(n_estimators=10, random_state=42),\n            max_iter=5,\n            random_state=42\n        )\n        self.numeric_cols = None\n\n    def fit(self, X, y=None):\n        print(\"üßπ Fit de l‚Äôimputeur sur les colonnes num√©riques...\")\n        self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n        self.imputer.fit(X[self.numeric_cols])\n        print(f\"‚úÖ Colonnes num√©riques d√©tect√©es : {self.numeric_cols}\")\n        return self\n\n    def transform(self, X):\n        print(\"üßπ Application de l‚Äôimputation et encodage binaire du genre...\")\n        df = X.copy()\n        df[self.numeric_cols] = self.imputer.transform(df[self.numeric_cols])\n        if 'gender' in df.columns:\n            df['gender'] = df['gender'].apply(lambda x: 1 if x == 'M' else 0)\n        print(\"‚úÖ Nettoyage termin√©.\")\n        return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AdvancedEnsembleClassifier:\n    def __init__(self, random_state=42):\n        print(\"ü§ñ Initialisation du classifieur d‚Äôensemble (RandomForest ici)...\")\n        self.random_state = random_state\n\n    def create_ensemble(self):\n        print(\"‚úÖ Cr√©ation d‚Äôun RandomForestClassifier\")\n        return RandomForestClassifier(random_state=self.random_state)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class OptunaMultiObjectiveOptimizer:\n    def __init__(self, config, ensemble_builder, study_path=\"optuna_study.pkl\"):\n        print(\"üìå Initialisation de l‚Äôoptimiseur Optuna multi-objectifs\")\n        self.config = config\n        self.ensemble_builder = ensemble_builder\n        self.study_path = study_path\n        self.scorers = {\n            'f2': make_scorer(fbeta_score, beta=2),\n            'recall': make_scorer(recall_score),\n            'precision': make_scorer(precision_score)\n        }\n        self.study = None\n\n    def _load_or_create_study(self):\n        if os.path.exists(self.study_path):\n            print(f\"üìÇ Chargement √©tude depuis {self.study_path}\")\n            self.study = joblib.load(self.study_path)\n        else:\n            print(\"üìà Cr√©ation nouvelle √©tude Optuna multi-objectifs\")\n            self.study = optuna.create_study(\n                directions=['maximize', 'maximize', 'maximize'],\n                sampler=optuna.samplers.TPESampler(seed=self.config.random_state)\n            )\n\n    def objective(self, trial, X, y):\n        print(f\"üéØ Trial {trial.number} - optimisation hyperparam√®tres\")\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n            'max_depth': trial.suggest_int('max_depth', 5, 20),\n        }\n        model = self.ensemble_builder.create_ensemble()\n        model.set_params(**params)\n\n        results = []\n        for name, scorer in self.scorers.items():\n            print(f\"üîç Validation crois√©e - scoring: {name}\")\n            score = cross_val_score(model, X, y, cv=3, scoring=scorer, n_jobs=-1).mean()\n            results.append(score)\n            print(f\"   ‚úÖ Score moyen {name}: {score:.4f}\")\n        return tuple(results)\n\n    def optimize(self, X, y, n_trials):\n        self._load_or_create_study()\n        print(\"üöÄ D√©but de l‚Äôoptimisation Optuna...\")\n\n        def save_checkpoint(study, trial):\n            joblib.dump(study, self.study_path)\n            print(f\"üíæ Checkpoint sauvegard√© apr√®s trial {trial.number}\")\n\n        self.study.optimize(\n            lambda trial: self.objective(trial, X, y),\n            n_trials=n_trials,\n            show_progress_bar=True,\n            callbacks=[save_checkpoint]\n        )\n        joblib.dump(self.study, self.study_path)\n        print(\"‚úÖ Optimisation termin√©e, √©tude sauvegard√©e\")\n        return self.study\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_save_best_model(study, X, y, builder, metric_index=0, filename=\"best_model.joblib\"):\n    print(f\"üèÜ S√©lection du meilleur mod√®le selon la m√©trique #{metric_index} (ex: 0=F2)...\")\n    best_trial = max(study.best_trials, key=lambda t: t.values[metric_index])\n    best_params = best_trial.params\n    print(f\"‚úÖ Param√®tres optimaux : {best_params}\")\n\n    model = builder.create_ensemble()\n    model.set_params(**best_params)\n    print(\"üîÑ Entra√Ænement du mod√®le...\")\n    model.fit(X, y)\n    joblib.dump(model, filename)\n    print(f\"üíæ Mod√®le sauvegard√© dans : {filename}\")\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_study(study_path=\"optuna_study.pkl\"):\n    import optuna.visualization as vis\n    if not os.path.exists(study_path):\n        print(\"‚ùå Aucun fichier d‚Äô√©tude trouv√©.\")\n        return\n    print(f\"üìä Chargement de l‚Äô√©tude {study_path} pour visualisation...\")\n    study = joblib.load(study_path)\n    print(\"üìà Pareto front (F2 vs recall vs precision)\")\n    vis.plot_pareto_front(study).show()\n    print(\"üìâ Importance des hyperparam√®tres\")\n    vis.plot_param_importances(study).show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_pipeline():\n    print(\"üèÅ Lancement complet du pipeline de bout-en-bout...\")\n    \n    # 1. Donn√©es synth√©tiques\n    df, y = generate_synthetic_data()\n\n    # 2. Preprocessing pipeline\n    print(\"üîß Application du pipeline de preprocessing...\")\n    feature_engineer = AdvancedFeatureEngineer()\n    cleaner = MedicalDataCleaner()\n    df = feature_engineer.fit_transform(df)\n    df = cleaner.fit_transform(df)\n\n    # 3. Ensemble Model\n    builder = AdvancedEnsembleClassifier()\n\n    # 4. Configuration simple\n    class Config:\n        random_state = 42\n        optuna_trials = 10\n    config = Config()\n\n    # 5. Optuna Optimizer\n    optimizer = OptunaMultiObjectiveOptimizer(config, builder)\n    study = optimizer.optimize(df, y, n_trials=config.optuna_trials)\n\n    # 6. Training du meilleur mod√®le\n    model = train_and_save_best_model(study, df, y, builder)\n\n    # 7. Visualisation\n    visualize_study()\n\n#if __name__ == \"__main__\":\n\nrun_pipeline()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint(\"üîç S√©lection de features via RandomForest (importance)...\")\nselector = SelectFromModel(RandomForestClassifier(n_estimators=100), threshold=\"mean\")\nX_selected = selector.fit_transform(df, y)\nselected_features = df.columns[selector.get_support()]\nprint(f\"‚úÖ {len(selected_features)} features s√©lectionn√©es sur {df.shape[1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfeature_selector = SelectFromModel(ExtraTreesClassifier(n_estimators=100), threshold=\"median\")\n\nfull_pipeline = Pipeline([\n    (\"feature_engineering\", AdvancedFeatureEngineer()),\n    (\"cleaning\", MedicalDataCleaner()),\n    (\"feature_selection\", feature_selector),\n    (\"model\", RandomForestClassifier())\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# 1. üì¶ INSTALLATION ET CONFIGURATION DE L'ENVIRONNEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ INSTALLATION ET CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Configuration initiale de l'environnement de travail avec toutes les d√©pendances n√©cessaires.\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils dont on aura besoin pour construire notre mod√®le, comme un bricoleur pr√©pare ses outils avant de commencer √† travailler.\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser un environnement virtuel (conda, venv) pour isoler les d√©pendances\")\nprint(\"- Pour une vraie industrialisation, on utiliserait Docker pour garantir la reproductibilit√©\")\n\n# La commande '%%capture' masque la sortie de la cellule pour plus de clart√©.\n# %%capture\n!pip install -q numpy==1.24.3 pandas==2.0.3 scikit-learn==1.2.2\n!pip install -q xgboost==1.7.6 lightgbm==4.0.0 catboost==1.2\n!pip install -q shap==0.42.1 imbalanced-learn==0.10.1\n!pip install -q optuna==3.3.0 plotly==5.15.0 seaborn==0.12.2\n!pip install -q evidently==0.4.2 pydantic==1.10.11\n!pip install -q joblib\n\nprint(\"\\nüîç [Expert] Importation de toutes les biblioth√®ques n√©cessaires pour le projet.\")\nprint(\"üë∂ [Enfant] On sort toutes les bo√Ætes d'outils dont on aura besoin et on les range sur la table.\")\n\n# --- Imports Essentiels ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport joblib\nimport os\nfrom dataclasses import dataclass\n\n# --- Imports pour le Preprocessing et la Pipeline ---\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# --- Imports pour la Mod√©lisation ---\nimport lightgbm as lgb\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score, fbeta_score,\n    precision_recall_curve, average_precision_score, make_scorer, roc_curve\n)\n\n# --- Imports pour l'Optimisation et l'Interpr√©tabilit√© ---\nimport optuna\nimport shap\n\n# --- Imports pour la Validation et le Monitoring ---\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset\n\nprint(\"\\nüîç [Expert] Configuration des param√®tres globaux pour la reproductibilit√© et la visualisation.\")\nprint(\"üë∂ [Enfant] On r√®gle tous nos outils pour qu'ils fonctionnent de la m√™me mani√®re √† chaque fois.\")\n\n# --- Configuration Globale ---\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# Seed pour une reproductibilit√© parfaite\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"\\n‚úÖ Environnement configur√© avec succ√®s. Les biblioth√®ques sont pr√™tes.\")\n\n# =============================================================================\n# 2. üèóÔ∏è ARCHITECTURE DE DONN√âES ET FONCTION DE G√âN√âRATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è ARCHITECTURE DE DONN√âES ET FONCTION DE G√âN√âRATION\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©finition de la structure des donn√©es via une classe de configuration centralis√©e.\")\nprint(\"üë∂ [Enfant] On cr√©e une recette qui explique comment notre mod√®le va apprendre.\")\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration centralis√©e pour la reproductibilit√© du mod√®le.\"\"\"\n    random_state: int = RANDOM_STATE\n    test_size: float = 0.2\n    cv_folds: int = 5\n    optuna_trials: int = 25\n    f_beta_value: float = 2.0\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser un fichier YAML/JSON pour la configuration\")\nprint(\"- Pour des projets complexes, on utiliserait Hydra ou OmegaConf\")\n\nconfig = ModelConfig()\nprint(f\"\\nüìã Configuration du mod√®le initialis√©e: {config}\")\n\nprint(\"\\nüîç [Expert] G√©n√©ration de donn√©es synth√©tiques r√©alistes pour simuler un dataset m√©dical.\")\nprint(\"üë∂ [Enfant] On invente des exemples de patients avec leurs caract√©ristiques pour que notre mod√®le puisse apprendre.\")\n\ndef generate_synthetic_frailty_data(n_samples: int = 2500) -> pd.DataFrame:\n    \"\"\"G√©n√®re des donn√©es synth√©tiques riches et r√©alistes pour la d√©tection de fragilit√©.\"\"\"\n    np.random.seed(config.random_state)\n    \n    # D√©mographie\n    patient_ids = [f\"P_{i:05d}\" for i in range(1, n_samples + 1)]\n    ages = np.random.normal(78, 8, n_samples).clip(65, 98).astype(int)\n    genders = np.random.choice(['Homme', 'Femme'], n_samples, p=[0.45, 0.55])\n    \n    # Signes vitaux\n    heart_rate = np.random.normal(72, 12, n_samples)\n    systolic_bp = 125 + (ages - 78) * 0.8 + np.random.normal(0, 15, n_samples)\n    diastolic_bp = 80 + (ages - 78) * 0.2 + np.random.normal(0, 10, n_samples)\n    \n    # M√©triques de fragilit√©\n    grip_strength = 30 - (ages - 65) * 0.4 + np.random.normal(0, 5, n_samples)\n    mobility_score = np.random.beta(2, 5, n_samples) * 10\n    \n    # Comorbidit√©s et style de vie\n    comorbidities_count = np.random.poisson(1.5, n_samples).clip(0, 5)\n    medication_count = np.random.poisson(3, n_samples).clip(0, 12)\n    \n    # Logique de la cible 'is_frail'\n    frailty_score = (\n        0.4 * (ages > 85).astype(int) + \n        0.3 * (grip_strength < 22).astype(int) + \n        0.2 * (mobility_score < 4).astype(int) + \n        0.15 * (comorbidities_count > 3).astype(int) + \n        0.1 * (medication_count > 6).astype(int) + \n        np.random.normal(0, 0.1, n_samples)\n    )\n    is_frail = (frailty_score > 0.55).astype(int)\n    \n    data = pd.DataFrame({\n        'patient_id': patient_ids, 'age': ages, 'gender': genders,\n        'heart_rate': heart_rate.clip(45, 120),\n        'systolic_bp': systolic_bp.clip(90, 200),\n        'diastolic_bp': diastolic_bp.clip(50, 110),\n        'grip_strength': grip_strength.clip(10, 50),\n        'mobility_score': mobility_score.clip(0, 10),\n        'comorbidities_count': comorbidities_count,\n        'medication_count': medication_count,\n        'is_frail': is_frail\n    })\n    \n    # Introduction de valeurs manquantes\n    for col in ['grip_strength', 'mobility_score']:\n        missing_mask = np.random.random(n_samples) < 0.10\n        data.loc[missing_mask, col] = np.nan\n    \n    return data\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait charger des donn√©es r√©elles depuis une base de donn√©es ou un fichier CSV\")\nprint(\"- Pour des donn√©es plus complexes, on utiliserait Faker ou SDV (Synthetic Data Vault)\")\n\nprint(\"\\nüîÑ G√©n√©ration des donn√©es synth√©tiques...\")\ndf_raw = generate_synthetic_frailty_data()\n\nprint(f\"\\nüìä Dataset brut g√©n√©r√©: {df_raw.shape[0]} patients, {df_raw.shape[1]} variables.\")\nprint(\"\\nüéØ Distribution de la cible (is_frail) :\")\nprint(df_raw['is_frail'].value_counts(normalize=True).round(3))\n\nprint(\"\\nüîç Aper√ßu des donn√©es et des valeurs manquantes :\")\ndf_raw.info()\n\n# =============================================================================\n# 3. üõ°Ô∏è S√âPARATION STRAT√âGIQUE DES DONN√âES (TRAIN/TEST)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ°Ô∏è S√âPARATION STRAT√âGIQUE DES DONN√âES (TRAIN/TEST)\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] S√©paration des donn√©es en ensembles d'entra√Ænement et de test avec stratification.\")\nprint(\"üë∂ [Enfant] On s√©pare nos exemples en deux piles : une pour apprendre et une pour v√©rifier qu'on a bien appris.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser TimeSeriesSplit pour des donn√©es temporelles\")\nprint(\"- Pour des petits datasets, on utiliserait Leave-One-Out ou Bootstrap\")\n\n# D√©finition des features (X) et de la cible (y)\nX = df_raw.drop(['is_frail', 'patient_id'], axis=1)\ny = df_raw['is_frail']\n\n# S√©paration stratifi√©e\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=config.test_size, \n    random_state=config.random_state,\n    stratify=y\n)\n\nprint(f\"\\nDimensions de l'ensemble d'entra√Ænement: X_train={X_train.shape}, y_train={y_train.shape}\")\nprint(f\"Dimensions de l'ensemble de test:       X_test={X_test.shape}, y_test={y_test.shape}\")\n\nprint(\"\\nDistribution de la cible dans l'ensemble d'entra√Ænement:\")\nprint(y_train.value_counts(normalize=True).round(3))\n\nprint(\"\\nDistribution de la cible dans l'ensemble de test:\")\nprint(y_test.value_counts(normalize=True).round(3))\nprint(\"\\n‚úÖ La stratification a bien conserv√© la distribution de la cible.\")\n\n# =============================================================================\n# 4. üõ†Ô∏è D√âFINITION DU PIPELINE DE PR√âTRAITEMENT ROBUSTE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üõ†Ô∏è D√âFINITION DU PIPELINE DE PR√âTRAITEMENT ROBUSTE\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Construction d'un pipeline de pr√©traitement robuste avec ColumnTransformer.\")\nprint(\"üë∂ [Enfant] On pr√©pare une machine qui nettoie et organise les donn√©es avant que le mod√®le ne les utilise.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser Feature-engine ou Scikit-lego pour des transformations plus avanc√©es\")\nprint(\"- Pour des donn√©es temporelles, on utiliserait tsfresh ou tsfel\")\n\n# Identification automatique des types de colonnes\nnumeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\ncategorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(f\"\\nD√©tection automatique des colonnes:\")\nprint(f\"  - Num√©riques ({len(numeric_features)}): {numeric_features}\")\nprint(f\"  - Cat√©gorielles ({len(categorical_features)}): {categorical_features}\")\n\nprint(\"\\nüîç [Expert] Cr√©ation d'un transformateur personnalis√© pour l'ing√©nierie de features.\")\nprint(\"üë∂ [Enfant] On ajoute une partie sp√©ciale √† notre machine qui cr√©e de nouvelles informations utiles.\")\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        return X_copy\n\nprint(\"\\nPipeline num√©rique: Imputer (m√©diane) -> RobustScaler\")\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', RobustScaler())\n])\n\nprint(\"\\nPipeline cat√©goriel: Imputer (mode) -> OneHotEncoder\")\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\nprint(\"\\nüîç [Expert] Combinaison des pipelines avec ColumnTransformer.\")\nprint(\"üë∂ [Enfant] On assemble toutes les parties de notre machine pour qu'elle fonctionne ensemble.\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\nprint(\"\\n‚úÖ Pr√©processeur combin√© (ColumnTransformer) cr√©√©.\")\n\n# =============================================================================\n# 5. üöÄ OPTIMISATION DES HYPERPARAM√àTRES AVEC OPTUNA\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION DES HYPERPARAM√àTRES AVEC OPTUNA\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Optimisation des hyperparam√®tres avec Optuna et validation crois√©e.\")\nprint(\"üë∂ [Enfant] On r√®gle finement notre machine pour qu'elle fonctionne le mieux possible.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser GridSearchCV ou RandomizedSearchCV de scikit-learn\")\nprint(\"- Pour des mod√®les complexes, on utiliserait Ray Tune ou Weights & Biases Sweeps\")\n\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'random_state': config.random_state,\n        'n_jobs': -1\n    }\n\n    f2_scorer = make_scorer(fbeta_score, beta=config.f_beta_value)\n    \n    scores = []\n    cv = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n    \n    for train_idx, val_idx in cv.split(X_train, y_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        \n        model_pipeline = Pipeline(steps=[\n            ('feature_engineering', FeatureEngineer()),\n            ('preprocessor', preprocessor),\n            ('classifier', lgb.LGBMClassifier(**params))\n        ])\n        \n        model_pipeline.fit(X_train_fold, y_train_fold)\n        preds = model_pipeline.predict(X_val_fold)\n        score = fbeta_score(y_val_fold, preds, beta=config.f_beta_value)\n        scores.append(score)\n\n    return np.mean(scores)\n\nprint(f\"\\nLancement de l'√©tude Optuna pour {config.optuna_trials} essais...\")\nprint(f\"Objectif: Maximiser le F{config.f_beta_value}-score moyen par validation crois√©e.\")\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=config.random_state))\nstudy.optimize(objective, n_trials=config.optuna_trials, show_progress_bar=True)\n\nprint(f\"\\n‚úÖ Optimisation termin√©e.\")\nprint(f\"Meilleur score F2 moyen (CV): {study.best_value:.4f}\")\nprint(f\"Meilleurs hyperparam√®tres trouv√©s: {study.best_params}\")\n\n# =============================================================================\n# 6. üéì ENTRA√éNEMENT ET SAUVEGARDE DU PIPELINE FINAL\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üéì ENTRA√éNEMENT ET SAUVEGARDE DU PIPELINE FINAL\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Entra√Ænement du pipeline final avec les meilleurs hyperparam√®tres.\")\nprint(\"üë∂ [Enfant] On fait travailler notre machine bien r√©gl√©e sur tous les exemples d'apprentissage.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser MLflow pour suivre les exp√©riences et versionner les mod√®les\")\nprint(\"- Pour le d√©ploiement, on pourrait sauvegarder au format ONNX ou PMML\")\n\nbest_params = study.best_params\n\nfinal_pipeline = Pipeline(steps=[\n    ('feature_engineering', FeatureEngineer()),\n    ('preprocessor', preprocessor),\n    ('classifier', lgb.LGBMClassifier(**best_params, random_state=config.random_state))\n])\n\nprint(\"\\nEntra√Ænement du pipeline final sur l'ENSEMBLE des donn√©es d'entra√Ænement...\")\nfinal_pipeline.fit(X_train, y_train)\nprint(\"‚úÖ Pipeline final entra√Æn√©.\")\n\nfilename = 'final_frailty_pipeline.joblib'\njoblib.dump(final_pipeline, filename)\nprint(f\"\\nüíæ Pipeline complet sauvegard√© sous le nom : '{filename}'\")\n\n# =============================================================================\n# 7. üìà √âVALUATION FINALE SUR L'ENSEMBLE DE TEST\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üìà √âVALUATION FINALE SUR L'ENSEMBLE DE TEST\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] √âvaluation rigoureuse du mod√®le sur des donn√©es jamais vues.\")\nprint(\"üë∂ [Enfant] On v√©rifie que notre machine fonctionne bien sur des nouveaux exemples qu'elle n'a jamais vus.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait calculer des intervalles de confiance via bootstrap\")\nprint(\"- Pour des probl√®mes d√©s√©quilibr√©s, on pourrait utiliser le score F1 ou le MCC\")\n\nprint(f\"\\nChargement du pipeline depuis '{filename}'...\")\nloaded_pipeline = joblib.load(filename)\n\nprint(\"\\nG√©n√©ration des pr√©dictions sur X_test...\")\ny_pred = loaded_pipeline.predict(X_test)\ny_pred_proba = loaded_pipeline.predict_proba(X_test)[:, 1]\n\nprint(\"\\n---------- RAPPORT DE CLASSIFICATION ----------\")\nprint(classification_report(y_test, y_pred, target_names=['Non Fragile (0)', 'Fragile (1)']))\n\nprint(\"\\nüîç [Expert] Visualisation de la matrice de confusion pour analyser les erreurs.\")\nprint(\"üë∂ [Enfant] On regarde o√π notre machine se trompe le plus souvent.\")\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Pr√©dit Non Fragile', 'Pr√©dit Fragile'],\n            yticklabels=['Vrai Non Fragile', 'Vrai Fragile'])\nplt.title('Matrice de Confusion sur l\\'Ensemble de Test')\nplt.ylabel('Classe R√©elle')\nplt.xlabel('Classe Pr√©dite')\nplt.show()\n\nprint(\"\\nüîç [Expert] Analyse des courbes ROC et Precision-Recall.\")\nprint(\"üë∂ [Enfant] On dessine des graphiques pour voir √† quel point notre machine est bonne.\")\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\npr_auc = average_precision_score(y_test, y_pred_proba)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\nax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (AUC = {roc_auc:.3f})')\nax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nax1.set_xlabel('Taux de Faux Positifs')\nax1.set_ylabel('Taux de Vrais Positifs (Rappel)')\nax1.set_title('Courbe ROC')\nax1.legend(loc='lower right')\n\nax2.plot(recall, precision, color='blue', lw=2, label=f'Courbe P-R (AUC = {pr_auc:.3f})')\nax2.set_xlabel('Rappel (Recall)')\nax2.set_ylabel('Pr√©cision (Precision)')\nax2.set_title('Courbe Pr√©cision-Rappel')\nax2.legend(loc='lower left')\n\nplt.suptitle('Analyse des Performances du Mod√®le sur l\\'Ensemble de Test', fontsize=16)\nplt.show()\n\n# =============================================================================\n# 8. üß† INTERPR√âTABILIT√â DU MOD√àLE AVEC SHAP\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. üß† INTERPR√âTABILIT√â DU MOD√àLE AVEC SHAP\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse SHAP pour comprendre l'importance des features et les pr√©dictions.\")\nprint(\"üë∂ [Enfant] On ouvre la machine pour voir comment elle prend ses d√©cisions.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser LIME pour des explications locales\")\nprint(\"- Pour les mod√®les lin√©aires, on analyserait directement les coefficients\")\n\nprint(\"\\nExtraction des composants du pipeline...\")\nfeature_engineer = loaded_pipeline.named_steps['feature_engineering']\npreprocessor = loaded_pipeline.named_steps['preprocessor']\nmodel = loaded_pipeline.named_steps['classifier']\n\nprint(\"\\nTransformation des donn√©es pour l'explainer SHAP...\")\nX_train_engineered = feature_engineer.transform(X_train)\nX_train_processed = preprocessor.transform(X_train_engineered)\n\nprint(\"\\nR√©cup√©ration des noms de features finaux...\")\nohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\nengineered_numeric_features = [col for col in X_train_engineered.columns if col not in categorical_features]\nfinal_feature_names = engineered_numeric_features + list(ohe_feature_names)\n\nX_train_processed_df = pd.DataFrame(X_train_processed, columns=final_feature_names, index=X_train.index)\n\nprint(\"\\nCalcul des valeurs SHAP... (peut prendre un moment)\")\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train_processed_df)\n\nprint(\"\\nAffichage du SHAP Summary Plot:\")\nprint(\"Chaque point est un patient. La couleur indique la valeur de la feature (rouge=√©lev√©e).\")\nprint(\"L'axe X montre l'impact sur la pr√©diction (vers la droite = probabilit√© de fragilit√© plus √©lev√©e).\")\n\nshap.summary_plot(shap_values[1], X_train_processed_df, plot_type=\"dot\")\n\n# =============================================================================\n# 9. üõ∞Ô∏è MONITORING DE D√âRIVE DES DONN√âES (DATA DRIFT)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. üõ∞Ô∏è MONITORING DE D√âRIVE DES DONN√âES (DATA DRIFT)\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©tection de d√©rive des donn√©es avec Evidently AI.\")\nprint(\"üë∂ [Enfant] On v√©rifie si les nouveaux patients ressemblent √† ceux sur lesquels on a appris.\")\n\nprint(\"\\nüí° Alternatives :\")\nprint(\"- On pourrait utiliser Alibi Detect ou Great Expectations\")\nprint(\"- Pour une int√©gration en production, on utiliserait Prometheus/Grafana\")\n\nprint(\"\\nPr√©paration des donn√©es de r√©f√©rence et courantes...\")\nreference_data = X_train.copy()\ncurrent_data = X_test.copy()\n\nreference_data['target'] = y_train\ncurrent_data['target'] = y_test\n\nprint(\"\\nCr√©ation du rapport de d√©rive des donn√©es...\")\ndata_drift_report = Report(metrics=[DataDriftPreset()])\ndata_drift_report.run(reference_data=reference_data, current_data=current_data)\n\nprint(\"\\n‚úÖ Rapport de d√©rive g√©n√©r√©.\")\nreport_path = 'data_drift_report.html'\ndata_drift_report.save_html(report_path)\nprint(f\"\\nüíæ Rapport de d√©rive sauvegard√© ici : '{report_path}'\")\n# =============================================================================\n# 10. ‚úÖ CONCLUSION ET PROCHAINES √âTAPES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"10. ‚úÖ CONCLUSION ET PROCHAINES √âTAPES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Synth√®se des r√©sultats et recommandations pour l'industrialisation.\")\nprint(\"üë∂ [Enfant] On regarde ce qu'on a appris et comment on pourrait faire encore mieux.\")\n\nprint(\"\\nüåü Synth√®se des R√©sultats :\")\nprint(\"- Pipeline complet entra√Æn√© et valid√© avec succ√®s\")\nprint(f\"- Performance sur le jeu de test : AUC-ROC = {roc_auc:.3f}, F2-score = {study.best_value:.3f}\")\nprint(\"- Principaux drivers identifi√©s : √¢ge, force de pr√©hension, mobilit√©\")\n\nprint(\"\\nüöÄ Prochaines √âtapes pour Industrialisation :\")\nprint(\"1. D√©ploiement en tant qu'API avec FastAPI ou Flask\")\nprint(\"2. Int√©gration MLOps avec MLflow ou Kubeflow\")\nprint(\"3. Monitoring continu des performances en production\")\nprint(\"4. Validation par des experts m√©tier (m√©decins g√©riatres)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üèÅ PROJET TERMIN√â AVEC SUCC√àS\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T20:08:10.034556Z","iopub.execute_input":"2025-06-21T20:08:10.035148Z","iopub.status.idle":"2025-06-21T20:09:38.298486Z","shell.execute_reply.started":"2025-06-21T20:08:10.035111Z","shell.execute_reply":"2025-06-21T20:09:38.297363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# 5. üöÄ OPTIMISATION DES HYPERPARAM√àTRES AVEC OPTUNA (AVEC CHECKPOINTS)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION DES HYPERPARAM√àTRES AVEC OPTUNA ET CHECKPOINTS\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Optimisation avec sauvegarde des checkpoints pour reprise apr√®s interruption.\")\nprint(\"üë∂ [Enfant] On enregistre r√©guli√®rement notre travail pour ne pas tout perdre si quelque chose se passe mal.\")\n\nprint(\"\\nüí° Bonnes pratiques :\")\nprint(\"- Sauvegarder apr√®s chaque essai d'hyperparam√®tres\")\nprint(\"- Stocker les m√©triques et les param√®tres\")\nprint(\"- Permettre la reprise de l'optimisation\")\n\n# Cr√©ation d'un r√©pertoire pour les checkpoints\nCHECKPOINT_DIR = \"optuna_checkpoints\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nCHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"optuna_study.pkl\")\n\nprint(f\"\\nüìÅ R√©pertoire des checkpoints cr√©√© : {CHECKPOINT_DIR}\")\n\n# Callback de sauvegarde\ndef save_checkpoint(study, trial):\n    joblib.dump(study, CHECKPOINT_PATH)\n    print(f\"üíæ Checkpoint sauvegard√© (essai {trial.number})\")\n\n# V√©rification si une √©tude existe d√©j√†\nif os.path.exists(CHECKPOINT_PATH):\n    print(\"\\nüîç Une √©tude existante a √©t√© trouv√©e. Chargement...\")\n    study = joblib.load(CHECKPOINT_PATH)\n    print(f\"Reprise √† partir de l'essai {len(study.trials)}\")\nelse:\n    print(\"\\nüîç Pas d'√©tude existante trouv√©e. Cr√©ation d'une nouvelle √©tude...\")\n    study = optuna.create_study(\n        direction='maximize', \n        sampler=optuna.samplers.TPESampler(seed=config.random_state),\n        study_name=\"frailty_detection\"\n    )\n\nprint(\"\\nüîç Configuration de la fonction objective avec gestion des checkpoints\")\ndef objective_with_checkpoint(trial):\n    try:\n        params = {\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'verbosity': -1,\n            'boosting_type': 'gbdt',\n            'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n            'max_depth': trial.suggest_int('max_depth', 3, 10),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n            'random_state': config.random_state,\n            'n_jobs': -1\n        }\n\n        f2_scorer = make_scorer(fbeta_score, beta=config.f_beta_value)\n        \n        scores = []\n        cv = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n        \n        for train_idx, val_idx in cv.split(X_train, y_train):\n            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n            \n            model_pipeline = Pipeline(steps=[\n                ('feature_engineering', FeatureEngineer()),\n                ('preprocessor', preprocessor),\n                ('classifier', lgb.LGBMClassifier(**params))\n            ])\n            \n            model_pipeline.fit(X_train_fold, y_train_fold)\n            preds = model_pipeline.predict(X_val_fold)\n            score = fbeta_score(y_val_fold, preds, beta=config.f_beta_value)\n            scores.append(score)\n\n        return np.mean(scores)\n    \n    except Exception as e:\n        print(f\"‚ùå Erreur dans l'essai {trial.number}: {str(e)}\")\n        raise optuna.TrialPruned()\n\nprint(\"\\nüîç Configuration de l'√©tude Optuna avec callback de sauvegarde\")\nstudy.optimize(\n    objective_with_checkpoint, \n    n_trials=config.optuna_trials, \n    callbacks=[save_checkpoint],\n    show_progress_bar=True\n)\n\nprint(f\"\\n‚úÖ Optimisation termin√©e. Meilleur score F2: {study.best_value:.4f}\")\n\n# =============================================================================\n# 6. üéì ENTRA√éNEMENT ET SAUVEGARDE DU PIPELINE FINAL (AVEC VERSIONNING)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üéì ENTRA√éNEMENT ET SAUVEGARDE AVEC VERSIONNING\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Sauvegarde versionn√©e du mod√®le final avec m√©tadonn√©es.\")\nprint(\"üë∂ [Enfant] On garde une copie de notre meilleure machine avec son mode d'emploi.\")\n\nprint(\"\\nüí° Bonnes pratiques :\")\nprint(\"- Versionner les mod√®les avec des tags clairs\")\nprint(\"- Sauvegarder les m√©tadonn√©es (hyperparam√®tres, m√©triques)\")\nprint(\"- Garder une trace des donn√©es d'entra√Ænement\")\n\n# Cr√©ation d'un r√©pertoire de versionnement\nMODEL_VERSION = \"1.0.0\"\nMODEL_DIR = f\"model_artifacts_v{MODEL_VERSION}\"\nos.makedirs(MODEL_DIR, exist_ok=True)\n\nprint(f\"\\nüìÅ R√©pertoire de versionnement cr√©√© : {MODEL_DIR}\")\n\n# Sauvegarde du pipeline final\nbest_params = study.best_params\n\nfinal_pipeline = Pipeline(steps=[\n    ('feature_engineering', FeatureEngineer()),\n    ('preprocessor', preprocessor),\n    ('classifier', lgb.LGBMClassifier(**best_params, random_state=config.random_state))\n])\n\nprint(\"\\nEntra√Ænement du pipeline final...\")\nfinal_pipeline.fit(X_train, y_train)\n\n# Sauvegarde des diff√©rents artefacts\nmodel_path = os.path.join(MODEL_DIR, 'model.joblib')\nmetadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n\nprint(\"\\nüíæ Sauvegarde des artefacts du mod√®le...\")\njoblib.dump(final_pipeline, model_path)\n\n# M√©tadonn√©es √† sauvegarder\nmetadata = {\n    \"model_version\": MODEL_VERSION,\n    \"training_date\": pd.Timestamp.now().isoformat(),\n    \"features\": list(X_train.columns),\n    \"best_params\": best_params,\n    \"performance\": {\n        \"roc_auc\": roc_auc_score(y_test, final_pipeline.predict_proba(X_test)[:, 1]),\n        \"f2_score\": fbeta_score(y_test, final_pipeline.predict(X_test), beta=2.0)\n    },\n    \"data_stats\": {\n        \"train_size\": len(X_train),\n        \"test_size\": len(X_test),\n        \"class_balance\": dict(y_train.value_counts(normalize=True).round(3))\n    }\n}\n\nimport json\nwith open(metadata_path, 'w') as f:\n    json.dump(metadata, f, indent=4)\n\nprint(\"\\nüìù M√©tadonn√©es sauvegard√©es :\")\nprint(json.dumps(metadata, indent=4))\n\nprint(\"\\n‚úÖ Pipeline final et m√©tadonn√©es sauvegard√©s avec versionnement.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T20:20:50.834834Z","iopub.execute_input":"2025-06-21T20:20:50.836483Z","iopub.status.idle":"2025-06-21T20:21:46.488776Z","shell.execute_reply.started":"2025-06-21T20:20:50.836398Z","shell.execute_reply":"2025-06-21T20:21:46.487683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC EXPLICATIONS D√âTAILL√âES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire une machine intelligente qui peut rep√©rer les personnes fragiles\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- Pour des projets simples : utiliser sklearn sans pipeline\")\nprint(\"- Pour le big data : utiliser Spark ML ou Dask\")\nprint(\"- Pour l'IA embarqu√©e : utiliser ONNX Runtime ou TensorFlow Lite\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Importation des librairies essentielles pour le machine learning industriel\")\nprint(\"üë∂ [Enfant] On sort toutes les bo√Ætes √† outils dont on aura besoin\")\nprint(\"\\nüí° Gestion des d√©pendances :\")\nprint(\"- En entreprise : utiliser un environnement conda/pip freeze\")\nprint(\"- En production : conteneur Docker avec versions fig√©es\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\n\nprint(\"\\nüîç [Expert] Importation des composants scikit-learn pour le pipeline\")\nprint(\"üë∂ [Enfant] On prend les pi√®ces pour construire notre machine\")\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nprint(\"\\nüîç [Expert] M√©triques pour l'√©valuation robuste en milieu clinique\")\nprint(\"üë∂ [Enfant] On choisit comment juger si notre machine fonctionne bien\")\n\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, \n                           precision_score, confusion_matrix, classification_report,\n                           precision_recall_curve, roc_curve, average_precision_score,\n                           make_scorer, balanced_accuracy_score)\n\nprint(\"\\nüîç [Expert] LightGBM pour des mod√®les performants et interpr√©tables\")\nprint(\"üë∂ [Enfant] On prend un moteur puissant mais qu'on peut comprendre\")\n\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.calibration import calibration_curve\n\nprint(\"\\nüîç [Expert] Configuration de base pour la reproductibilit√©\")\nprint(\"üë∂ [Enfant] On r√®gle notre machine pour qu'elle donne toujours les m√™mes r√©sultats\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©finition des KPI m√©tier pour √©valuer le mod√®le\")\nprint(\"üë∂ [Enfant] On d√©cide comment noter notre machine\")\nprint(\"\\nüí° Choix des m√©triques :\")\nprint(\"- Probl√®me √©quilibr√© : accuracy et AUC\")\nprint(\"- D√©s√©quilibre mod√©r√© : F1-score\")\nprint(\"- Cas critique (comme ici) : F2-score privil√©giant le recall\")\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation\"\"\"\n    def __init__(self):\n        print(\"\\nüîç [Expert] Initialisation des m√©triques avec contraintes m√©tier\")\n        print(\"üë∂ [Enfant] On pr√©pare notre carte de notation\")\n        \n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        \n        print(\"\\nüîç [Expert] D√©finition des seuils d'alerte cliniques\")\n        print(\"üë∂ [Enfant] On fixe les notes en dessous desquelles c'est inqui√©tant\")\n        \n        self.alert_thresholds = {\n            'roc_auc': 0.75,  # Minimum acceptable en milieu clinique\n            'f2_score': 0.6,  # Seuil empirique pour les cas critiques\n            'recall': 0.8     # Doit d√©tecter au moins 80% des cas fragiles\n        }\n    \n    def _specificity(self, y_true, y_pred):\n        \"\"\"Calcul sp√©cifique de la sp√©cificit√© (True Negative Rate)\"\"\"\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n        return tn / (tn + fp)\n\nprint(\"\\nüîç [Expert] Instanciation de la configuration d'√©valuation\")\nprint(\"üë∂ [Enfant] On active notre syst√®me de notation\")\n\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Cr√©ation de features cliniques pertinentes\")\nprint(\"üë∂ [Enfant] On ajoute des mesures utiles que notre machine pourra comprendre\")\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Ing√©nierie de caract√©ristiques personnalis√©es\"\"\"\n    def fit(self, X, y=None):\n        print(\"\\nüîç [Expert] Phase d'apprentissage des transformations (rien √† apprendre ici)\")\n        print(\"üë∂ [Enfant] Notre machine regarde les donn√©es mais n'a rien besoin de m√©moriser\")\n        return self\n\n    def transform(self, X):\n        print(\"\\nüîç [Expert] Cr√©ation de features cliniques d√©riv√©es\")\n        print(\"üë∂ [Enfant] On calcule de nouvelles informations utiles √† partir des mesures existantes\")\n        \n        X_copy = X.copy()\n        # Pression puls√©e (indicateur clinique important)\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        \n        # Interaction tension/mobilit√© (relation clinique connue)\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        \n        return X_copy\n\nprint(\"\\nüîç [Expert] Configuration du pr√©processeur industriel\")\nprint(\"üë∂ [Enfant] On pr√©pare les √©tapes pour nettoyer et pr√©parer les donn√©es\")\nprint(\"\\nüí° Alternatives de pr√©traitement :\")\nprint(\"- Donn√©es manquantes : imputation par la m√©diane/mode ou mod√®le pr√©dictif\")\nprint(\"- Variables cat√©gorielles : one-hot encoding, target encoding ou embeddings\")\nprint(\"- Normalisation : StandardScaler, RobustScaler ou QuantileTransformer\")\n\nprint(\"\\nüîç [Expert] D√©finition des caract√©ristiques num√©riques et cat√©gorielles\")\nprint(\"üë∂ [Enfant] On trie les informations en nombres et en cat√©gories\")\n\nnumeric_features = ['age', 'heart_rate', 'systolic_bp', 'diastolic_bp', \n                   'grip_strength', 'mobility_score', 'comorbidities_count', \n                   'medication_count']\ncategorical_features = ['gender']\n\nprint(\"\\nüîç [Expert] Pipeline pour les variables num√©riques (imputation + scaling robuste)\")\nprint(\"üë∂ [Enfant] On nettoie et met √† l'√©chelle les nombres\")\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),  # Robust aux outliers\n    ('scaler', RobustScaler())                      # Peu sensible aux valeurs extr√™mes\n])\n\nprint(\"\\nüîç [Expert] Pipeline pour les variables cat√©gorielles (imputation + one-hot)\")\nprint(\"üë∂ [Enfant] On nettoie et transforme les cat√©gories en nombres\")\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # Remplissage par le mode\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  # √âvite la multicolin√©arit√©\n])\n\nprint(\"\\nüîç [Expert] Combinaison des pipelines avec ColumnTransformer\")\nprint(\"üë∂ [Enfant] On assemble toutes les pi√®ces du nettoyage\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'  # Conserve les autres colonnes si besoin\n)\n# 4. üéØ FONCTIONS D'√âVALUATION INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ VALIDATION ROBUSTE\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Impl√©mentation d'une validation crois√©e stratifi√©e compl√®te\")\nprint(\"üë∂ [Enfant] On v√©rifie plusieurs fois que notre machine marche bien sur des d√©coupages diff√©rents\")\nprint(\"\\nüí° Strat√©gies alternatives :\")\nprint(\"- TimeSeriesSplit pour donn√©es temporelles\")\nprint(\"- GroupKFold pour donn√©es corr√©l√©es\")\nprint(\"- Bootstrap pour petits datasets\")\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"\n    √âvaluation compl√®te avec validation crois√©e\n    \n    Args:\n        pipeline: Pipeline sklearn √† √©valuer\n        X: Features\n        y: Target\n        cv_folds: Nombre de folds\n    \n    Returns:\n        DataFrame avec r√©sultats d√©taill√©s\n    \"\"\"\n    print(f\"\\nüîç [Expert] Lancement de la validation crois√©e ({cv_folds} folds)\")\n    print(\"üë∂ [Enfant] On d√©coupe les donn√©es en plusieurs parties pour tester plusieurs fois\")\n    \n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    \n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2),\n        'recall': 'recall',\n        'precision': 'precision',\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    \n    print(\"\\nüîç [Expert] Calcul des m√©triques sur chaque fold (parall√©lis√©)\")\n    cv_results = cross_validate(\n        pipeline, X, y,\n        cv=cv,\n        scoring=scoring,\n        return_train_score=True,\n        n_jobs=-1  # Utilisation de tous les c≈ìurs disponibles\n    )\n    \n    results_df = pd.DataFrame(cv_results)\n    print(f\"\\nüìä R√©sultats moyens :\\n{results_df.mean().to_string()}\")\n    \n    return results_df\n\n# 5. üìä ANALYSE DES SEUILS DE D√âCISION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üìä OPTIMISATION DES SEUILS\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse du trade-off pr√©cision/rappel par seuil\")\nprint(\"üë∂ [Enfant] On cherche le meilleur r√©glage pour notre machine\")\nprint(\"\\nüí° M√©thodes alternatives :\")\nprint(\"- Optimisation directe du seuil avec BayesSearchCV\")\nprint(\"- Maximisation du score m√©tier personnalis√©\")\nprint(\"- Courbe ROC Youden pour seuil optimal\")\n\ndef analyze_decision_threshold(y_true, y_probs):\n    \"\"\"Analyse approfondie des seuils de classification\"\"\"\n    print(\"\\nüîç [Expert] Calcul des m√©triques pour 20 seuils diff√©rents\")\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    \n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    \n    metrics_df = pd.DataFrame(metrics)\n    \n    print(\"\\nüîç [Expert] Visualisation interactive des m√©triques\")\n    plt.figure(figsize=(10, 6))\n    metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'])\n    plt.axvline(x=0.5, color='gray', linestyle='--', label='Seuil par d√©faut')\n    plt.title(\"Performance par seuil de d√©cision\")\n    plt.xlabel(\"Seuil de classification\")\n    plt.ylabel(\"Score\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \n    return metrics_df\n\n# 6. üí∞ ANALYSE CO√õT DES ERREURS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üí∞ IMPACT M√âTIER\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse co√ªt/b√©n√©fice des diff√©rents types d'erreurs\")\nprint(\"üë∂ [Enfant] On calcule combien co√ªtent les erreurs de notre machine\")\nprint(\"\\nüí° Approches compl√©mentaires :\")\nprint(\"- Analyse ROI complet\")\nprint(\"- Matrice de confusion pond√©r√©e\")\nprint(\"- Optimisation directe du co√ªt m√©tier\")\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions\"\"\"\n    print(\"\\nüîç [Expert] Extraction de la matrice de confusion\")\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    \n    print(\"\\nüîç [Expert] D√©finition des co√ªts m√©tier (√† adapter)\")\n    cost_matrix = {\n        'fn_cost': 1000,  # Co√ªt d'un faux n√©gatif (cas manqu√©)\n        'fp_cost': 200,    # Co√ªt d'un faux positif (surdiagnostic)\n        'tp_gain': 500,    # Gain d'un vrai positif (bonne prise en charge)\n        'tn_gain': 50      # Gain d'un vrai n√©gatif (pas d'intervention inutile)\n    }\n    \n    total_cost = (fn * cost_matrix['fn_cost'] + \n                 fp * cost_matrix['fp_cost'] - \n                 tp * cost_matrix['tp_gain'] - \n                 tn * cost_matrix['tn_gain'])\n    \n    print(\"\\nüí∞ Bilan des co√ªts :\")\n    print(f\"- Faux n√©gatifs (FN): {fn} √ó {cost_matrix['fn_cost']} = {fn * cost_matrix['fn_cost']}\")\n    print(f\"- Faux positifs (FP): {fp} √ó {cost_matrix['fp_cost']} = {fp * cost_matrix['fp_cost']}\")\n    print(f\"- Vrais positifs (TP): {tp} √ó -{cost_matrix['tp_gain']} = {-tp * cost_matrix['tp_gain']}\")\n    print(f\"- Vrais n√©gatifs (TN): {tn} √ó -{cost_matrix['tn_gain']} = {-tn * cost_matrix['tn_gain']}\")\n    print(f\"‚Üí CO√õT TOTAL: {total_cost}\")\n    \n    return {\n        'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp},\n        'cost_analysis': cost_matrix,\n        'total_cost': total_cost\n    }\n\n# 7. üöÄ OPTIMISATION AVEC CHECKPOINTS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üöÄ OPTIMISATION DES HYPERPARAM√àTRES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Configuration d'Optuna avec sauvegarde des checkpoints\")\nprint(\"üë∂ [Enfant] On r√®gle finement notre machine en gardant des sauvegardes\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- GridSearchCV pour un espace de recherche petit\")\nprint(\"- Ray Tune pour le scale-out\")\nprint(\"- Hyperopt pour des algorithmes d'optimisation avanc√©s\")\n\ndef create_study_with_checkpoints(study_name, storage_name=None):\n    \"\"\"Cr√©e ou charge une √©tude Optuna\"\"\"\n    CHECKPOINT_DIR = \"optuna_checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{study_name}.pkl\")\n    \n    if os.path.exists(CHECKPOINT_PATH):\n        print(\"\\nüîç [Expert] Chargement d'une √©tude existante\")\n        study = joblib.load(CHECKPOINT_PATH)\n        print(f\"üìö Essais pr√©c√©dents charg√©s: {len(study.trials)}\")\n    else:\n        print(\"\\nüîç [Expert] Cr√©ation d'une nouvelle √©tude\")\n        study = optuna.create_study(\n            direction='maximize',\n            sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n            study_name=study_name,\n            storage=storage_name\n        )\n    \n    return study, CHECKPOINT_PATH\n\ndef objective(trial, X, y):\n    \"\"\"Fonction objective pour Optuna\"\"\"\n    print(f\"\\nüîç [Expert] Essai {trial.number} - Exploration des hyperparam√®tres\")\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'random_state': RANDOM_STATE\n    }\n    \n    pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**params))\n    ])\n    \n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    # Contrainte m√©tier sur le recall minimum\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall trop faible\")\n        raise optuna.TrialPruned()\n    \n    return mean_f2\n\n# 8. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. üè≠ PIPELINE FINAL\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Entra√Ænement et versionning du mod√®le avec m√©tadonn√©es\")\nprint(\"üë∂ [Enfant] On sauvegarde notre meilleure machine avec son mode d'emploi\")\nprint(\"\\nüí° Bonnes pratiques :\")\nprint(\"- MLflow pour le suivi complet\")\nprint(\"- ONNX pour l'interop√©rabilit√©\")\nprint(\"- DVC pour la gestion des versions\")\n\ndef train_final_model(X_train, y_train, best_params, version=\"1.0.0\"):\n    \"\"\"Entra√Æne et sauvegarde le mod√®le final\"\"\"\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le final (version {version})\")\n    \n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE))\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    # Sauvegarde du mod√®le\n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    print(f\"üíæ Mod√®le sauvegard√©: {model_path}\")\n    \n    # M√©tadonn√©es\n    metadata = {\n        \"model_version\": version,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features\": list(X_train.columns),\n        \"best_params\": best_params,\n        \"metrics\": {\n            \"cv_folds\": 5,\n            \"optimization_metric\": \"f2_score\",\n            \"recall_constraint\": eval_config.alert_thresholds['recall']\n        },\n        \"data_schema\": {\n            \"numeric_features\": numeric_features,\n            \"categorical_features\": categorical_features\n        }\n    }\n    \n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4)\n    print(f\"üìù M√©tadonn√©es sauvegard√©es: {metadata_path}\")\n    \n    return final_pipeline, MODEL_DIR\n\n# 9. üìä VISUALISATIONS CLINIQUES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. üìä ANALYSE DES PERFORMANCES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Visualisations professionnelles pour l'analyse\")\nprint(\"üë∂ [Enfant] On dessine des graphiques pour comprendre notre machine\")\nprint(\"\\nüí° Visualisations compl√©mentaires :\")\nprint(\"- Diagramme de d√©cision clinique\")\nprint(\"- Analyse des erreurs par sous-groupes\")\nprint(\"- Courbes de calibration par √¢ge\")\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Courbe de calibration\"\"\"\n    print(\"\\nüîç [Expert] V√©rification de la calibration des probabilit√©s\")\n    prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n    plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n    plt.xlabel(\"Probabilit√© pr√©dite\")\n    plt.ylabel(\"Probabilit√© r√©elle\")\n    plt.title(\"Courbe de calibration\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# 10. üèÅ EX√âCUTION COMPL√àTE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"10. üèÅ PIPELINE COMPLET\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Orchestration de l'ensemble du workflow\")\nprint(\"üë∂ [Enfant] On lance toutes les √©tapes d'un coup\")\n\ndef run_full_pipeline(X, y, n_trials=50):\n    \"\"\"Ex√©cute l'ensemble du pipeline\"\"\"\n    print(\"\\nüîç [Expert] S√©paration train/test stratifi√©e\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n    )\n    \n    print(\"\\nüîç [Expert] Optimisation hyperparam√©trique\")\n    study, checkpoint_path = create_study_with_checkpoints(\"frailty_detection\")\n    \n    def save_checkpoint(study, trial):\n        joblib.dump(study, checkpoint_path)\n        print(f\"üíæ Checkpoint sauvegard√© (essai {trial.number})\")\n    \n    study.optimize(\n        lambda trial: objective(trial, X_train, y_train),\n        n_trials=n_trials,\n        callbacks=[save_checkpoint],\n        show_progress_bar=True\n    )\n    \n    print(\"\\nüîç [Expert] Entra√Ænement du mod√®le final\")\n    best_params = study.best_params\n    model, model_dir = train_final_model(X_train, y_train, best_params)\n    \n    print(\"\\nüîç [Expert] √âvaluation finale\")\n    y_probs = model.predict_proba(X_test)[:, 1]\n    y_pred = model.predict(X_test)\n    \n    print(\"\\nüìä Rapport de classification :\")\n    print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n    \n    print(\"\\nüîç [Expert] Analyses avanc√©es\")\n    metrics_df = analyze_decision_threshold(y_test, y_probs)\n    cost_results = cost_analysis(y_test, y_pred)\n    plot_calibration(y_test, y_probs)\n    \n    print(\"\\nüîç [Expert] Sauvegarde du rapport final\")\n    report = {\n        \"best_params\": best_params,\n        \"test_metrics\": {\n            \"roc_auc\": roc_auc_score(y_test, y_probs),\n            \"f2_score\": fbeta_score(y_test, y_pred, beta=2),\n            \"recall\": recall_score(y_test, y_pred),\n            \"precision\": precision_score(y_test, y_pred),\n            \"specificity\": eval_config.metrics['specificity'](y_test, y_pred)\n        },\n        \"cost_analysis\": cost_results,\n        \"optimal_threshold\": metrics_df.loc[metrics_df['f2'].idxmax()].to_dict()\n    }\n    \n    report_path = os.path.join(model_dir, 'final_report.json')\n    with open(report_path, 'w') as f:\n        json.dump(report, f, indent=4)\n    \n    print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {report_path}\")\n    return model, report\n\n# Exemple d'utilisation\nif __name__ == \"__main__\":\n    # Chargement des donn√©es (exemple avec donn√©es synth√©tiques)\n    from sklearn.datasets import make_classification\n    X, y = make_classification(n_samples=2000, n_features=10, n_classes=2,\n                              weights=[0.7, 0.3], random_state=RANDOM_STATE)\n    \n    # Ex√©cution\n    model, report = run_full_pipeline(X, y, n_trials=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T20:47:04.669662Z","iopub.execute_input":"2025-06-21T20:47:04.670206Z","iopub.status.idle":"2025-06-21T20:47:11.080551Z","shell.execute_reply.started":"2025-06-21T20:47:04.670167Z","shell.execute_reply":"2025-06-21T20:47:11.078783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline  \n!pip install matplotlib --upgrade\nimport matplotlib\nmatplotlib.use('Agg')  # Optionnel, utile si vous faites des .savefig() sans affichage\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:54:29.465047Z","iopub.execute_input":"2025-06-21T21:54:29.465461Z","iopub.status.idle":"2025-06-21T21:54:55.135056Z","shell.execute_reply.started":"2025-06-21T21:54:29.465436Z","shell.execute_reply":"2025-06-21T21:54:55.133667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC EXPLICATIONS D√âTAILL√âES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire une machine intelligente qui peut rep√©rer les personnes fragiles\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- Pour des projets simples : utiliser sklearn sans pipeline\")\nprint(\"- Pour le big data : utiliser Spark ML ou Dask\")\nprint(\"- Pour l'IA embarqu√©e : utiliser ONNX Runtime ou TensorFlow Lite\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Importation des librairies essentielles pour le machine learning industriel\")\nprint(\"üë∂ [Enfant] On sort toutes les bo√Ætes √† outils dont on aura besoin\")\nprint(\"\\nüí° Gestion des d√©pendances :\")\nprint(\"- En entreprise : utiliser un environnement conda/pip freeze\")\nprint(\"- En production : conteneur Docker avec versions fig√©es\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\n\nprint(\"\\nüîç [Expert] Importation des composants scikit-learn pour le pipeline\")\nprint(\"üë∂ [Enfant] On prend les pi√®ces pour construire notre machine\")\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nprint(\"\\nüîç [Expert] M√©triques pour l'√©valuation robuste en milieu clinique\")\nprint(\"üë∂ [Enfant] On choisit comment juger si notre machine fonctionne bien\")\n\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, \n                           precision_score, confusion_matrix, classification_report,\n                           precision_recall_curve, roc_curve, average_precision_score,\n                           make_scorer, balanced_accuracy_score)\n\nprint(\"\\nüîç [Expert] LightGBM pour des mod√®les performants et interpr√©tables\")\nprint(\"üë∂ [Enfant] On prend un moteur puissant mais qu'on peut comprendre\")\n\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.calibration import calibration_curve\n\nprint(\"\\nüîç [Expert] Configuration de base pour la reproductibilit√©\")\nprint(\"üë∂ [Enfant] On r√®gle notre machine pour qu'elle donne toujours les m√™mes r√©sultats\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©finition des KPI m√©tier pour √©valuer le mod√®le\")\nprint(\"üë∂ [Enfant] On d√©cide comment noter notre machine\")\nprint(\"\\nüí° Choix des m√©triques :\")\nprint(\"- Probl√®me √©quilibr√© : accuracy et AUC\")\nprint(\"- D√©s√©quilibre mod√©r√© : F1-score\")\nprint(\"- Cas critique (comme ici) : F2-score privil√©giant le recall\")\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation\"\"\"\n    def __init__(self):\n        print(\"\\nüîç [Expert] Initialisation des m√©triques avec contraintes m√©tier\")\n        print(\"üë∂ [Enfant] On pr√©pare notre carte de notation\")\n        \n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        \n        print(\"\\nüîç [Expert] D√©finition des seuils d'alerte cliniques\")\n        print(\"üë∂ [Enfant] On fixe les notes en dessous desquelles c'est inqui√©tant\")\n        \n        self.alert_thresholds = {\n            'roc_auc': 0.75,  # Minimum acceptable en milieu clinique\n            'f2_score': 0.6,  # Seuil empirique pour les cas critiques\n            'recall': 0.8     # Doit d√©tecter au moins 80% des cas fragiles\n        }\n    \n    def _specificity(self, y_true, y_pred):\n        \"\"\"Calcul sp√©cifique de la sp√©cificit√© (True Negative Rate)\"\"\"\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n        return tn / (tn + fp)\n\nprint(\"\\nüîç [Expert] Instanciation de la configuration d'√©valuation\")\nprint(\"üë∂ [Enfant] On active notre syst√®me de notation\")\n\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Cr√©ation de features cliniques pertinentes\")\nprint(\"üë∂ [Enfant] On ajoute des mesures utiles que notre machine pourra comprendre\")\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Ing√©nierie de caract√©ristiques personnalis√©es\"\"\"\n    def fit(self, X, y=None):\n        print(\"\\nüîç [Expert] Phase d'apprentissage des transformations (rien √† apprendre ici)\")\n        print(\"üë∂ [Enfant] Notre machine regarde les donn√©es mais n'a rien besoin de m√©moriser\")\n        return self\n\n    def transform(self, X):\n        print(\"\\nüîç [Expert] Cr√©ation de features cliniques d√©riv√©es\")\n        print(\"üë∂ [Enfant] On calcule de nouvelles informations utiles √† partir des mesures existantes\")\n        \n        X_copy = X.copy()\n        # Pression puls√©e (indicateur clinique important)\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        \n        # Interaction tension/mobilit√© (relation clinique connue)\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        \n        return X_copy\n\nprint(\"\\nüîç [Expert] Configuration du pr√©processeur industriel\")\nprint(\"üë∂ [Enfant] On pr√©pare les √©tapes pour nettoyer et pr√©parer les donn√©es\")\nprint(\"\\nüí° Alternatives de pr√©traitement :\")\nprint(\"- Donn√©es manquantes : imputation par la m√©diane/mode ou mod√®le pr√©dictif\")\nprint(\"- Variables cat√©gorielles : one-hot encoding, target encoding ou embeddings\")\nprint(\"- Normalisation : StandardScaler, RobustScaler ou QuantileTransformer\")\n\nprint(\"\\nüîç [Expert] D√©finition des caract√©ristiques num√©riques et cat√©gorielles\")\nprint(\"üë∂ [Enfant] On trie les informations en nombres et en cat√©gories\")\n\nnumeric_features = ['age', 'heart_rate', 'systolic_bp', 'diastolic_bp', \n                   'grip_strength', 'mobility_score', 'comorbidities_count', \n                   'medication_count']\ncategorical_features = ['gender']\n\nprint(\"\\nüîç [Expert] Pipeline pour les variables num√©riques (imputation + scaling robuste)\")\nprint(\"üë∂ [Enfant] On nettoie et met √† l'√©chelle les nombres\")\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),  # Robust aux outliers\n    ('scaler', RobustScaler())                      # Peu sensible aux valeurs extr√™mes\n])\n\nprint(\"\\nüîç [Expert] Pipeline pour les variables cat√©gorielles (imputation + one-hot)\")\nprint(\"üë∂ [Enfant] On nettoie et transforme les cat√©gories en nombres\")\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # Remplissage par le mode\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  # √âvite la multicolin√©arit√©\n])\n\nprint(\"\\nüîç [Expert] Combinaison des pipelines avec ColumnTransformer\")\nprint(\"üë∂ [Enfant] On assemble toutes les pi√®ces du nettoyage\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'  # Conserve les autres colonnes si besoin\n)\n# 4. üéØ FONCTIONS D'√âVALUATION INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ VALIDATION ROBUSTE\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Impl√©mentation d'une validation crois√©e stratifi√©e compl√®te\")\nprint(\"üë∂ [Enfant] On v√©rifie plusieurs fois que notre machine marche bien sur des d√©coupages diff√©rents\")\nprint(\"\\nüí° Strat√©gies alternatives :\")\nprint(\"- TimeSeriesSplit pour donn√©es temporelles\")\nprint(\"- GroupKFold pour donn√©es corr√©l√©es\")\nprint(\"- Bootstrap pour petits datasets\")\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"\n    √âvaluation compl√®te avec validation crois√©e\n    \n    Args:\n        pipeline: Pipeline sklearn √† √©valuer\n        X: Features\n        y: Target\n        cv_folds: Nombre de folds\n    \n    Returns:\n        DataFrame avec r√©sultats d√©taill√©s\n    \"\"\"\n    print(f\"\\nüîç [Expert] Lancement de la validation crois√©e ({cv_folds} folds)\")\n    print(\"üë∂ [Enfant] On d√©coupe les donn√©es en plusieurs parties pour tester plusieurs fois\")\n    \n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    \n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2),\n        'recall': 'recall',\n        'precision': 'precision',\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    \n    print(\"\\nüîç [Expert] Calcul des m√©triques sur chaque fold (parall√©lis√©)\")\n    cv_results = cross_validate(\n        pipeline, X, y,\n        cv=cv,\n        scoring=scoring,\n        return_train_score=True,\n        n_jobs=-1  # Utilisation de tous les c≈ìurs disponibles\n    )\n    \n    results_df = pd.DataFrame(cv_results)\n    print(f\"\\nüìä R√©sultats moyens :\\n{results_df.mean().to_string()}\")\n    \n    return results_df\n\n# 5. üìä ANALYSE DES SEUILS DE D√âCISION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üìä OPTIMISATION DES SEUILS\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse du trade-off pr√©cision/rappel par seuil\")\nprint(\"üë∂ [Enfant] On cherche le meilleur r√©glage pour notre machine\")\nprint(\"\\nüí° M√©thodes alternatives :\")\nprint(\"- Optimisation directe du seuil avec BayesSearchCV\")\nprint(\"- Maximisation du score m√©tier personnalis√©\")\nprint(\"- Courbe ROC Youden pour seuil optimal\")\n\ndef analyze_decision_threshold(y_true, y_probs):\n    \"\"\"Analyse approfondie des seuils de classification\"\"\"\n    print(\"\\nüîç [Expert] Calcul des m√©triques pour 20 seuils diff√©rents\")\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    \n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    \n    metrics_df = pd.DataFrame(metrics)\n    \n    print(\"\\nüîç [Expert] Visualisation interactive des m√©triques\")\n    plt.figure(figsize=(10, 6))\n    metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'])\n    plt.axvline(x=0.5, color='gray', linestyle='--', label='Seuil par d√©faut')\n    plt.title(\"Performance par seuil de d√©cision\")\n    plt.xlabel(\"Seuil de classification\")\n    plt.ylabel(\"Score\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \n    return metrics_df\n\n# 6. üí∞ ANALYSE CO√õT DES ERREURS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üí∞ IMPACT M√âTIER\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse co√ªt/b√©n√©fice des diff√©rents types d'erreurs\")\nprint(\"üë∂ [Enfant] On calcule combien co√ªtent les erreurs de notre machine\")\nprint(\"\\nüí° Approches compl√©mentaires :\")\nprint(\"- Analyse ROI complet\")\nprint(\"- Matrice de confusion pond√©r√©e\")\nprint(\"- Optimisation directe du co√ªt m√©tier\")\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions\"\"\"\n    print(\"\\nüîç [Expert] Extraction de la matrice de confusion\")\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    \n    print(\"\\nüîç [Expert] D√©finition des co√ªts m√©tier (√† adapter)\")\n    cost_matrix = {\n        'fn_cost': 1000,  # Co√ªt d'un faux n√©gatif (cas manqu√©)\n        'fp_cost': 200,    # Co√ªt d'un faux positif (surdiagnostic)\n        'tp_gain': 500,    # Gain d'un vrai positif (bonne prise en charge)\n        'tn_gain': 50      # Gain d'un vrai n√©gatif (pas d'intervention inutile)\n    }\n    \n    total_cost = (fn * cost_matrix['fn_cost'] + \n                 fp * cost_matrix['fp_cost'] - \n                 tp * cost_matrix['tp_gain'] - \n                 tn * cost_matrix['tn_gain'])\n    \n    print(\"\\nüí∞ Bilan des co√ªts :\")\n    print(f\"- Faux n√©gatifs (FN): {fn} √ó {cost_matrix['fn_cost']} = {fn * cost_matrix['fn_cost']}\")\n    print(f\"- Faux positifs (FP): {fp} √ó {cost_matrix['fp_cost']} = {fp * cost_matrix['fp_cost']}\")\n    print(f\"- Vrais positifs (TP): {tp} √ó -{cost_matrix['tp_gain']} = {-tp * cost_matrix['tp_gain']}\")\n    print(f\"- Vrais n√©gatifs (TN): {tn} √ó -{cost_matrix['tn_gain']} = {-tn * cost_matrix['tn_gain']}\")\n    print(f\"‚Üí CO√õT TOTAL: {total_cost}\")\n    \n    return {\n        'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp},\n        'cost_analysis': cost_matrix,\n        'total_cost': total_cost\n    }\n\n# 7. üöÄ OPTIMISATION AVEC CHECKPOINTS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üöÄ OPTIMISATION DES HYPERPARAM√àTRES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Configuration d'Optuna avec sauvegarde des checkpoints\")\nprint(\"üë∂ [Enfant] On r√®gle finement notre machine en gardant des sauvegardes\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- GridSearchCV pour un espace de recherche petit\")\nprint(\"- Ray Tune pour le scale-out\")\nprint(\"- Hyperopt pour des algorithmes d'optimisation avanc√©s\")\n\ndef create_study_with_checkpoints(study_name, storage_name=None):\n    \"\"\"Cr√©e ou charge une √©tude Optuna\"\"\"\n    CHECKPOINT_DIR = \"optuna_checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{study_name}.pkl\")\n    \n    if os.path.exists(CHECKPOINT_PATH):\n        print(\"\\nüîç [Expert] Chargement d'une √©tude existante\")\n        study = joblib.load(CHECKPOINT_PATH)\n        print(f\"üìö Essais pr√©c√©dents charg√©s: {len(study.trials)}\")\n    else:\n        print(\"\\nüîç [Expert] Cr√©ation d'une nouvelle √©tude\")\n        study = optuna.create_study(\n            direction='maximize',\n            sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n            study_name=study_name,\n            storage=storage_name\n        )\n    \n    return study, CHECKPOINT_PATH\n\ndef objective(trial, X, y):\n    \"\"\"Fonction objective pour Optuna\"\"\"\n    print(f\"\\nüîç [Expert] Essai {trial.number} - Exploration des hyperparam√®tres\")\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'random_state': RANDOM_STATE\n    }\n    \n    pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**params))\n    ])\n    \n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    # Contrainte m√©tier sur le recall minimum\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall trop faible\")\n        raise optuna.TrialPruned()\n    \n    return mean_f2\n\n# 8. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. üè≠ PIPELINE FINAL\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Entra√Ænement et versionning du mod√®le avec m√©tadonn√©es\")\nprint(\"üë∂ [Enfant] On sauvegarde notre meilleure machine avec son mode d'emploi\")\nprint(\"\\nüí° Bonnes pratiques :\")\nprint(\"- MLflow pour le suivi complet\")\nprint(\"- ONNX pour l'interop√©rabilit√©\")\nprint(\"- DVC pour la gestion des versions\")\n\ndef train_final_model(X_train, y_train, best_params, version=\"1.0.0\"):\n    \"\"\"Entra√Æne et sauvegarde le mod√®le final\"\"\"\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le final (version {version})\")\n    \n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE))\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    # Sauvegarde du mod√®le\n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    print(f\"üíæ Mod√®le sauvegard√©: {model_path}\")\n    \n    # M√©tadonn√©es\n    metadata = {\n        \"model_version\": version,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features\": list(X_train.columns),\n        \"best_params\": best_params,\n        \"metrics\": {\n            \"cv_folds\": 5,\n            \"optimization_metric\": \"f2_score\",\n            \"recall_constraint\": eval_config.alert_thresholds['recall']\n        },\n        \"data_schema\": {\n            \"numeric_features\": numeric_features,\n            \"categorical_features\": categorical_features\n        }\n    }\n    \n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4)\n    print(f\"üìù M√©tadonn√©es sauvegard√©es: {metadata_path}\")\n    \n    return final_pipeline, MODEL_DIR\n\n# 9. üìä VISUALISATIONS CLINIQUES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. üìä ANALYSE DES PERFORMANCES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Visualisations professionnelles pour l'analyse\")\nprint(\"üë∂ [Enfant] On dessine des graphiques pour comprendre notre machine\")\nprint(\"\\nüí° Visualisations compl√©mentaires :\")\nprint(\"- Diagramme de d√©cision clinique\")\nprint(\"- Analyse des erreurs par sous-groupes\")\nprint(\"- Courbes de calibration par √¢ge\")\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Courbe de calibration\"\"\"\n    print(\"\\nüîç [Expert] V√©rification de la calibration des probabilit√©s\")\n    prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n    plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n    plt.xlabel(\"Probabilit√© pr√©dite\")\n    plt.ylabel(\"Probabilit√© r√©elle\")\n    plt.title(\"Courbe de calibration\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# =============================================================================\n# 10. üèÅ PIPELINE COMPLET CORRIG√â\n# =============================================================================\n\n# üîç [Expert] Affichage format√© du titre de section avec s√©parateurs visuels\n# üë∂ [Enfant] On √©crit un joli titre pour montrer qu'on commence quelque chose d'important\n# üí° Alternatives : logger.info(), rich.print(), ou simple print() sans formatage\nprint(\"\\n\" + \"=\"*80)\n\n# üîç [Expert] Titre descriptif de la section avec emoji pour identification rapide\n# üë∂ [Enfant] On dit ce qu'on va faire : un pipeline qui sait se r√©parer tout seul\n# üí° Alternatives : logging.info(), f-string formatting, ou titre sans emoji\nprint(\"10. üèÅ PIPELINE COMPLET AVEC GESTION D'ERREURS\")\n\n# üîç [Expert] Fermeture du cadre visuel pour d√©limiter la section\n# üë∂ [Enfant] On ferme notre jolie bo√Æte de titre\n# üí° Alternatives : print(\"-\"*80), logging separator, ou pas de s√©parateur\nprint(\"=\"*80)\n\n# üîç [Expert] Documentation technique pour experts avec contexte d'orchestration\n# üë∂ [Enfant] On explique aux grands ce qu'on fait : on organise tout comme un chef d'orchestre\n# üí° Alternatives : docstring, commentaire inline, ou documentation externe\nprint(\"\\nüîç [Expert] Orchestration robuste avec gestion des erreurs et logging\")\n\n# üîç [Expert] Vulgarisation pour faciliter la compr√©hension m√©tier\n# üë∂ [Enfant] On dit aux petits : on pr√©pare nos outils pour r√©parer si √ßa casse\n# üí° Alternatives : pas d'explication enfant, explication technique uniquement\nprint(\"üë∂ [Enfant] On lance tout en √©tant pr√™t √† r√©parer si quelque chose casse\")\n\n# üîç [Expert] Liste des bonnes pratiques pour guidance architecturale\n# üë∂ [Enfant] On dit les r√®gles importantes √† suivre\n# üí° Alternatives : documentation s√©par√©e, configuration YAML, ou constantes\nprint(\"\\nüí° Bonnes pratiques :\")\n\n# üîç [Expert] Premi√®re bonne pratique : observabilit√© et tra√ßabilit√©\n# üë∂ [Enfant] On note tout ce qui se passe comme dans un journal\n# üí° Alternatives : logging.basicConfig(), structlog, ou prints simples\nprint(\"- Journalisation d√©taill√©e\")\n\n# üîç [Expert] Deuxi√®me bonne pratique : gestion fine des exceptions\n# üë∂ [Enfant] On attrape les erreurs une par une pour mieux les comprendre\n# üí° Alternatives : try-catch global, assert statements, ou pas de gestion\nprint(\"- Gestion des erreurs granulaires\")\n\n# üîç [Expert] Troisi√®me bonne pratique : validation defensive programming\n# üë∂ [Enfant] On v√©rifie que nos donn√©es sont correctes avant de commencer\n# üí° Alternatives : schema validation (Pydantic), unit tests, ou pas de validation\nprint(\"- Validation des donn√©es en entr√©e\")\n\n# üîç [Expert] D√©finition de fonction avec signature claire et documentation\n# üë∂ [Enfant] On cr√©e une fonction qui fait tout le travail d'un coup\n# üí° Alternatives : classe Pipeline, script s√©quentiel, ou notebook cells\ndef run_full_pipeline(X, y, n_trials=50):\n    \"\"\"Ex√©cute l'ensemble du pipeline avec gestion robuste des erreurs\"\"\"\n    \n    # üîç [Expert] Bloc try principal pour capture globale des exceptions critiques\n    # üë∂ [Enfant] On met tout dans une bo√Æte sp√©ciale qui attrape les erreurs\n    # üí° Alternatives : multiple try-catch, decorateur @exception_handler, ou pas de gestion\n    try:\n        # 1. Validation des donn√©es d'entr√©e\n        \n        # üîç [Expert] Log de d√©but d'√©tape pour tra√ßabilit√© du flux\n        # üë∂ [Enfant] On dit qu'on commence √† v√©rifier nos donn√©es\n        # üí° Alternatives : logger.debug(), logging.info(), ou commentaire seulement\n        print(\"\\nüîç [Expert] Validation des donn√©es d'entr√©e\")\n        \n        # üîç [Expert] Conversion d√©fensive en DataFrame avec copie pour √©viter side-effects\n        # üë∂ [Enfant] On transforme nos donn√©es en tableau propre et on en fait une copie\n        # üí° Alternatives : pd.DataFrame(X), np.array(X), ou validation Pydantic\n        X = pd.DataFrame(X).copy()\n        \n        # üîç [Expert] Conversion en Series avec copie pour coh√©rence de type\n        # üë∂ [Enfant] On met nos r√©ponses dans une liste sp√©ciale et on la copie\n        # üí° Alternatives : np.array(y), list(y), ou validation avec assert\n        y = pd.Series(y).copy()\n        \n        # 2. S√©paration des donn√©es\n        \n        # üîç [Expert] Log de l'√©tape de split avec m√©thode de stratification\n        # üë∂ [Enfant] On dit qu'on va s√©parer nos donn√©es en deux tas √©quilibr√©s\n        # üí° Alternatives : logging.info(), pas de log, ou log plus d√©taill√©\n        print(\"\\nüîç [Expert] S√©paration train/test stratifi√©e\")\n        \n        # üîç [Expert] Bloc try sp√©cifique pour gestion granulaire des erreurs de split\n        # üë∂ [Enfant] On essaie de s√©parer les donn√©es et on se pr√©pare si √ßa marche pas\n        # üí° Alternatives : validation pr√©alable, assert, ou pas de gestion d'erreur\n        try:\n            # üîç [Expert] Split stratifi√© avec param√®tres reproductibles et ratio standard\n            # üë∂ [Enfant] On s√©pare nos donn√©es : 80% pour apprendre, 20% pour tester\n            # üí° Alternatives : train_test_split simple, StratifiedKFold, ou split manuel\n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, \n                test_size=0.2, \n                random_state=RANDOM_STATE, \n                stratify=y\n            )\n            \n            # üîç [Expert] Confirmation du split avec statistiques de taille\n            # üë∂ [Enfant] On dit combien on a dans chaque tas pour v√©rifier\n            # üí° Alternatives : logging.info(), assert sur les tailles, ou pas de v√©rification\n            print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n            \n        # üîç [Expert] Capture sp√©cifique des erreurs de s√©paration avec context\n        # üë∂ [Enfant] Si la s√©paration marche pas, on attrape l'erreur et on dit pourquoi\n        # üí° Alternatives : except ValueError, except sp√©cifiques, ou logging.exception()\n        except Exception as e:\n            # üîç [Expert] Log d'erreur avec message explicite pour debugging\n            # üë∂ [Enfant] On √©crit dans notre journal que quelque chose a mal tourn√©\n            # üí° Alternatives : logging.error(), sys.stderr.write(), ou raise direct\n            print(f\"‚ùå Erreur lors de la s√©paration des donn√©es: {str(e)}\")\n            \n            # üîç [Expert] Re-raise pour propager l'erreur vers le niveau sup√©rieur\n            # üë∂ [Enfant] On lance l'erreur plus haut pour que quelqu'un d'autre s'en occupe\n            # üí° Alternatives : return None, sys.exit(), ou custom exception\n            raise\n\n        # 3. Optimisation hyperparam√©trique\n        \n        # üîç [Expert] Log du d√©but d'optimisation avec mention de la persistance\n        # üë∂ [Enfant] On dit qu'on va chercher les meilleurs r√©glages et les sauvegarder\n        # üí° Alternatives : logging.info(), print simple, ou pas de log\n        print(\"\\nüîç [Expert] Optimisation hyperparam√©trique avec checkpoints\")\n        \n        # üîç [Expert] Cr√©ation d'√©tude Optuna avec syst√®me de sauvegarde\n        # üë∂ [Enfant] On pr√©pare notre syst√®me d'exp√©rimentation avec sauvegarde automatique\n        # üí° Alternatives : GridSearchCV, RandomizedSearchCV, ou hyperopt\n        study, checkpoint_path = create_study_with_checkpoints(\"frailty_detection\")\n        \n        # üîç [Expert] D√©finition de callback pour sauvegarde p√©riodique des r√©sultats\n        # üë∂ [Enfant] On cr√©e une fonction qui sauvegarde automatiquement nos progr√®s\n        # üí° Alternatives : sauvegarde manuelle, pas de checkpoint, ou callback Optuna\n        def save_checkpoint(study, trial):\n            # üîç [Expert] Bloc try pour gestion des erreurs de sauvegarde non-critiques\n            # üë∂ [Enfant] On essaie de sauvegarder, mais si √ßa marche pas, on continue quand m√™me\n            # üí° Alternatives : pas de gestion d'erreur, logging.exception(), ou validation pr√©alable\n            try:\n                # üîç [Expert] S√©rialisation de l'objet study avec joblib pour performance\n                # üë∂ [Enfant] On sauvegarde tous nos essais dans un fichier\n                # üí° Alternatives : pickle.dump(), JSON export, ou base de donn√©es\n                joblib.dump(study, checkpoint_path)\n                \n                # üîç [Expert] Confirmation de sauvegarde avec num√©ro d'essai pour tra√ßabilit√©\n                # üë∂ [Enfant] On dit qu'on a bien sauvegard√© et √† quel essai on en est\n                # üí° Alternatives : logging.debug(), pas de confirmation, ou log plus d√©taill√©\n                print(f\"üíæ Checkpoint sauvegard√© (essai {trial.number})\")\n                \n            # üîç [Expert] Gestion des erreurs de sauvegarde non-bloquantes\n            # üë∂ [Enfant] Si on arrive pas √† sauvegarder, on le dit mais on arr√™te pas tout\n            # üí° Alternatives : logging.warning(), raise, ou ignore silencieusement\n            except Exception as e:\n                # üîç [Expert] Warning plut√¥t qu'erreur car la sauvegarde n'est pas critique\n                # üë∂ [Enfant] On dit qu'il y a eu un petit probl√®me mais c'est pas grave\n                # üí° Alternatives : logging.error(), print simple, ou pas de message\n                print(f\"‚ö†Ô∏è Erreur sauvegarde checkpoint: {str(e)}\")\n\n        # üîç [Expert] Bloc try pour l'optimisation avec gestion des interruptions\n        # üë∂ [Enfant] On lance notre recherche des meilleurs r√©glages en se pr√©parant aux probl√®mes\n        # üí° Alternatives : pas de gestion d'erreur, validation pr√©alable, ou timeout\n        try:\n            # üîç [Expert] Log informatif avec nombre d'essais pour suivi de progression\n            # üë∂ [Enfant] On dit combien d'essais on va faire pour trouver les meilleurs r√©glages\n            # üí° Alternatives : logging.info(), progress bar personnalis√©e, ou pas de log\n            print(f\"\\nüîç Lancement de l'optimisation ({n_trials} essais)\")\n            \n            # üîç [Expert] Lancement de l'optimisation Optuna avec callback et progress bar\n            # üë∂ [Enfant] On lance la recherche automatique des meilleurs r√©glages\n            # üí° Alternatives : GridSearchCV, RandomizedSearchCV, ou recherche manuelle\n            study.optimize(\n                lambda trial: objective(trial, X_train, y_train),\n                n_trials=n_trials,\n                callbacks=[save_checkpoint],\n                show_progress_bar=True\n            )\n            \n        # üîç [Expert] Capture des erreurs d'optimisation avec propagation\n        # üë∂ [Enfant] Si la recherche plante, on attrape l'erreur et on l'explique\n        # üí° Alternatives : except specific errors, logging.exception(), ou recovery automatique\n        except Exception as e:\n            # üîç [Expert] Log d'erreur sp√©cifique √† l'optimisation pour debugging\n            # üë∂ [Enfant] On √©crit que la recherche a plant√© et pourquoi\n            # üí° Alternatives : logging.error(), sys.stderr, ou message plus d√©taill√©\n            print(f\"‚ùå Erreur lors de l'optimisation: {str(e)}\")\n            \n            # üîç [Expert] Re-raise car l'optimisation est critique pour la suite\n            # üë∂ [Enfant] On remonte l'erreur car sans optimisation, on peut pas continuer\n            # üí° Alternatives : return default params, continue avec params par d√©faut\n            raise\n\n        # üîç [Expert] Validation de la pr√©sence de r√©sultats avant de continuer\n        # üë∂ [Enfant] On v√©rifie qu'on a trouv√© au moins un bon r√©glage\n        # üí° Alternatives : assert, try-catch sur best_params, ou valeur par d√©faut\n        if not study.best_trial:\n            # üîç [Expert] Erreur explicite si aucun essai n'a r√©ussi\n            # üë∂ [Enfant] Si on a trouv√© aucun bon r√©glage, on dit qu'il faut regarder les erreurs\n            # üí° Alternatives : ValueError custom, logging.critical(), ou valeurs par d√©faut\n            raise ValueError(\"Aucun essai valide - v√©rifiez les logs des erreurs\")\n\n        # 4. Entra√Ænement du mod√®le final\n        \n        # üîç [Expert] Log du d√©but d'entra√Ænement avec mention des hyperparam√®tres optimaux\n        # üë∂ [Enfant] On dit qu'on va entra√Æner notre mod√®le final avec les meilleurs r√©glages\n        # üí° Alternatives : logging.info(), print d√©taill√©, ou pas de log\n        print(\"\\nüîç [Expert] Entra√Ænement du mod√®le final avec les meilleurs param√®tres\")\n        \n        # üîç [Expert] Bloc try pour gestion des erreurs d'entra√Ænement\n        # üë∂ [Enfant] On essaie d'entra√Æner le mod√®le et on se pr√©pare si √ßa marche pas\n        # üí° Alternatives : validation pr√©alable, pas de gestion, ou retry automatique\n        try:\n            # üîç [Expert] Extraction des meilleurs hyperparam√®tres de l'√©tude\n            # üë∂ [Enfant] On r√©cup√®re les meilleurs r√©glages qu'on a trouv√©s\n            # üí° Alternatives : study.best_params.copy(), dict(study.best_params)\n            best_params = study.best_params\n            \n            # üîç [Expert] Affichage des hyperparam√®tres pour tra√ßabilit√©\n            # üë∂ [Enfant] On montre quels sont les meilleurs r√©glages trouv√©s\n            # üí° Alternatives : logging.info(), pprint(), ou JSON format\n            print(f\"‚öôÔ∏è Meilleurs param√®tres: {best_params}\")\n            \n            # üîç [Expert] Entra√Ænement final avec sauvegarde automatique du mod√®le\n            # üë∂ [Enfant] On entra√Æne notre mod√®le final et on le sauvegarde\n            # üí° Alternatives : fit() simple, cross-validation, ou ensemble methods\n            model, model_dir = train_final_model(X_train, y_train, best_params)\n            \n            # üîç [Expert] Confirmation d'entra√Ænement avec chemin de sauvegarde\n            # üë∂ [Enfant] On dit que notre mod√®le est pr√™t et o√π on l'a rang√©\n            # üí° Alternatives : logging.info(), pas de confirmation, ou m√©tadonn√©es d√©taill√©es\n            print(f\"‚úÖ Mod√®le final entra√Æn√© et sauvegard√© dans {model_dir}\")\n            \n        # üîç [Expert] Gestion des erreurs d'entra√Ænement avec contexte sp√©cifique\n        # üë∂ [Enfant] Si l'entra√Ænement plante, on attrape l'erreur et on explique\n        # üí° Alternatives : except sp√©cifiques, logging.exception(), ou fallback model\n        except Exception as e:\n            # üîç [Expert] Log d'erreur sp√©cifique √† l'entra√Ænement pour debugging\n            # üë∂ [Enfant] On √©crit que l'entra√Ænement a plant√© et pourquoi\n            # üí° Alternatives : logging.error(), structured logging, ou details techniques\n            print(f\"‚ùå Erreur lors de l'entra√Ænement final: {str(e)}\")\n            \n            # üîç [Expert] Re-raise car le mod√®le est n√©cessaire pour l'√©valuation\n            # üë∂ [Enfant] On remonte l'erreur car sans mod√®le, on peut pas continuer\n            # üí° Alternatives : return baseline model, continue sans √©valuation\n            raise\n\n        # 5. √âvaluation finale\n        \n        # üîç [Expert] Log du d√©but d'√©valuation sur le jeu de test\n        # üë∂ [Enfant] On dit qu'on va tester notre mod√®le sur des donn√©es qu'il a jamais vues\n        # üí° Alternatives : logging.info(), print d√©taill√©, ou pas de log\n        print(\"\\nüîç [Expert] √âvaluation compl√®te sur le jeu de test\")\n        \n        # üîç [Expert] Bloc try pour gestion des erreurs d'√©valuation non-critiques\n        # üë∂ [Enfant] On essaie de tester notre mod√®le, si √ßa marche pas compl√®tement, c'est pas grave\n        # üí° Alternatives : pas de gestion, validation pr√©alable, ou √©valuation simplifi√©e\n        try:\n            # Pr√©dictions\n            \n            # üîç [Expert] G√©n√©ration des probabilit√©s pour m√©triques avanc√©es (ROC, calibration)\n            # üë∂ [Enfant] On demande au mod√®le de dire √† quel point il est s√ªr de ses r√©ponses\n            # üí° Alternatives : decision_function(), predict() seulement, ou probabilit√©s calibr√©es\n            y_probs = model.predict_proba(X_test)[:, 1]\n            \n            # üîç [Expert] G√©n√©ration des pr√©dictions binaires pour m√©triques classiques\n            # üë∂ [Enfant] On demande au mod√®le de donner ses r√©ponses finales oui/non\n            # üí° Alternatives : np.where(y_probs > threshold), argmax(), ou seuil optimis√©\n            y_pred = model.predict(X_test)\n            \n            # Rapport de classification\n            \n            # üîç [Expert] Log descriptif pour le rapport de classification d√©taill√©\n            # üë∂ [Enfant] On va montrer un tableau avec tous les scores de notre mod√®le\n            # üí° Alternatives : logging.info(), print simple, ou pas de log\n            print(\"\\nüìä Rapport de classification :\")\n            \n            # üîç [Expert] G√©n√©ration du rapport de classification avec noms de classes explicites\n            # üë∂ [Enfant] On affiche un joli tableau qui dit si notre mod√®le est bon\n            # üí° Alternatives : confusion_matrix(), m√©triques individuelles, ou custom report\n            print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n            \n            # Analyses avanc√©es\n            \n            # üîç [Expert] Log pour l'analyse des seuils de d√©cision optimaux\n            # üë∂ [Enfant] On va chercher le meilleur point pour dire oui ou non\n            # üí° Alternatives : pas de log, logging.debug(), ou description d√©taill√©e\n            print(\"\\nüîç Analyse des seuils de d√©cision\")\n            \n            # üîç [Expert] Analyse des seuils pour optimisation du trade-off pr√©cision/rappel\n            # üë∂ [Enfant] On teste plein de fa√ßons de dire oui/non pour trouver la meilleure\n            # üí° Alternatives : seuil fixe 0.5, courbe ROC, ou optimisation m√©tier\n            metrics_df = analyze_decision_threshold(y_test, y_probs)\n            \n            # üîç [Expert] Log pour l'analyse des co√ªts m√©tier\n            # üë∂ [Enfant] On va calculer combien √ßa co√ªte si on se trompe\n            # üí° Alternatives : pas de log, logging.info(), ou description m√©tier\n            print(\"\\nüí∞ Analyse des co√ªts\")\n            \n            # üîç [Expert] Analyse co√ªt-b√©n√©fice pour aide √† la d√©cision m√©tier\n            # üë∂ [Enfant] On calcule si notre mod√®le fait gagner ou perdre de l'argent\n            # üí° Alternatives : pas d'analyse co√ªt, m√©triques techniques seulement\n            cost_results = cost_analysis(y_test, y_pred)\n            \n            # üîç [Expert] Log pour g√©n√©ration des visualisations\n            # üë∂ [Enfant] On va faire de jolis dessins pour montrer si notre mod√®le est bon\n            # üí° Alternatives : pas de log, logging.debug(), ou description des plots\n            print(\"\\nüìà Visualisations\")\n            \n            # üîç [Expert] G√©n√©ration de graphiques de calibration pour fiabilit√© des probabilit√©s\n            # üë∂ [Enfant] On dessine un graphique pour voir si notre mod√®le dit la v√©rit√© sur ses probabilit√©s\n            # üí° Alternatives : ROC curve, confusion matrix, ou learning curves\n            plot_calibration(y_test, y_probs)\n            \n        # üîç [Expert] Gestion des erreurs d'√©valuation non-critiques avec warning\n        # üë∂ [Enfant] Si on arrive pas √† tout tester, on le dit mais on continue\n        # üí° Alternatives : logging.warning(), continue silencieusement, ou raise\n        except Exception as e:\n            # üîç [Expert] Warning car l'√©valuation est importante mais pas critique\n            # üë∂ [Enfant] On dit qu'il y a eu un probl√®me pour tester mais c'est pas trop grave\n            # üí° Alternatives : logging.error(), print simple, ou ignore\n            print(f\"‚ö†Ô∏è Erreur lors de l'√©valuation: {str(e)}\")\n            \n            # üîç [Expert] Re-raise pour signaler l'importance de l'√©valuation\n            # üë∂ [Enfant] On remonte quand m√™me l'erreur car c'est important de savoir si le mod√®le marche\n            # üí° Alternatives : continue, return partial results, ou default metrics\n            raise\n\n        # 6. Sauvegarde du rapport final\n        \n        # üîç [Expert] Log pour g√©n√©ration du rapport de synth√®se\n        # üë∂ [Enfant] On va faire un r√©sum√© de tout ce qu'on a fait et trouv√©\n        # üí° Alternatives : logging.info(), pas de log, ou description d√©taill√©e\n        print(\"\\nüîç [Expert] G√©n√©ration du rapport final\")\n        \n        # üîç [Expert] Bloc try pour gestion des erreurs de rapport non-critiques\n        # üë∂ [Enfant] On essaie de faire notre r√©sum√©, si √ßa marche pas c'est pas tr√®s grave\n        # üí° Alternatives : pas de gestion, validation pr√©alable, ou rapport simplifi√©\n        try:\n            # üîç [Expert] Construction du dictionnaire de rapport avec toutes les m√©triques importantes\n            # üë∂ [Enfant] On met tous nos r√©sultats dans une grande bo√Æte bien organis√©e\n            # üí° Alternatives : dataclass, JSON schema, ou objet custom\n            report = {\n                \"best_params\": best_params,\n                \"test_metrics\": {\n                    \"roc_auc\": roc_auc_score(y_test, y_probs),\n                    \"f2_score\": fbeta_score(y_test, y_pred, beta=2),\n                    \"recall\": recall_score(y_test, y_pred),\n                    \"precision\": precision_score(y_test, y_pred),\n                },\n                \"cost_analysis\": cost_results,\n                \"optimal_threshold\": metrics_df.loc[metrics_df['f2'].idxmax()].to_dict(),\n                \"data_stats\": {\n                    \"train_size\": len(X_train),\n                    \"test_size\": len(X_test),\n                    \"class_balance\": dict(y.value_counts(normalize=True))\n                }\n            }\n            \n            # üîç [Expert] D√©finition du chemin de sauvegarde dans le r√©pertoire du mod√®le\n            # üë∂ [Enfant] On choisit o√π ranger notre r√©sum√© avec le mod√®le\n            # üí° Alternatives : r√©pertoire s√©par√©, timestamp dans nom, ou base de donn√©es\n            report_path = os.path.join(model_dir, 'final_report.json')\n            \n            # üîç [Expert] Sauvegarde du rapport en JSON pour lisibilit√© et interop√©rabilit√©\n            # üë∂ [Enfant] On √©crit notre r√©sum√© dans un fichier que tout le monde peut lire\n            # üí° Alternatives : pickle, YAML, CSV, ou base de donn√©es\n            with open(report_path, 'w') as f:\n                json.dump(report, f, indent=4)\n                \n            # üîç [Expert] Message de succ√®s final avec localisation du rapport\n            # üë∂ [Enfant] On dit qu'on a fini et o√π trouver notre r√©sum√©\n            # üí° Alternatives : logging.info(), return path, ou print simple\n            print(f\"\\n‚úÖ Pipeline termin√© avec succ√®s! Rapport sauvegard√©: {report_path}\")\n            \n            # üîç [Expert] Retour du mod√®le et rapport pour utilisation ult√©rieure\n            # üë∂ [Enfant] On donne le mod√®le et le r√©sum√© √† celui qui nous a demand√© de travailler\n            # üí° Alternatives : save et return path, return dict, ou pas de retour\n            return model, report\n            \n        # üîç [Expert] Gestion des erreurs de g√©n√©ration de rapport avec propagation\n        # üë∂ [Enfant] Si on arrive pas √† faire le r√©sum√©, on attrape l'erreur\n        # üí° Alternatives : logging.error(), continue sans rapport, ou rapport minimal\n        except Exception as e:\n            # üîç [Expert] Log d'erreur car le rapport est important pour la tra√ßabilit√©\n            # üë∂ [Enfant] On dit qu'on a pas r√©ussi √† faire le r√©sum√© et pourquoi\n            # üí° Alternatives : logging.error(), warning seulement, ou details techniques\n            print(f\"‚ùå Erreur lors de la g√©n√©ration du rapport: {str(e)}\")\n            \n            # üîç [Expert] Re-raise car le rapport fait partie des livrables attendus\n            # üë∂ [Enfant] On remonte l'erreur car c'est important d'avoir le r√©sum√©\n            # üí° Alternatives : return mod√®le seulement, continue, ou rapport par d√©faut\n            raise\n\n    # üîç [Expert] Gestion globale des erreurs critiques du pipeline avec logging d√©taill√©\n    # üë∂ [Enfant] Si quelque chose de tr√®s grave arrive, on l'attrape ici\n    # üí° Alternatives : except sp√©cifiques, sys.excepthook, ou crash avec traceback\n    except Exception as e:\n        # üîç [Expert] Header d'erreur critique pour visibilit√© maximale\n        # üë∂ [Enfant] On √©crit en gros que quelque chose de tr√®s grave s'est pass√©\n        # üí° Alternatives : logging.critical(), sys.stderr, ou exception custom\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        \n        # üîç [Expert] Affichage du message d'erreur pour debugging imm√©diat\n        # üë∂ [Enfant] On dit exactement ce qui s'est mal pass√©\n        # üí° Alternatives : logging.exception(), traceback.print_exc(), ou structured logging\n        print(f\"Message d'erreur: {str(e)}\")\n        \n        # üîç [Expert] Section de conseils de d√©bogage pour r√©solution rapide\n        # üë∂ [Enfant] On donne des conseils pour r√©parer le probl√®me\n        # üí° Alternatives : documentation link, automated diagnostics, ou pas de conseils\n        print(\"\\nüîç [Expert] Conseils de d√©bogage:\")\n        \n        # üîç [Expert] Premier conseil : validation des donn√©es d'entr√©e\n        # üë∂ [Enfant] Premier truc √† v√©rifier : est-ce que les donn√©es sont correctes ?\n        # üí° Alternatives : automated validation, schema check, ou unit tests\n        print(\"- V√©rifier la coh√©rence des donn√©es d'entr√©e\")\n        \n        # üîç [Expert] Deuxi√®me conseil : tests unitaires des composants\n        # üë∂ [Enfant] Deuxi√®me truc : tester chaque morceau un par un\n        # üí° Alternatives : integration tests, debugging step by step, ou profiling\n        print(\"- Tester chaque composant du pipeline s√©par√©ment\")\n        \n        # üîç [Expert] Troisi√®me conseil : activation du mode debug\n        # üë∂ [Enfant] Troisi√®me truc : mettre le mode d√©taill√© pour voir plus d'informations\n        # üí° Alternatives : verbose logging, pdb debugger, ou traceback complet\n        print(\"- Activer les logs d√©taill√©s avec error_score='raise'\")\n        \n        # üîç [Expert] Re-raise de l'exception pour propagation vers l'appelant\n        # üë∂ [Enfant] On relance l'erreur pour que celui qui nous appelle puisse la g√©rer\n        # üí° Alternatives : sys.exit(), return None, ou exception wrapping\n        raise\n\n# Correction du FeatureEngineer\n\n# üîç [Expert] D√©finition de classe avec h√©ritage sklearn pour int√©gration pipeline\n# üë∂ [Enfant] On cr√©e une classe sp√©ciale qui sait transformer nos donn√©es\n# üí° Alternatives : function\n# üîç [Expert] D√©finition de classe avec h√©ritage sklearn pour int√©gration pipeline\n# üë∂ [Enfant] On cr√©e une classe sp√©ciale qui sait transformer nos donn√©es\n# üí° Alternatives : fonction simple, FunctionTransformer, ou classe custom sans h√©ritage\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n   \"\"\"Version corrig√©e du FeatureEngineer\"\"\"\n   \n   # üîç [Expert] M√©thode fit obligatoire pour compatibilit√© sklearn (ici pas d'apprentissage)\n   # üë∂ [Enfant] On cr√©e une fonction qui apprend, mais ici on apprend rien\n   # üí° Alternatives : fit with parameters, stateful fitting, ou NotImplementedError\n   def fit(self, X, y=None):\n       # üîç [Expert] Retour de self pour cha√Ænage des m√©thodes sklearn\n       # üë∂ [Enfant] On se rend nous-m√™me pour pouvoir √™tre utilis√© apr√®s\n       # üí° Alternatives : return None, store fitted state, ou validation\n       return self\n\n   # üîç [Expert] M√©thode transform principale pour cr√©ation de features\n   # üë∂ [Enfant] On cr√©e une fonction qui transforme nos donn√©es en ajoutant de nouvelles colonnes\n   # üí° Alternatives : fit_transform only, __call__, ou multiple transform methods\n   def transform(self, X):\n       \"\"\"Version robuste avec v√©rification des colonnes\"\"\"\n       \n       # üîç [Expert] Copie d√©fensive pour √©viter la modification des donn√©es originales\n       # üë∂ [Enfant] On fait une copie de nos donn√©es pour pas ab√Æmer les originales\n       # üí° Alternatives : X.copy(deep=True), inplace=True parameter, ou pas de copie\n       X = X.copy()\n       \n       # V√©rification des colonnes n√©cessaires\n       \n       # üîç [Expert] D√©finition de l'ensemble des colonnes requises pour les transformations\n       # üë∂ [Enfant] On fait une liste des colonnes qu'on a absolument besoin\n       # üí° Alternatives : list, tuple, ou validation schema avec Pydantic\n       required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n       \n       # üîç [Expert] Validation de pr√©sence des colonnes avec op√©ration d'ensemble\n       # üë∂ [Enfant] On v√©rifie qu'on a bien toutes les colonnes n√©cessaires\n       # üí° Alternatives : all(col in X.columns for col in required_cols), try-except, ou assert\n       if not required_cols.issubset(X.columns):\n           # üîç [Expert] Calcul des colonnes manquantes pour message d'erreur informatif\n           # üë∂ [Enfant] On trouve quelles colonnes il nous manque pour dire lesquelles\n           # üí° Alternatives : list comprehension, set intersection, ou message g√©n√©rique\n           missing = required_cols - set(X.columns)\n           \n           # üîç [Expert] Lev√©e d'exception avec message explicite pour debugging\n           # üë∂ [Enfant] On dit exactement quelles colonnes il manque\n           # üí° Alternatives : KeyError, custom exception, ou logging + return None\n           raise ValueError(f\"Colonnes manquantes: {missing}\")\n       \n       # Cr√©ation des nouvelles features\n       \n       # üîç [Expert] Cr√©ation de feature engineered : pression puls√©e (indicateur cardiovasculaire)\n       # üë∂ [Enfant] On calcule la diff√©rence entre deux pressions pour avoir une nouvelle mesure\n       # üí° Alternatives : np.subtract(), diff√©rence absolue, ou ratio\n       X['pulse_pressure'] = X['systolic_bp'] - X['diastolic_bp']\n       \n       # üîç [Expert] Cr√©ation d'interaction entre pression et mobilit√© (feature complexe)\n       # üë∂ [Enfant] On multiplie deux mesures ensemble pour cr√©er une nouvelle information\n       # üí° Alternatives : addition, division, ou transformation non-lin√©aire\n       X['bp_mobility_interaction'] = X['systolic_bp'] * X['mobility_score']\n       \n       # üîç [Expert] Retour du DataFrame transform√© avec nouvelles features\n       # üë∂ [Enfant] On rend nos donn√©es avec les nouvelles colonnes ajout√©es\n       # üí° Alternatives : return specific columns, return numpy array, ou inplace modification\n       return X\n\n# Exemple d'utilisation avec donn√©es synth√©tiques adapt√©es\n\n# üîç [Expert] Guard clause pour ex√©cution en tant que script principal\n# üë∂ [Enfant] On v√©rifie qu'on lance ce fichier directement (pas qu'on l'importe)\n# üí° Alternatives : main() function, argparse, ou pas de guard\nif __name__ == \"__main__\":\n   \n   # üîç [Expert] Log de g√©n√©ration de donn√©es synth√©tiques pour test\n   # üë∂ [Enfant] On dit qu'on va cr√©er de fausses donn√©es pour tester notre code\n   # üí° Alternatives : logging.info(), pas de log, ou description d√©taill√©e\n   print(\"\\nüîç [Expert] G√©n√©ration de donn√©es synth√©tiques compatibles\")\n   \n   # üîç [Expert] Import de la fonction de g√©n√©ration de donn√©es sklearn\n   # üë∂ [Enfant] On va chercher un outil pour cr√©er de fausses donn√©es\n   # üí° Alternatives : np.random, pandas, ou datasets r√©els\n   from sklearn.datasets import make_classification\n   \n   # Cr√©ation d'un DataFrame avec les bonnes colonnes\n   \n   # üîç [Expert] G√©n√©ration de dataset synth√©tique avec param√®tres reproductibles\n   # üë∂ [Enfant] On cr√©e de fausses donn√©es avec 2000 lignes et 10 colonnes\n   # üí° Alternatives : make_regression, load_iris, ou donn√©es manuelles\n   X, _ = make_classification(n_samples=2000, n_features=10, random_state=RANDOM_STATE)\n   \n   # üîç [Expert] Conversion en DataFrame avec noms de colonnes explicites pour compatibilit√©\n   # üë∂ [Enfant] On transforme nos nombres en tableau avec des noms de colonnes\n   # üí° Alternatives : generic column names, keep as numpy, ou real column names\n   X = pd.DataFrame(X, columns=[\n       'age', 'heart_rate', 'systolic_bp', 'diastolic_bp', \n       'grip_strength', 'mobility_score', 'comorbidities_count',\n       'medication_count', 'feature_8', 'feature_9'\n   ])\n   \n   # üîç [Expert] G√©n√©ration de variable cible d√©s√©quilibr√©e pour r√©alisme\n   # üë∂ [Enfant] On cr√©e nos r√©ponses avec plus de \"non\" que de \"oui\" comme dans la vraie vie\n   # üí° Alternatives : balanced classes, make_classification target, ou real targets\n   y = pd.Series(np.random.choice([0, 1], size=2000, p=[0.7, 0.3]))\n\n   # 1. G√©n√©ration de donn√©es de test adapt√©es\n   data = {\n        'age': np.random.normal(65, 10, 1000),\n        'heart_rate': np.random.normal(72, 10, 1000),\n        'systolic_bp': np.random.normal(120, 15, 1000),\n        'diastolic_bp': np.random.normal(80, 10, 1000),\n        'grip_strength': np.random.normal(30, 5, 1000),\n        'mobility_score': np.random.uniform(0, 10, 1000),\n        'comorbidities_count': np.random.poisson(2, 1000),\n        'medication_count': np.random.poisson(3, 1000),\n        'gender': np.random.choice(['M', 'F'], 1000)\n    }\n   X = pd.DataFrame(data)\n   y = (X['age'] > 75).astype(int)  # Cible synth√©tique\n\n    # 2. Ex√©cution\n    #model, report = run_full_pipeline(X, y, n_trials=5)\n    \n   # Ex√©cution du pipeline corrig√©\n   \n   # üîç [Expert] Bloc try pour test d'int√©gration du pipeline complet\n   # üë∂ [Enfant] On essaie de lancer tout notre syst√®me et on se pr√©pare aux erreurs\n   # üí° Alternatives : pas de gestion d'erreur, unittest, ou validation pr√©alable\n   try:\n       # üîç [Expert] Ex√©cution du pipeline avec param√®tres r√©duits pour test rapide\n       # üë∂ [Enfant] On lance notre machine avec seulement 10 essais pour aller vite\n       # üí° Alternatives : full n_trials, cross-validation, ou dry run\n       model, report = run_full_pipeline(X, y, n_trials=10)\n       \n       # üîç [Expert] Message de succ√®s avec emoji pour visibilit√©\n       # üë∂ [Enfant] On dit qu'on a r√©ussi √† tout faire marcher !\n       # üí° Alternatives : logging.info(), print simple, ou rapport d√©taill√©\n       print(\"\\nüéâ Pipeline ex√©cut√© avec succ√®s!\")\n       \n   # üîç [Expert] Gestion des erreurs d'ex√©cution avec message informatif\n   # üë∂ [Enfant] Si quelque chose plante, on l'attrape et on dit de regarder les erreurs\n   # üí° Alternatives : logging.exception(), sys.exit(), ou re-raise\n   except Exception as e:\n       # üîç [Expert] Message d'√©chec avec r√©f√©rence aux logs pour debugging\n       # üë∂ [Enfant] On dit que √ßa a pas march√© et qu'il faut regarder les messages d'erreur\n       # üí° Alternatives : print(str(e)), logging.error(), ou traceback complet\n       print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T21:54:55.139766Z","iopub.execute_input":"2025-06-21T21:54:55.140207Z","iopub.status.idle":"2025-06-21T21:55:44.961801Z","shell.execute_reply.started":"2025-06-21T21:54:55.140169Z","shell.execute_reply":"2025-06-21T21:55:44.960826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC EXPLICATIONS D√âTAILL√âES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire une machine intelligente qui peut rep√©rer les personnes fragiles\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- Pour des projets simples : utiliser sklearn sans pipeline\")\nprint(\"- Pour le big data : utiliser Spark ML ou Dask\")\nprint(\"- Pour l'IA embarqu√©e : utiliser ONNX Runtime ou TensorFlow Lite\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Importation des librairies essentielles pour le machine learning industriel\")\nprint(\"üë∂ [Enfant] On sort toutes les bo√Ætes √† outils dont on aura besoin\")\nprint(\"\\nüí° Gestion des d√©pendances :\")\nprint(\"- En entreprise : utiliser un environnement conda/pip freeze\")\nprint(\"- En production : conteneur Docker avec versions fig√©es\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\n\nprint(\"\\nüîç [Expert] Importation des composants scikit-learn pour le pipeline\")\nprint(\"üë∂ [Enfant] On prend les pi√®ces pour construire notre machine\")\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nprint(\"\\nüîç [Expert] M√©triques pour l'√©valuation robuste en milieu clinique\")\nprint(\"üë∂ [Enfant] On choisit comment juger si notre machine fonctionne bien\")\n\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, \n                           precision_score, confusion_matrix, classification_report,\n                           precision_recall_curve, roc_curve, average_precision_score,\n                           make_scorer, balanced_accuracy_score)\n\nprint(\"\\nüîç [Expert] LightGBM pour des mod√®les performants et interpr√©tables\")\nprint(\"üë∂ [Enfant] On prend un moteur puissant mais qu'on peut comprendre\")\n\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.calibration import calibration_curve\n\nprint(\"\\nüîç [Expert] Configuration de base pour la reproductibilit√©\")\nprint(\"üë∂ [Enfant] On r√®gle notre machine pour qu'elle donne toujours les m√™mes r√©sultats\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©finition des KPI m√©tier pour √©valuer le mod√®le\")\nprint(\"üë∂ [Enfant] On d√©cide comment noter notre machine\")\nprint(\"\\nüí° Choix des m√©triques :\")\nprint(\"- Probl√®me √©quilibr√© : accuracy et AUC\")\nprint(\"- D√©s√©quilibre mod√©r√© : F1-score\")\nprint(\"- Cas critique (comme ici) : F2-score privil√©giant le recall\")\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation\"\"\"\n    def __init__(self):\n        print(\"\\nüîç [Expert] Initialisation des m√©triques avec contraintes m√©tier\")\n        print(\"üë∂ [Enfant] On pr√©pare notre carte de notation\")\n        \n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        \n        print(\"\\nüîç [Expert] D√©finition des seuils d'alerte cliniques\")\n        print(\"üë∂ [Enfant] On fixe les notes en dessous desquelles c'est inqui√©tant\")\n        \n        self.alert_thresholds = {\n            'roc_auc': 0.75,  # Minimum acceptable en milieu clinique\n            'f2_score': 0.6,  # Seuil empirique pour les cas critiques\n            'recall': 0.8     # Doit d√©tecter au moins 80% des cas fragiles\n        }\n    \n    def _specificity(self, y_true, y_pred):\n        \"\"\"Calcul sp√©cifique de la sp√©cificit√© (True Negative Rate)\"\"\"\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n        return tn / (tn + fp)\n\nprint(\"\\nüîç [Expert] Instanciation de la configuration d'√©valuation\")\nprint(\"üë∂ [Enfant] On active notre syst√®me de notation\")\n\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Cr√©ation de features cliniques pertinentes\")\nprint(\"üë∂ [Enfant] On ajoute des mesures utiles que notre machine pourra comprendre\")\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Ing√©nierie de caract√©ristiques personnalis√©es\"\"\"\n    def fit(self, X, y=None):\n        print(\"\\nüîç [Expert] Phase d'apprentissage des transformations (rien √† apprendre ici)\")\n        print(\"üë∂ [Enfant] Notre machine regarde les donn√©es mais n'a rien besoin de m√©moriser\")\n        return self\n\n    def transform(self, X):\n        print(\"\\nüîç [Expert] Cr√©ation de features cliniques d√©riv√©es\")\n        print(\"üë∂ [Enfant] On calcule de nouvelles informations utiles √† partir des mesures existantes\")\n        \n        X_copy = X.copy()\n        # Pression puls√©e (indicateur clinique important)\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        \n        # Interaction tension/mobilit√© (relation clinique connue)\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        \n        return X_copy\n\nprint(\"\\nüîç [Expert] Configuration du pr√©processeur industriel\")\nprint(\"üë∂ [Enfant] On pr√©pare les √©tapes pour nettoyer et pr√©parer les donn√©es\")\nprint(\"\\nüí° Alternatives de pr√©traitement :\")\nprint(\"- Donn√©es manquantes : imputation par la m√©diane/mode ou mod√®le pr√©dictif\")\nprint(\"- Variables cat√©gorielles : one-hot encoding, target encoding ou embeddings\")\nprint(\"- Normalisation : StandardScaler, RobustScaler ou QuantileTransformer\")\n\nprint(\"\\nüîç [Expert] D√©finition des caract√©ristiques num√©riques et cat√©gorielles\")\nprint(\"üë∂ [Enfant] On trie les informations en nombres et en cat√©gories\")\n\nnumeric_features = ['age', 'heart_rate', 'systolic_bp', 'diastolic_bp', \n                   'grip_strength', 'mobility_score', 'comorbidities_count', \n                   'medication_count']\ncategorical_features = ['gender']\n\nprint(\"\\nüîç [Expert] Pipeline pour les variables num√©riques (imputation + scaling robuste)\")\nprint(\"üë∂ [Enfant] On nettoie et met √† l'√©chelle les nombres\")\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),  # Robust aux outliers\n    ('scaler', RobustScaler())                      # Peu sensible aux valeurs extr√™mes\n])\n\nprint(\"\\nüîç [Expert] Pipeline pour les variables cat√©gorielles (imputation + one-hot)\")\nprint(\"üë∂ [Enfant] On nettoie et transforme les cat√©gories en nombres\")\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # Remplissage par le mode\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  # √âvite la multicolin√©arit√©\n])\n\nprint(\"\\nüîç [Expert] Combinaison des pipelines avec ColumnTransformer\")\nprint(\"üë∂ [Enfant] On assemble toutes les pi√®ces du nettoyage\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'  # Conserve les autres colonnes si besoin\n)\n# 4. üéØ FONCTIONS D'√âVALUATION INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ VALIDATION ROBUSTE\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Impl√©mentation d'une validation crois√©e stratifi√©e compl√®te\")\nprint(\"üë∂ [Enfant] On v√©rifie plusieurs fois que notre machine marche bien sur des d√©coupages diff√©rents\")\nprint(\"\\nüí° Strat√©gies alternatives :\")\nprint(\"- TimeSeriesSplit pour donn√©es temporelles\")\nprint(\"- GroupKFold pour donn√©es corr√©l√©es\")\nprint(\"- Bootstrap pour petits datasets\")\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"\n    √âvaluation compl√®te avec validation crois√©e\n    \n    Args:\n        pipeline: Pipeline sklearn √† √©valuer\n        X: Features\n        y: Target\n        cv_folds: Nombre de folds\n    \n    Returns:\n        DataFrame avec r√©sultats d√©taill√©s\n    \"\"\"\n    print(f\"\\nüîç [Expert] Lancement de la validation crois√©e ({cv_folds} folds)\")\n    print(\"üë∂ [Enfant] On d√©coupe les donn√©es en plusieurs parties pour tester plusieurs fois\")\n    \n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    \n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2),\n        'recall': 'recall',\n        'precision': 'precision',\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    \n    print(\"\\nüîç [Expert] Calcul des m√©triques sur chaque fold (parall√©lis√©)\")\n    cv_results = cross_validate(\n        pipeline, X, y,\n        cv=cv,\n        scoring=scoring,\n        return_train_score=True,\n        n_jobs=-1  # Utilisation de tous les c≈ìurs disponibles\n    )\n    \n    results_df = pd.DataFrame(cv_results)\n    print(f\"\\nüìä R√©sultats moyens :\\n{results_df.mean().to_string()}\")\n    \n    return results_df\n\n# 5. üìä ANALYSE DES SEUILS DE D√âCISION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üìä OPTIMISATION DES SEUILS\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse du trade-off pr√©cision/rappel par seuil\")\nprint(\"üë∂ [Enfant] On cherche le meilleur r√©glage pour notre machine\")\nprint(\"\\nüí° M√©thodes alternatives :\")\nprint(\"- Optimisation directe du seuil avec BayesSearchCV\")\nprint(\"- Maximisation du score m√©tier personnalis√©\")\nprint(\"- Courbe ROC Youden pour seuil optimal\")\n\"\"\"\ndef analyze_decision_threshold(y_true, y_probs):\n    \n    print(\"\\nüîç [Expert] Calcul des m√©triques pour 20 seuils diff√©rents\")\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    \n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    \n    metrics_df = pd.DataFrame(metrics)\n    \n    print(\"\\nüîç [Expert] Visualisation interactive des m√©triques\")\n    plt.figure(figsize=(10, 6))\n    metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'])\n    plt.axvline(x=0.5, color='gray', linestyle='--', label='Seuil par d√©faut')\n    plt.title(\"Performance par seuil de d√©cision\")\n    plt.xlabel(\"Seuil de classification\")\n    plt.ylabel(\"Score\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \n    return metrics_df\n\"\"\"\n# Modifiez la fonction analyze_decision_threshold\ndef analyze_decision_threshold(y_true, y_probs, plot=False):\n    \"\"\"Analyse approfondie des seuils de classification\"\"\"\n    print(\"\\nüîç [Expert] Calcul des m√©triques pour 20 seuils diff√©rents\")\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    \n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    \n    metrics_df = pd.DataFrame(metrics)\n    \n    if plot:\n        try:\n            print(\"\\nüîç [Expert] Visualisation interactive des m√©triques\")\n            plt.figure(figsize=(10, 6))\n            metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'])\n            plt.axvline(x=0.5, color='gray', linestyle='--', label='Seuil par d√©faut')\n            plt.title(\"Performance par seuil de d√©cision\")\n            plt.xlabel(\"Seuil de classification\")\n            plt.ylabel(\"Score\")\n            plt.grid(True)\n            plt.legend()\n            plt.show()\n        except ImportError:\n            print(\"‚ö†Ô∏è Matplotlib non disponible - les visualisations sont d√©sactiv√©es\")\n    \n    return metrics_df\n# 6. üí∞ ANALYSE CO√õT DES ERREURS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üí∞ IMPACT M√âTIER\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Analyse co√ªt/b√©n√©fice des diff√©rents types d'erreurs\")\nprint(\"üë∂ [Enfant] On calcule combien co√ªtent les erreurs de notre machine\")\nprint(\"\\nüí° Approches compl√©mentaires :\")\nprint(\"- Analyse ROI complet\")\nprint(\"- Matrice de confusion pond√©r√©e\")\nprint(\"- Optimisation directe du co√ªt m√©tier\")\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions\"\"\"\n    print(\"\\nüîç [Expert] Extraction de la matrice de confusion\")\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    \n    print(\"\\nüîç [Expert] D√©finition des co√ªts m√©tier (√† adapter)\")\n    cost_matrix = {\n        'fn_cost': 1000,  # Co√ªt d'un faux n√©gatif (cas manqu√©)\n        'fp_cost': 200,    # Co√ªt d'un faux positif (surdiagnostic)\n        'tp_gain': 500,    # Gain d'un vrai positif (bonne prise en charge)\n        'tn_gain': 50      # Gain d'un vrai n√©gatif (pas d'intervention inutile)\n    }\n    \n    total_cost = (fn * cost_matrix['fn_cost'] + \n                 fp * cost_matrix['fp_cost'] - \n                 tp * cost_matrix['tp_gain'] - \n                 tn * cost_matrix['tn_gain'])\n    \n    print(\"\\nüí∞ Bilan des co√ªts :\")\n    print(f\"- Faux n√©gatifs (FN): {fn} √ó {cost_matrix['fn_cost']} = {fn * cost_matrix['fn_cost']}\")\n    print(f\"- Faux positifs (FP): {fp} √ó {cost_matrix['fp_cost']} = {fp * cost_matrix['fp_cost']}\")\n    print(f\"- Vrais positifs (TP): {tp} √ó -{cost_matrix['tp_gain']} = {-tp * cost_matrix['tp_gain']}\")\n    print(f\"- Vrais n√©gatifs (TN): {tn} √ó -{cost_matrix['tn_gain']} = {-tn * cost_matrix['tn_gain']}\")\n    print(f\"‚Üí CO√õT TOTAL: {total_cost}\")\n    \n    return {\n        'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp},\n        'cost_analysis': cost_matrix,\n        'total_cost': total_cost\n    }\n\n# 7. üöÄ OPTIMISATION AVEC CHECKPOINTS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üöÄ OPTIMISATION DES HYPERPARAM√àTRES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Configuration d'Optuna avec sauvegarde des checkpoints\")\nprint(\"üë∂ [Enfant] On r√®gle finement notre machine en gardant des sauvegardes\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- GridSearchCV pour un espace de recherche petit\")\nprint(\"- Ray Tune pour le scale-out\")\nprint(\"- Hyperopt pour des algorithmes d'optimisation avanc√©s\")\n\ndef create_study_with_checkpoints(study_name, storage_name=None):\n    \"\"\"Cr√©e ou charge une √©tude Optuna\"\"\"\n    CHECKPOINT_DIR = \"optuna_checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{study_name}.pkl\")\n    \n    if os.path.exists(CHECKPOINT_PATH):\n        print(\"\\nüîç [Expert] Chargement d'une √©tude existante\")\n        study = joblib.load(CHECKPOINT_PATH)\n        print(f\"üìö Essais pr√©c√©dents charg√©s: {len(study.trials)}\")\n    else:\n        print(\"\\nüîç [Expert] Cr√©ation d'une nouvelle √©tude\")\n        study = optuna.create_study(\n            direction='maximize',\n            sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n            study_name=study_name,\n            storage=storage_name\n        )\n    \n    return study, CHECKPOINT_PATH\n\ndef objective(trial, X, y):\n    \"\"\"Fonction objective pour Optuna\"\"\"\n    print(f\"\\nüîç [Expert] Essai {trial.number} - Exploration des hyperparam√®tres\")\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'random_state': RANDOM_STATE\n    }\n    \n    pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**params))\n    ])\n    \n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    # Contrainte m√©tier sur le recall minimum\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall trop faible\")\n        raise optuna.TrialPruned()\n    \n    return mean_f2\n\n# 8. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. üè≠ PIPELINE FINAL\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Entra√Ænement et versionning du mod√®le avec m√©tadonn√©es\")\nprint(\"üë∂ [Enfant] On sauvegarde notre meilleure machine avec son mode d'emploi\")\nprint(\"\\nüí° Bonnes pratiques :\")\nprint(\"- MLflow pour le suivi complet\")\nprint(\"- ONNX pour l'interop√©rabilit√©\")\nprint(\"- DVC pour la gestion des versions\")\n\ndef train_final_model(X_train, y_train, best_params, version=\"1.0.0\"):\n    \"\"\"Entra√Æne et sauvegarde le mod√®le final\"\"\"\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le final (version {version})\")\n    \n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE))\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    # Sauvegarde du mod√®le\n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    print(f\"üíæ Mod√®le sauvegard√©: {model_path}\")\n    \n    # M√©tadonn√©es\n    metadata = {\n        \"model_version\": version,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features\": list(X_train.columns),\n        \"best_params\": best_params,\n        \"metrics\": {\n            \"cv_folds\": 5,\n            \"optimization_metric\": \"f2_score\",\n            \"recall_constraint\": eval_config.alert_thresholds['recall']\n        },\n        \"data_schema\": {\n            \"numeric_features\": numeric_features,\n            \"categorical_features\": categorical_features\n        }\n    }\n    \n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4)\n    print(f\"üìù M√©tadonn√©es sauvegard√©es: {metadata_path}\")\n    \n    return final_pipeline, MODEL_DIR\n\n# 9. üìä VISUALISATIONS CLINIQUES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. üìä ANALYSE DES PERFORMANCES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Visualisations professionnelles pour l'analyse\")\nprint(\"üë∂ [Enfant] On dessine des graphiques pour comprendre notre machine\")\nprint(\"\\nüí° Visualisations compl√©mentaires :\")\nprint(\"- Diagramme de d√©cision clinique\")\nprint(\"- Analyse des erreurs par sous-groupes\")\nprint(\"- Courbes de calibration par √¢ge\")\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Courbe de calibration\"\"\"\n    print(\"\\nüîç [Expert] V√©rification de la calibration des probabilit√©s\")\n    prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n    plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n    plt.xlabel(\"Probabilit√© pr√©dite\")\n    plt.ylabel(\"Probabilit√© r√©elle\")\n    plt.title(\"Courbe de calibration\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef validate_input_data(X, y):\n    \"\"\"Valide les donn√©es d'entr√©e avant le traitement\"\"\"\n    if not isinstance(X, (pd.DataFrame, np.ndarray)):\n        raise TypeError(\"X doit √™tre un DataFrame pandas ou un array numpy\")\n    \n    if not isinstance(y, (pd.Series, np.ndarray)):\n        raise TypeError(\"y doit √™tre une Series pandas ou un array numpy\")\n    \n    if len(X) != len(y):\n        raise ValueError(\"X et y doivent avoir la m√™me longueur\")\n    \n    # V√©rification des colonnes requises\n    required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n    if isinstance(X, pd.DataFrame):\n        missing = required_cols - set(X.columns)\n        if missing:\n            raise ValueError(f\"Colonnes manquantes: {missing}\")\n\n\ndef check_dependencies():\n    \"\"\"V√©rifie que toutes les d√©pendances sont install√©es\"\"\"\n    required = {\n        'numpy': 'np',\n        'pandas': 'pd',\n        'sklearn': 'sklearn',\n        'lightgbm': 'lgb',\n        'optuna': 'optuna'\n    }\n    \n    missing = []\n    for lib, short in required.items():\n        try:\n            __import__(short)\n        except ImportError:\n            missing.append(lib)\n    \n    if missing:\n        raise ImportError(\n            f\"D√©pendances manquantes: {', '.join(missing)}\\n\"\n            f\"Installez-les avec: pip install {' '.join(missing)}\"\n        )\n\n\n\n# =============================================================================\n# 10. üèÅ PIPELINE COMPLET CORRIG√â\n# =============================================================================\n\n# üîç [Expert] Affichage format√© du titre de section avec s√©parateurs visuels\n# üë∂ [Enfant] On √©crit un joli titre pour montrer qu'on commence quelque chose d'important\n# üí° Alternatives : logger.info(), rich.print(), ou simple print() sans formatage\nprint(\"\\n\" + \"=\"*80)\n\n# üîç [Expert] Titre descriptif de la section avec emoji pour identification rapide\n# üë∂ [Enfant] On dit ce qu'on va faire : un pipeline qui sait se r√©parer tout seul\n# üí° Alternatives : logging.info(), f-string formatting, ou titre sans emoji\nprint(\"10. üèÅ PIPELINE COMPLET AVEC GESTION D'ERREURS\")\n\n# üîç [Expert] Fermeture du cadre visuel pour d√©limiter la section\n# üë∂ [Enfant] On ferme notre jolie bo√Æte de titre\n# üí° Alternatives : print(\"-\"*80), logging separator, ou pas de s√©parateur\nprint(\"=\"*80)\n\n# üîç [Expert] Documentation technique pour experts avec contexte d'orchestration\n# üë∂ [Enfant] On explique aux grands ce qu'on fait : on organise tout comme un chef d'orchestre\n# üí° Alternatives : docstring, commentaire inline, ou documentation externe\nprint(\"\\nüîç [Expert] Orchestration robuste avec gestion des erreurs et logging\")\n\n# üîç [Expert] Vulgarisation pour faciliter la compr√©hension m√©tier\n# üë∂ [Enfant] On dit aux petits : on pr√©pare nos outils pour r√©parer si √ßa casse\n# üí° Alternatives : pas d'explication enfant, explication technique uniquement\nprint(\"üë∂ [Enfant] On lance tout en √©tant pr√™t √† r√©parer si quelque chose casse\")\n\n# üîç [Expert] Liste des bonnes pratiques pour guidance architecturale\n# üë∂ [Enfant] On dit les r√®gles importantes √† suivre\n# üí° Alternatives : documentation s√©par√©e, configuration YAML, ou constantes\nprint(\"\\nüí° Bonnes pratiques :\")\n\n# üîç [Expert] Premi√®re bonne pratique : observabilit√© et tra√ßabilit√©\n# üë∂ [Enfant] On note tout ce qui se passe comme dans un journal\n# üí° Alternatives : logging.basicConfig(), structlog, ou prints simples\nprint(\"- Journalisation d√©taill√©e\")\n\n# üîç [Expert] Deuxi√®me bonne pratique : gestion fine des exceptions\n# üë∂ [Enfant] On attrape les erreurs une par une pour mieux les comprendre\n# üí° Alternatives : try-catch global, assert statements, ou pas de gestion\nprint(\"- Gestion des erreurs granulaires\")\n\n# üîç [Expert] Troisi√®me bonne pratique : validation defensive programming\n# üë∂ [Enfant] On v√©rifie que nos donn√©es sont correctes avant de commencer\n# üí° Alternatives : schema validation (Pydantic), unit tests, ou pas de validation\nprint(\"- Validation des donn√©es en entr√©e\")\n\n# üîç [Expert] D√©finition de fonction avec signature claire et documentation\n# üë∂ [Enfant] On cr√©e une fonction qui fait tout le travail d'un coup\n# üí° Alternatives : classe Pipeline, script s√©quentiel, ou notebook cells\ndef run_full_pipeline(X, y, n_trials=50):\n    \"\"\"Ex√©cute l'ensemble du pipeline avec gestion robuste des erreurs\"\"\"\n    \n    # üîç [Expert] Bloc try principal pour capture globale des exceptions critiques\n    # üë∂ [Enfant] On met tout dans une bo√Æte sp√©ciale qui attrape les erreurs\n    # üí° Alternatives : multiple try-catch, decorateur @exception_handler, ou pas de gestion\n    try:\n        # 1. Validation des donn√©es d'entr√©e\n        \n        # üîç [Expert] Log de d√©but d'√©tape pour tra√ßabilit√© du flux\n        # üë∂ [Enfant] On dit qu'on commence √† v√©rifier nos donn√©es\n        # üí° Alternatives : logger.debug(), logging.info(), ou commentaire seulement\n        print(\"\\nüîç [Expert] Validation des donn√©es d'entr√©e\")\n        \n        # üîç [Expert] Conversion d√©fensive en DataFrame avec copie pour √©viter side-effects\n        # üë∂ [Enfant] On transforme nos donn√©es en tableau propre et on en fait une copie\n        # üí° Alternatives : pd.DataFrame(X), np.array(X), ou validation Pydantic\n        X = pd.DataFrame(X).copy()\n        \n        # üîç [Expert] Conversion en Series avec copie pour coh√©rence de type\n        # üë∂ [Enfant] On met nos r√©ponses dans une liste sp√©ciale et on la copie\n        # üí° Alternatives : np.array(y), list(y), ou validation avec assert\n        y = pd.Series(y).copy()\n        \n        # 2. S√©paration des donn√©es\n        \n        # üîç [Expert] Log de l'√©tape de split avec m√©thode de stratification\n        # üë∂ [Enfant] On dit qu'on va s√©parer nos donn√©es en deux tas √©quilibr√©s\n        # üí° Alternatives : logging.info(), pas de log, ou log plus d√©taill√©\n        print(\"\\nüîç [Expert] S√©paration train/test stratifi√©e\")\n        \n        # üîç [Expert] Bloc try sp√©cifique pour gestion granulaire des erreurs de split\n        # üë∂ [Enfant] On essaie de s√©parer les donn√©es et on se pr√©pare si √ßa marche pas\n        # üí° Alternatives : validation pr√©alable, assert, ou pas de gestion d'erreur\n        try:\n            # üîç [Expert] Split stratifi√© avec param√®tres reproductibles et ratio standard\n            # üë∂ [Enfant] On s√©pare nos donn√©es : 80% pour apprendre, 20% pour tester\n            # üí° Alternatives : train_test_split simple, StratifiedKFold, ou split manuel\n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, \n                test_size=0.2, \n                random_state=RANDOM_STATE, \n                stratify=y\n            )\n            \n            # üîç [Expert] Confirmation du split avec statistiques de taille\n            # üë∂ [Enfant] On dit combien on a dans chaque tas pour v√©rifier\n            # üí° Alternatives : logging.info(), assert sur les tailles, ou pas de v√©rification\n            print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n            \n        # üîç [Expert] Capture sp√©cifique des erreurs de s√©paration avec context\n        # üë∂ [Enfant] Si la s√©paration marche pas, on attrape l'erreur et on dit pourquoi\n        # üí° Alternatives : except ValueError, except sp√©cifiques, ou logging.exception()\n        except Exception as e:\n            # üîç [Expert] Log d'erreur avec message explicite pour debugging\n            # üë∂ [Enfant] On √©crit dans notre journal que quelque chose a mal tourn√©\n            # üí° Alternatives : logging.error(), sys.stderr.write(), ou raise direct\n            print(f\"‚ùå Erreur lors de la s√©paration des donn√©es: {str(e)}\")\n            \n            # üîç [Expert] Re-raise pour propager l'erreur vers le niveau sup√©rieur\n            # üë∂ [Enfant] On lance l'erreur plus haut pour que quelqu'un d'autre s'en occupe\n            # üí° Alternatives : return None, sys.exit(), ou custom exception\n            raise\n\n        # 3. Optimisation hyperparam√©trique\n        \n        # üîç [Expert] Log du d√©but d'optimisation avec mention de la persistance\n        # üë∂ [Enfant] On dit qu'on va chercher les meilleurs r√©glages et les sauvegarder\n        # üí° Alternatives : logging.info(), print simple, ou pas de log\n        print(\"\\nüîç [Expert] Optimisation hyperparam√©trique avec checkpoints\")\n        \n        # üîç [Expert] Cr√©ation d'√©tude Optuna avec syst√®me de sauvegarde\n        # üë∂ [Enfant] On pr√©pare notre syst√®me d'exp√©rimentation avec sauvegarde automatique\n        # üí° Alternatives : GridSearchCV, RandomizedSearchCV, ou hyperopt\n        study, checkpoint_path = create_study_with_checkpoints(\"frailty_detection\")\n        \n        # üîç [Expert] D√©finition de callback pour sauvegarde p√©riodique des r√©sultats\n        # üë∂ [Enfant] On cr√©e une fonction qui sauvegarde automatiquement nos progr√®s\n        # üí° Alternatives : sauvegarde manuelle, pas de checkpoint, ou callback Optuna\n        def save_checkpoint(study, trial):\n            # üîç [Expert] Bloc try pour gestion des erreurs de sauvegarde non-critiques\n            # üë∂ [Enfant] On essaie de sauvegarder, mais si √ßa marche pas, on continue quand m√™me\n            # üí° Alternatives : pas de gestion d'erreur, logging.exception(), ou validation pr√©alable\n            try:\n                # üîç [Expert] S√©rialisation de l'objet study avec joblib pour performance\n                # üë∂ [Enfant] On sauvegarde tous nos essais dans un fichier\n                # üí° Alternatives : pickle.dump(), JSON export, ou base de donn√©es\n                joblib.dump(study, checkpoint_path)\n                \n                # üîç [Expert] Confirmation de sauvegarde avec num√©ro d'essai pour tra√ßabilit√©\n                # üë∂ [Enfant] On dit qu'on a bien sauvegard√© et √† quel essai on en est\n                # üí° Alternatives : logging.debug(), pas de confirmation, ou log plus d√©taill√©\n                print(f\"üíæ Checkpoint sauvegard√© (essai {trial.number})\")\n                \n            # üîç [Expert] Gestion des erreurs de sauvegarde non-bloquantes\n            # üë∂ [Enfant] Si on arrive pas √† sauvegarder, on le dit mais on arr√™te pas tout\n            # üí° Alternatives : logging.warning(), raise, ou ignore silencieusement\n            except Exception as e:\n                # üîç [Expert] Warning plut√¥t qu'erreur car la sauvegarde n'est pas critique\n                # üë∂ [Enfant] On dit qu'il y a eu un petit probl√®me mais c'est pas grave\n                # üí° Alternatives : logging.error(), print simple, ou pas de message\n                print(f\"‚ö†Ô∏è Erreur sauvegarde checkpoint: {str(e)}\")\n\n        # üîç [Expert] Bloc try pour l'optimisation avec gestion des interruptions\n        # üë∂ [Enfant] On lance notre recherche des meilleurs r√©glages en se pr√©parant aux probl√®mes\n        # üí° Alternatives : pas de gestion d'erreur, validation pr√©alable, ou timeout\n        try:\n            # üîç [Expert] Log informatif avec nombre d'essais pour suivi de progression\n            # üë∂ [Enfant] On dit combien d'essais on va faire pour trouver les meilleurs r√©glages\n            # üí° Alternatives : logging.info(), progress bar personnalis√©e, ou pas de log\n            print(f\"\\nüîç Lancement de l'optimisation ({n_trials} essais)\")\n            \n            # üîç [Expert] Lancement de l'optimisation Optuna avec callback et progress bar\n            # üë∂ [Enfant] On lance la recherche automatique des meilleurs r√©glages\n            # üí° Alternatives : GridSearchCV, RandomizedSearchCV, ou recherche manuelle\n            study.optimize(\n                lambda trial: objective(trial, X_train, y_train),\n                n_trials=n_trials,\n                callbacks=[save_checkpoint],\n                show_progress_bar=True\n            )\n            \n        # üîç [Expert] Capture des erreurs d'optimisation avec propagation\n        # üë∂ [Enfant] Si la recherche plante, on attrape l'erreur et on l'explique\n        # üí° Alternatives : except specific errors, logging.exception(), ou recovery automatique\n        except Exception as e:\n            # üîç [Expert] Log d'erreur sp√©cifique √† l'optimisation pour debugging\n            # üë∂ [Enfant] On √©crit que la recherche a plant√© et pourquoi\n            # üí° Alternatives : logging.error(), sys.stderr, ou message plus d√©taill√©\n            print(f\"‚ùå Erreur lors de l'optimisation: {str(e)}\")\n            \n            # üîç [Expert] Re-raise car l'optimisation est critique pour la suite\n            # üë∂ [Enfant] On remonte l'erreur car sans optimisation, on peut pas continuer\n            # üí° Alternatives : return default params, continue avec params par d√©faut\n            raise\n\n        # üîç [Expert] Validation de la pr√©sence de r√©sultats avant de continuer\n        # üë∂ [Enfant] On v√©rifie qu'on a trouv√© au moins un bon r√©glage\n        # üí° Alternatives : assert, try-catch sur best_params, ou valeur par d√©faut\n        if not study.best_trial:\n            # üîç [Expert] Erreur explicite si aucun essai n'a r√©ussi\n            # üë∂ [Enfant] Si on a trouv√© aucun bon r√©glage, on dit qu'il faut regarder les erreurs\n            # üí° Alternatives : ValueError custom, logging.critical(), ou valeurs par d√©faut\n            raise ValueError(\"Aucun essai valide - v√©rifiez les logs des erreurs\")\n\n        # 4. Entra√Ænement du mod√®le final\n        \n        # üîç [Expert] Log du d√©but d'entra√Ænement avec mention des hyperparam√®tres optimaux\n        # üë∂ [Enfant] On dit qu'on va entra√Æner notre mod√®le final avec les meilleurs r√©glages\n        # üí° Alternatives : logging.info(), print d√©taill√©, ou pas de log\n        print(\"\\nüîç [Expert] Entra√Ænement du mod√®le final avec les meilleurs param√®tres\")\n        \n        # üîç [Expert] Bloc try pour gestion des erreurs d'entra√Ænement\n        # üë∂ [Enfant] On essaie d'entra√Æner le mod√®le et on se pr√©pare si √ßa marche pas\n        # üí° Alternatives : validation pr√©alable, pas de gestion, ou retry automatique\n        try:\n            # üîç [Expert] Extraction des meilleurs hyperparam√®tres de l'√©tude\n            # üë∂ [Enfant] On r√©cup√®re les meilleurs r√©glages qu'on a trouv√©s\n            # üí° Alternatives : study.best_params.copy(), dict(study.best_params)\n            best_params = study.best_params\n            \n            # üîç [Expert] Affichage des hyperparam√®tres pour tra√ßabilit√©\n            # üë∂ [Enfant] On montre quels sont les meilleurs r√©glages trouv√©s\n            # üí° Alternatives : logging.info(), pprint(), ou JSON format\n            print(f\"‚öôÔ∏è Meilleurs param√®tres: {best_params}\")\n            \n            # üîç [Expert] Entra√Ænement final avec sauvegarde automatique du mod√®le\n            # üë∂ [Enfant] On entra√Æne notre mod√®le final et on le sauvegarde\n            # üí° Alternatives : fit() simple, cross-validation, ou ensemble methods\n            model, model_dir = train_final_model(X_train, y_train, best_params)\n            \n            # üîç [Expert] Confirmation d'entra√Ænement avec chemin de sauvegarde\n            # üë∂ [Enfant] On dit que notre mod√®le est pr√™t et o√π on l'a rang√©\n            # üí° Alternatives : logging.info(), pas de confirmation, ou m√©tadonn√©es d√©taill√©es\n            print(f\"‚úÖ Mod√®le final entra√Æn√© et sauvegard√© dans {model_dir}\")\n            \n        # üîç [Expert] Gestion des erreurs d'entra√Ænement avec contexte sp√©cifique\n        # üë∂ [Enfant] Si l'entra√Ænement plante, on attrape l'erreur et on explique\n        # üí° Alternatives : except sp√©cifiques, logging.exception(), ou fallback model\n        except Exception as e:\n            # üîç [Expert] Log d'erreur sp√©cifique √† l'entra√Ænement pour debugging\n            # üë∂ [Enfant] On √©crit que l'entra√Ænement a plant√© et pourquoi\n            # üí° Alternatives : logging.error(), structured logging, ou details techniques\n            print(f\"‚ùå Erreur lors de l'entra√Ænement final: {str(e)}\")\n            \n            # üîç [Expert] Re-raise car le mod√®le est n√©cessaire pour l'√©valuation\n            # üë∂ [Enfant] On remonte l'erreur car sans mod√®le, on peut pas continuer\n            # üí° Alternatives : return baseline model, continue sans √©valuation\n            raise\n\n        # 5. √âvaluation finale\n        \n        # üîç [Expert] Log du d√©but d'√©valuation sur le jeu de test\n        # üë∂ [Enfant] On dit qu'on va tester notre mod√®le sur des donn√©es qu'il a jamais vues\n        # üí° Alternatives : logging.info(), print d√©taill√©, ou pas de log\n        print(\"\\nüîç [Expert] √âvaluation compl√®te sur le jeu de test\")\n        \n        # üîç [Expert] Bloc try pour gestion des erreurs d'√©valuation non-critiques\n        # üë∂ [Enfant] On essaie de tester notre mod√®le, si √ßa marche pas compl√®tement, c'est pas grave\n        # üí° Alternatives : pas de gestion, validation pr√©alable, ou √©valuation simplifi√©e\n        try:\n            # Pr√©dictions\n            \n            # üîç [Expert] G√©n√©ration des probabilit√©s pour m√©triques avanc√©es (ROC, calibration)\n            # üë∂ [Enfant] On demande au mod√®le de dire √† quel point il est s√ªr de ses r√©ponses\n            # üí° Alternatives : decision_function(), predict() seulement, ou probabilit√©s calibr√©es\n            y_probs = model.predict_proba(X_test)[:, 1]\n            \n            # üîç [Expert] G√©n√©ration des pr√©dictions binaires pour m√©triques classiques\n            # üë∂ [Enfant] On demande au mod√®le de donner ses r√©ponses finales oui/non\n            # üí° Alternatives : np.where(y_probs > threshold), argmax(), ou seuil optimis√©\n            y_pred = model.predict(X_test)\n            \n            # Rapport de classification\n            \n            # üîç [Expert] Log descriptif pour le rapport de classification d√©taill√©\n            # üë∂ [Enfant] On va montrer un tableau avec tous les scores de notre mod√®le\n            # üí° Alternatives : logging.info(), print simple, ou pas de log\n            print(\"\\nüìä Rapport de classification :\")\n            \n            # üîç [Expert] G√©n√©ration du rapport de classification avec noms de classes explicites\n            # üë∂ [Enfant] On affiche un joli tableau qui dit si notre mod√®le est bon\n            # üí° Alternatives : confusion_matrix(), m√©triques individuelles, ou custom report\n            print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n            \n            # Analyses avanc√©es\n            \n            # üîç [Expert] Log pour l'analyse des seuils de d√©cision optimaux\n            # üë∂ [Enfant] On va chercher le meilleur point pour dire oui ou non\n            # üí° Alternatives : pas de log, logging.debug(), ou description d√©taill√©e\n            print(\"\\nüîç Analyse des seuils de d√©cision\")\n            \n            # üîç [Expert] Analyse des seuils pour optimisation du trade-off pr√©cision/rappel\n            # üë∂ [Enfant] On teste plein de fa√ßons de dire oui/non pour trouver la meilleure\n            # üí° Alternatives : seuil fixe 0.5, courbe ROC, ou optimisation m√©tier\n            metrics_df = analyze_decision_threshold(y_test, y_probs)\n            \n            # üîç [Expert] Log pour l'analyse des co√ªts m√©tier\n            # üë∂ [Enfant] On va calculer combien √ßa co√ªte si on se trompe\n            # üí° Alternatives : pas de log, logging.info(), ou description m√©tier\n            print(\"\\nüí∞ Analyse des co√ªts\")\n            \n            # üîç [Expert] Analyse co√ªt-b√©n√©fice pour aide √† la d√©cision m√©tier\n            # üë∂ [Enfant] On calcule si notre mod√®le fait gagner ou perdre de l'argent\n            # üí° Alternatives : pas d'analyse co√ªt, m√©triques techniques seulement\n            cost_results = cost_analysis(y_test, y_pred)\n            \n            # üîç [Expert] Log pour g√©n√©ration des visualisations\n            # üë∂ [Enfant] On va faire de jolis dessins pour montrer si notre mod√®le est bon\n            # üí° Alternatives : pas de log, logging.debug(), ou description des plots\n            print(\"\\nüìà Visualisations\")\n            \n            # üîç [Expert] G√©n√©ration de graphiques de calibration pour fiabilit√© des probabilit√©s\n            # üë∂ [Enfant] On dessine un graphique pour voir si notre mod√®le dit la v√©rit√© sur ses probabilit√©s\n            # üí° Alternatives : ROC curve, confusion matrix, ou learning curves\n            plot_calibration(y_test, y_probs)\n            \n        # üîç [Expert] Gestion des erreurs d'√©valuation non-critiques avec warning\n        # üë∂ [Enfant] Si on arrive pas √† tout tester, on le dit mais on continue\n        # üí° Alternatives : logging.warning(), continue silencieusement, ou raise\n        except Exception as e:\n            # üîç [Expert] Warning car l'√©valuation est importante mais pas critique\n            # üë∂ [Enfant] On dit qu'il y a eu un probl√®me pour tester mais c'est pas trop grave\n            # üí° Alternatives : logging.error(), print simple, ou ignore\n            print(f\"‚ö†Ô∏è Erreur lors de l'√©valuation: {str(e)}\")\n            \n            # üîç [Expert] Re-raise pour signaler l'importance de l'√©valuation\n            # üë∂ [Enfant] On remonte quand m√™me l'erreur car c'est important de savoir si le mod√®le marche\n            # üí° Alternatives : continue, return partial results, ou default metrics\n            raise\n\n        # 6. Sauvegarde du rapport final\n        \n        # üîç [Expert] Log pour g√©n√©ration du rapport de synth√®se\n        # üë∂ [Enfant] On va faire un r√©sum√© de tout ce qu'on a fait et trouv√©\n        # üí° Alternatives : logging.info(), pas de log, ou description d√©taill√©e\n        print(\"\\nüîç [Expert] G√©n√©ration du rapport final\")\n        \n        # üîç [Expert] Bloc try pour gestion des erreurs de rapport non-critiques\n        # üë∂ [Enfant] On essaie de faire notre r√©sum√©, si √ßa marche pas c'est pas tr√®s grave\n        # üí° Alternatives : pas de gestion, validation pr√©alable, ou rapport simplifi√©\n        try:\n            # üîç [Expert] Construction du dictionnaire de rapport avec toutes les m√©triques importantes\n            # üë∂ [Enfant] On met tous nos r√©sultats dans une grande bo√Æte bien organis√©e\n            # üí° Alternatives : dataclass, JSON schema, ou objet custom\n            report = {\n                \"best_params\": best_params,\n                \"test_metrics\": {\n                    \"roc_auc\": roc_auc_score(y_test, y_probs),\n                    \"f2_score\": fbeta_score(y_test, y_pred, beta=2),\n                    \"recall\": recall_score(y_test, y_pred),\n                    \"precision\": precision_score(y_test, y_pred),\n                },\n                \"cost_analysis\": cost_results,\n                \"optimal_threshold\": metrics_df.loc[metrics_df['f2'].idxmax()].to_dict(),\n                \"data_stats\": {\n                    \"train_size\": len(X_train),\n                    \"test_size\": len(X_test),\n                    \"class_balance\": dict(y.value_counts(normalize=True))\n                }\n            }\n            \n            # üîç [Expert] D√©finition du chemin de sauvegarde dans le r√©pertoire du mod√®le\n            # üë∂ [Enfant] On choisit o√π ranger notre r√©sum√© avec le mod√®le\n            # üí° Alternatives : r√©pertoire s√©par√©, timestamp dans nom, ou base de donn√©es\n            report_path = os.path.join(model_dir, 'final_report.json')\n            \n            # üîç [Expert] Sauvegarde du rapport en JSON pour lisibilit√© et interop√©rabilit√©\n            # üë∂ [Enfant] On √©crit notre r√©sum√© dans un fichier que tout le monde peut lire\n            # üí° Alternatives : pickle, YAML, CSV, ou base de donn√©es\n            with open(report_path, 'w') as f:\n                json.dump(report, f, indent=4)\n                \n            # üîç [Expert] Message de succ√®s final avec localisation du rapport\n            # üë∂ [Enfant] On dit qu'on a fini et o√π trouver notre r√©sum√©\n            # üí° Alternatives : logging.info(), return path, ou print simple\n            print(f\"\\n‚úÖ Pipeline termin√© avec succ√®s! Rapport sauvegard√©: {report_path}\")\n            \n            # üîç [Expert] Retour du mod√®le et rapport pour utilisation ult√©rieure\n            # üë∂ [Enfant] On donne le mod√®le et le r√©sum√© √† celui qui nous a demand√© de travailler\n            # üí° Alternatives : save et return path, return dict, ou pas de retour\n            return model, report\n            \n        # üîç [Expert] Gestion des erreurs de g√©n√©ration de rapport avec propagation\n        # üë∂ [Enfant] Si on arrive pas √† faire le r√©sum√©, on attrape l'erreur\n        # üí° Alternatives : logging.error(), continue sans rapport, ou rapport minimal\n        except Exception as e:\n            # üîç [Expert] Log d'erreur car le rapport est important pour la tra√ßabilit√©\n            # üë∂ [Enfant] On dit qu'on a pas r√©ussi √† faire le r√©sum√© et pourquoi\n            # üí° Alternatives : logging.error(), warning seulement, ou details techniques\n            print(f\"‚ùå Erreur lors de la g√©n√©ration du rapport: {str(e)}\")\n            \n            # üîç [Expert] Re-raise car le rapport fait partie des livrables attendus\n            # üë∂ [Enfant] On remonte l'erreur car c'est important d'avoir le r√©sum√©\n            # üí° Alternatives : return mod√®le seulement, continue, ou rapport par d√©faut\n            raise\n\n    # üîç [Expert] Gestion globale des erreurs critiques du pipeline avec logging d√©taill√©\n    # üë∂ [Enfant] Si quelque chose de tr√®s grave arrive, on l'attrape ici\n    # üí° Alternatives : except sp√©cifiques, sys.excepthook, ou crash avec traceback\n    except Exception as e:\n        # üîç [Expert] Header d'erreur critique pour visibilit√© maximale\n        # üë∂ [Enfant] On √©crit en gros que quelque chose de tr√®s grave s'est pass√©\n        # üí° Alternatives : logging.critical(), sys.stderr, ou exception custom\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        \n        # üîç [Expert] Affichage du message d'erreur pour debugging imm√©diat\n        # üë∂ [Enfant] On dit exactement ce qui s'est mal pass√©\n        # üí° Alternatives : logging.exception(), traceback.print_exc(), ou structured logging\n        print(f\"Message d'erreur: {str(e)}\")\n        \n        # üîç [Expert] Section de conseils de d√©bogage pour r√©solution rapide\n        # üë∂ [Enfant] On donne des conseils pour r√©parer le probl√®me\n        # üí° Alternatives : documentation link, automated diagnostics, ou pas de conseils\n        print(\"\\nüîç [Expert] Conseils de d√©bogage:\")\n        \n        # üîç [Expert] Premier conseil : validation des donn√©es d'entr√©e\n        # üë∂ [Enfant] Premier truc √† v√©rifier : est-ce que les donn√©es sont correctes ?\n        # üí° Alternatives : automated validation, schema check, ou unit tests\n        print(\"- V√©rifier la coh√©rence des donn√©es d'entr√©e\")\n        \n        # üîç [Expert] Deuxi√®me conseil : tests unitaires des composants\n        # üë∂ [Enfant] Deuxi√®me truc : tester chaque morceau un par un\n        # üí° Alternatives : integration tests, debugging step by step, ou profiling\n        print(\"- Tester chaque composant du pipeline s√©par√©ment\")\n        \n        # üîç [Expert] Troisi√®me conseil : activation du mode debug\n        # üë∂ [Enfant] Troisi√®me truc : mettre le mode d√©taill√© pour voir plus d'informations\n        # üí° Alternatives : verbose logging, pdb debugger, ou traceback complet\n        print(\"- Activer les logs d√©taill√©s avec error_score='raise'\")\n        \n        # üîç [Expert] Re-raise de l'exception pour propagation vers l'appelant\n        # üë∂ [Enfant] On relance l'erreur pour que celui qui nous appelle puisse la g√©rer\n        # üí° Alternatives : sys.exit(), return None, ou exception wrapping\n        raise\n\n# Correction du FeatureEngineer\n\n# üîç [Expert] D√©finition de classe avec h√©ritage sklearn pour int√©gration pipeline\n# üë∂ [Enfant] On cr√©e une classe sp√©ciale qui sait transformer nos donn√©es\n# üí° Alternatives : function\n# üîç [Expert] D√©finition de classe avec h√©ritage sklearn pour int√©gration pipeline\n# üë∂ [Enfant] On cr√©e une classe sp√©ciale qui sait transformer nos donn√©es\n# üí° Alternatives : fonction simple, FunctionTransformer, ou classe custom sans h√©ritage\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n   \"\"\"Version corrig√©e du FeatureEngineer\"\"\"\n   \n   # üîç [Expert] M√©thode fit obligatoire pour compatibilit√© sklearn (ici pas d'apprentissage)\n   # üë∂ [Enfant] On cr√©e une fonction qui apprend, mais ici on apprend rien\n   # üí° Alternatives : fit with parameters, stateful fitting, ou NotImplementedError\n   def fit(self, X, y=None):\n       # üîç [Expert] Retour de self pour cha√Ænage des m√©thodes sklearn\n       # üë∂ [Enfant] On se rend nous-m√™me pour pouvoir √™tre utilis√© apr√®s\n       # üí° Alternatives : return None, store fitted state, ou validation\n       return self\n\n   # üîç [Expert] M√©thode transform principale pour cr√©ation de features\n   # üë∂ [Enfant] On cr√©e une fonction qui transforme nos donn√©es en ajoutant de nouvelles colonnes\n   # üí° Alternatives : fit_transform only, __call__, ou multiple transform methods\n   def transform(self, X):\n       \"\"\"Version robuste avec v√©rification des colonnes\"\"\"\n       \n       # üîç [Expert] Copie d√©fensive pour √©viter la modification des donn√©es originales\n       # üë∂ [Enfant] On fait une copie de nos donn√©es pour pas ab√Æmer les originales\n       # üí° Alternatives : X.copy(deep=True), inplace=True parameter, ou pas de copie\n       X = X.copy()\n       \n       # V√©rification des colonnes n√©cessaires\n       \n       # üîç [Expert] D√©finition de l'ensemble des colonnes requises pour les transformations\n       # üë∂ [Enfant] On fait une liste des colonnes qu'on a absolument besoin\n       # üí° Alternatives : list, tuple, ou validation schema avec Pydantic\n       required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n       \n       # üîç [Expert] Validation de pr√©sence des colonnes avec op√©ration d'ensemble\n       # üë∂ [Enfant] On v√©rifie qu'on a bien toutes les colonnes n√©cessaires\n       # üí° Alternatives : all(col in X.columns for col in required_cols), try-except, ou assert\n       if not required_cols.issubset(X.columns):\n           # üîç [Expert] Calcul des colonnes manquantes pour message d'erreur informatif\n           # üë∂ [Enfant] On trouve quelles colonnes il nous manque pour dire lesquelles\n           # üí° Alternatives : list comprehension, set intersection, ou message g√©n√©rique\n           missing = required_cols - set(X.columns)\n           \n           # üîç [Expert] Lev√©e d'exception avec message explicite pour debugging\n           # üë∂ [Enfant] On dit exactement quelles colonnes il manque\n           # üí° Alternatives : KeyError, custom exception, ou logging + return None\n           raise ValueError(f\"Colonnes manquantes: {missing}\")\n       \n       # Cr√©ation des nouvelles features\n       \n       # üîç [Expert] Cr√©ation de feature engineered : pression puls√©e (indicateur cardiovasculaire)\n       # üë∂ [Enfant] On calcule la diff√©rence entre deux pressions pour avoir une nouvelle mesure\n       # üí° Alternatives : np.subtract(), diff√©rence absolue, ou ratio\n       X['pulse_pressure'] = X['systolic_bp'] - X['diastolic_bp']\n       \n       # üîç [Expert] Cr√©ation d'interaction entre pression et mobilit√© (feature complexe)\n       # üë∂ [Enfant] On multiplie deux mesures ensemble pour cr√©er une nouvelle information\n       # üí° Alternatives : addition, division, ou transformation non-lin√©aire\n       X['bp_mobility_interaction'] = X['systolic_bp'] * X['mobility_score']\n       \n       # üîç [Expert] Retour du DataFrame transform√© avec nouvelles features\n       # üë∂ [Enfant] On rend nos donn√©es avec les nouvelles colonnes ajout√©es\n       # üí° Alternatives : return specific columns, return numpy array, ou inplace modification\n       return X\n\n# Exemple d'utilisation avec donn√©es synth√©tiques adapt√©es\n\n# üîç [Expert] Guard clause pour ex√©cution en tant que script principal\n# üë∂ [Enfant] On v√©rifie qu'on lance ce fichier directement (pas qu'on l'importe)\n# üí° Alternatives : main() function, argparse, ou pas de guard\nif __name__ == \"__main__\":\n\n   try:\n\n       check_dependencies()\n       \n       # üîç [Expert] Log de g√©n√©ration de donn√©es synth√©tiques pour test\n       # üë∂ [Enfant] On dit qu'on va cr√©er de fausses donn√©es pour tester notre code\n       # üí° Alternatives : logging.info(), pas de log, ou description d√©taill√©e\n       print(\"\\nüîç [Expert] G√©n√©ration de donn√©es synth√©tiques compatibles\")\n       \n       # üîç [Expert] Import de la fonction de g√©n√©ration de donn√©es sklearn\n       # üë∂ [Enfant] On va chercher un outil pour cr√©er de fausses donn√©es\n       # üí° Alternatives : np.random, pandas, ou datasets r√©els\n       from sklearn.datasets import make_classification\n       \n       # Cr√©ation d'un DataFrame avec les bonnes colonnes\n       \n       # üîç [Expert] G√©n√©ration de dataset synth√©tique avec param√®tres reproductibles\n       # üë∂ [Enfant] On cr√©e de fausses donn√©es avec 2000 lignes et 10 colonnes\n       # üí° Alternatives : make_regression, load_iris, ou donn√©es manuelles\n       X, _ = make_classification(n_samples=2000, n_features=10, random_state=RANDOM_STATE)\n       \n       # üîç [Expert] Conversion en DataFrame avec noms de colonnes explicites pour compatibilit√©\n       # üë∂ [Enfant] On transforme nos nombres en tableau avec des noms de colonnes\n       # üí° Alternatives : generic column names, keep as numpy, ou real column names\n       X = pd.DataFrame(X, columns=[\n           'age', 'heart_rate', 'systolic_bp', 'diastolic_bp', \n           'grip_strength', 'mobility_score', 'comorbidities_count',\n           'medication_count', 'feature_8', 'feature_9'\n       ])\n       \n       # üîç [Expert] G√©n√©ration de variable cible d√©s√©quilibr√©e pour r√©alisme\n       # üë∂ [Enfant] On cr√©e nos r√©ponses avec plus de \"non\" que de \"oui\" comme dans la vraie vie\n       # üí° Alternatives : balanced classes, make_classification target, ou real targets\n       y = pd.Series(np.random.choice([0, 1], size=2000, p=[0.7, 0.3]))\n    \n       # 1. G√©n√©ration de donn√©es de test adapt√©es\n       data = {\n            'age': np.random.normal(65, 10, 1000),\n            'heart_rate': np.random.normal(72, 10, 1000),\n            'systolic_bp': np.random.normal(120, 15, 1000),\n            'diastolic_bp': np.random.normal(80, 10, 1000),\n            'grip_strength': np.random.normal(30, 5, 1000),\n            'mobility_score': np.random.uniform(0, 10, 1000),\n            'comorbidities_count': np.random.poisson(2, 1000),\n            'medication_count': np.random.poisson(3, 1000),\n            'gender': np.random.choice(['M', 'F'], 1000)\n        }\n       X = pd.DataFrame(data)\n       y = (X['age'] > 75).astype(int)  # Cible synth√©tique\n    \n        # 2. Ex√©cution\n        #model, report = run_full_pipeline(X, y, n_trials=5)\n        \n       # Ex√©cution du pipeline corrig√©\n       \n       # üîç [Expert] Bloc try pour test d'int√©gration du pipeline complet\n       # üë∂ [Enfant] On essaie de lancer tout notre syst√®me et on se pr√©pare aux erreurs\n       # üí° Alternatives : pas de gestion d'erreur, unittest, ou validation pr√©alable\n       try:\n           # üîç [Expert] Ex√©cution du pipeline avec param√®tres r√©duits pour test rapide\n           # üë∂ [Enfant] On lance notre machine avec seulement 10 essais pour aller vite\n           # üí° Alternatives : full n_trials, cross-validation, ou dry run\n           model, report = run_full_pipeline(X, y, n_trials=10)\n                      \n           # üîç [Expert] Message de succ√®s avec emoji pour visibilit√©\n           # üë∂ [Enfant] On dit qu'on a r√©ussi √† tout faire marcher !\n           # üí° Alternatives : logging.info(), print simple, ou rapport d√©taill√©\n           \n           print(\"\\nüéâ Pipeline ex√©cut√© avec succ√®s!\")\n           \n           print(f\"Meilleur score F2: {report['test_metrics']['f2_score']:.4f}\")\n           \n       # üîç [Expert] Gestion des erreurs d'ex√©cution avec message informatif\n       # üë∂ [Enfant] Si quelque chose plante, on l'attrape et on dit de regarder les erreurs\n       # üí° Alternatives : logging.exception(), sys.exit(), ou re-raise\n       except Exception as e:\n           # üîç [Expert] Message d'√©chec avec r√©f√©rence aux logs pour debugging\n           # üë∂ [Enfant] On dit que √ßa a pas march√© et qu'il faut regarder les messages d'erreur\n           # üí° Alternatives : print(str(e)), logging.error(), ou traceback complet\n           print(f\"\\n‚ùå Erreur lors de l'ex√©cution du pipeline: {str(e)}\")\n           print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n   except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR INITIALE: {str(e)}\")\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:35:04.136638Z","iopub.execute_input":"2025-06-22T15:35:04.136970Z","iopub.status.idle":"2025-06-22T15:35:04.229688Z","shell.execute_reply.started":"2025-06-22T15:35:04.136943Z","shell.execute_reply":"2025-06-22T15:35:04.228606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC EXPLICATIONS D√âTAILL√âES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire une machine intelligente qui peut rep√©rer les personnes fragiles\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- Pour des projets simples : utiliser sklearn sans pipeline\")\nprint(\"- Pour le big data : utiliser Spark ML ou Dask\")\nprint(\"- Pour l'IA embarqu√©e : utiliser ONNX Runtime ou TensorFlow Lite\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Importation des librairies essentielles pour le machine learning industriel\")\nprint(\"üë∂ [Enfant] On sort toutes les bo√Ætes √† outils dont on aura besoin\")\nprint(\"\\nüí° Gestion des d√©pendances :\")\nprint(\"- En entreprise : utiliser un environnement conda/pip freeze\")\nprint(\"- En production : conteneur Docker avec versions fig√©es\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\n\nprint(\"\\nüîç [Expert] Importation des composants scikit-learn pour le pipeline\")\nprint(\"üë∂ [Enfant] On prend les pi√®ces pour construire notre machine\")\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nprint(\"\\nüîç [Expert] M√©triques pour l'√©valuation robuste en milieu clinique\")\nprint(\"üë∂ [Enfant] On choisit comment juger si notre machine fonctionne bien\")\n\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score,\n                           precision_score, confusion_matrix, classification_report,\n                           precision_recall_curve, roc_curve, average_precision_score,\n                           make_scorer, balanced_accuracy_score)\n\nprint(\"\\nüîç [Expert] LightGBM pour des mod√®les performants et interpr√©tables\")\nprint(\"üë∂ [Enfant] On prend un moteur puissant mais qu'on peut comprendre\")\n\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.calibration import calibration_curve\n\nprint(\"\\nüîç [Expert] Configuration de base pour la reproductibilit√©\")\nprint(\"üë∂ [Enfant] On r√®gle notre machine pour qu'elle donne toujours les m√™mes r√©sultats\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n# Correction pour compatibilit√© avec diff√©rentes versions de matplotlib/seaborn\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©finition des KPI m√©tier pour √©valuer le mod√®le\")\nprint(\"üë∂ [Enfant] On d√©cide comment noter notre machine\")\nprint(\"\\nüí° Choix des m√©triques :\")\nprint(\"- Probl√®me √©quilibr√© : accuracy et AUC\")\nprint(\"- D√©s√©quilibre mod√©r√© : F1-score\")\nprint(\"- Cas critique (comme ici) : F2-score privil√©giant le recall\")\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation\"\"\"\n    def __init__(self):\n        print(\"\\nüîç [Expert] Initialisation des m√©triques avec contraintes m√©tier\")\n        print(\"üë∂ [Enfant] On pr√©pare notre carte de notation\")\n        \n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        \n        print(\"\\nüîç [Expert] D√©finition des seuils d'alerte cliniques\")\n        print(\"üë∂ [Enfant] On fixe les notes en dessous desquelles c'est inqui√©tant\")\n        \n        self.alert_thresholds = {\n            'roc_auc': 0.75,\n            'f2_score': 0.6,\n            'recall': 0.5\n        }\n\n    def _specificity(self, y_true, y_pred):\n        \"\"\"Calcul sp√©cifique de la sp√©cificit√© (True Negative Rate)\"\"\"\n        # Utilisation de ravel() pour g√©rer les cas o√π la matrice est 1D\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        # Ajout d'une protection contre la division par z√©ro\n        if (tn + fp) == 0:\n            return 0.0\n        return tn / (tn + fp)\n\nprint(\"\\nüîç [Expert] Instanciation de la configuration d'√©valuation\")\nprint(\"üë∂ [Enfant] On active notre syst√®me de notation\")\n\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT (CORRIG√â)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (CORRIG√â)\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Cr√©ation de features cliniques pertinentes\")\nprint(\"üë∂ [Enfant] On ajoute des mesures utiles que notre machine pourra comprendre\")\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    CORRECTION : Ing√©nierie de caract√©ristiques robuste qui v√©rifie les colonnes.\n    Cette √©tape doit s'ex√©cuter sur un DataFrame, avant que les noms de colonnes\n    ne soient perdus par les transformateurs num√©riques/cat√©goriels.\n    \"\"\"\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Validation robuste que les colonnes n√©cessaires existent\n        required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n        if not required_cols.issubset(X_copy.columns):\n            missing = required_cols - set(X_copy.columns)\n            raise ValueError(f\"Colonnes manquantes pour FeatureEngineer: {missing}\")\n\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        \n        # CORRECTION : Retourne le DataFrame avec les nouvelles colonnes\n        return X_copy\n\nprint(\"\\nüîç [Expert] Configuration du pr√©processeur industriel\")\nprint(\"üë∂ [Enfant] On pr√©pare les √©tapes pour nettoyer et pr√©parer les donn√©es\")\n\nnumeric_features = ['age', 'heart_rate', 'systolic_bp', 'diastolic_bp',\n                   'grip_strength', 'mobility_score', 'comorbidities_count',\n                   'medication_count', 'pulse_pressure', 'bp_mobility_interaction']\ncategorical_features = ['gender']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', RobustScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# 4. üéØ FONCTIONS D'√âVALUATION INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ VALIDATION ROBUSTE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvaluation compl√®te avec validation crois√©e\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    \n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2),\n        'recall': 'recall',\n        'precision': 'precision',\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    \n    # CORRECTION : Ajout de error_score='raise' pour un d√©bogage plus facile\n    cv_results = cross_validate(\n        pipeline, X, y,\n        cv=cv,\n        scoring=scoring,\n        return_train_score=True,\n        n_jobs=-1,\n        error_score='raise'\n    )\n    \n    results_df = pd.DataFrame(cv_results)\n    print(f\"\\nüìä R√©sultats moyens de la validation crois√©e :\\n{results_df.mean().to_string()}\")\n    return results_df\n\n# 5. üìä ANALYSE DES SEUILS DE D√âCISION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üìä OPTIMISATION DES SEUILS\")\nprint(\"=\"*80)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    \"\"\"Analyse approfondie des seuils de classification\"\"\"\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    \n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        # Gestion des cas o√π une classe n'est pas pr√©dite\n        with np.errstate(divide='ignore', invalid='ignore'):\n            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n            precision = precision_score(y_true, y_pred, zero_division=0)\n        \n        metrics.append({\n            'threshold': thresh,\n            'f2': f2,\n            'recall': recall_score(y_true, y_pred, zero_division=0),\n            'precision': precision,\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    \n    metrics_df = pd.DataFrame(metrics)\n    \n    if plot:\n        try:\n            plt.figure(figsize=(12, 7))\n            plt.plot(metrics_df['threshold'], metrics_df['f2'], label='F2 Score', lw=2)\n            plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall (Sensibilit√©)', linestyle='--')\n            plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linestyle=':')\n            plt.plot(metrics_df['threshold'], metrics_df['specificity'], label='Specificity', linestyle='-.')\n            plt.title(\"Analyse des M√©triques par Seuil de D√©cision\")\n            plt.xlabel(\"Seuil de Classification\")\n            plt.ylabel(\"Score\")\n            plt.grid(True)\n            plt.legend()\n            plt.show()\n        except Exception as e:\n            print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    \n    return metrics_df\n\n# 6. üí∞ ANALYSE CO√õT DES ERREURS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üí∞ IMPACT M√âTIER\")\nprint(\"=\"*80)\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    \n    cost_matrix = {\n        'fn_cost': 1000,\n        'fp_cost': 200,\n        'tp_gain': 500,\n        'tn_gain': 50\n    }\n    \n    total_cost = (fn * cost_matrix['fn_cost'] + \n                 fp * cost_matrix['fp_cost'] - \n                 tp * cost_matrix['tp_gain'] - \n                 tn * cost_matrix['tn_gain'])\n    \n    print(\"\\nüí∞ Bilan des co√ªts :\")\n    print(f\"- Faux N√©gatifs (FN): {fn:3d} √ó {cost_matrix['fn_cost']}‚Ç¨ = {fn * cost_matrix['fn_cost']:7d}‚Ç¨\")\n    print(f\"- Faux Positifs (FP): {fp:3d} √ó {cost_matrix['fp_cost']}‚Ç¨ = {fp * cost_matrix['fp_cost']:7d}‚Ç¨\")\n    print(f\"üí∞ Bilan des gains :\")\n    print(f\"- Vrais Positifs (TP): {tp:3d} √ó {cost_matrix['tp_gain']}‚Ç¨ = {tp * cost_matrix['tp_gain']:7d}‚Ç¨\")\n    print(f\"- Vrais N√©gatifs (TN): {tn:3d} √ó {cost_matrix['tn_gain']}‚Ç¨ = {tn * cost_matrix['tn_gain']:7d}‚Ç¨\")\n    print(\"-\" * 30)\n    print(f\"‚Üí IMPACT TOTAL (Co√ªt - Gain): {total_cost:7d}‚Ç¨\")\n    \n    return {\n        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n        'cost_analysis': cost_matrix,\n        'total_cost': int(total_cost)\n    }\n    \n# 7. üöÄ OPTIMISATION AVEC CHECKPOINTS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üöÄ OPTIMISATION DES HYPERPARAM√àTRES\")\nprint(\"=\"*80)\n\ndef create_study_with_checkpoints(study_name):\n    \"\"\"Cr√©e ou charge une √©tude Optuna\"\"\"\n    CHECKPOINT_DIR = \"optuna_checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    storage_name = f\"sqlite:///{CHECKPOINT_DIR}/{study_name}.db\"\n    \n    study = optuna.create_study(\n        direction='maximize',\n        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n        study_name=study_name,\n        storage=storage_name,\n        load_if_exists=True\n    )\n    \n    if study.trials:\n        print(f\"\\nüîç [Expert] Chargement d'une √©tude existante depuis {storage_name}\")\n        print(f\"üìö Essais pr√©c√©dents charg√©s: {len(study.trials)}\")\n    else:\n        print(f\"\\nüîç [Expert] Cr√©ation d'une nouvelle √©tude dans {storage_name}\")\n        \n    return study\n\ndef objective(trial, X, y):\n    \"\"\"Fonction objective pour Optuna\"\"\"\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'random_state': RANDOM_STATE,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n    }\n    \n    # CORRECTION: Le FeatureEngineer est maintenant la premi√®re √©tape\n    pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**params))\n    ])\n    \n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    \n    return mean_f2\n\n# 8. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. üè≠ PIPELINE FINAL\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, best_params, version=\"1.0.0\"):\n    \"\"\"Entra√Æne et sauvegarde le mod√®le final\"\"\"\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le final (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    # CORRECTION: Le FeatureEngineer est la premi√®re √©tape du pipeline final\n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE))\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    print(f\"üíæ Mod√®le sauvegard√©: {model_path}\")\n    \n    metadata = {\n        \"model_version\": version,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features_input\": list(X_train.columns),\n        \"best_params\": best_params,\n        \"metrics_config\": {\n            \"optimization_metric\": \"f2_score\",\n            \"recall_constraint\": eval_config.alert_thresholds['recall']\n        },\n        \"data_schema\": {\n            \"numeric_features\": numeric_features,\n            \"categorical_features\": categorical_features\n        }\n    }\n    \n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4, default=str) # Ajout de default=str pour s√©rialiser\n    print(f\"üìù M√©tadonn√©es sauvegard√©es: {metadata_path}\")\n    \n    return final_pipeline, MODEL_DIR\n\n# 9. üìä VISUALISATIONS CLINIQUES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. üìä ANALYSE DES PERFORMANCES\")\nprint(\"=\"*80)\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Courbe de calibration robuste\"\"\"\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6))\n        plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\")\n        plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\n# =============================================================================\n# 10. üèÅ PIPELINE COMPLET CORRIG√â ET ROBUSTE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"10. üèÅ PIPELINE COMPLET AVEC GESTION D'ERREURS\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Orchestration robuste avec gestion des erreurs et logging\")\nprint(\"üë∂ [Enfant] On lance tout en √©tant pr√™t √† r√©parer si quelque chose casse\")\n\ndef run_full_pipeline(X, y, n_trials=50, version=\"1.0.0\"):\n    \"\"\"Ex√©cute l'ensemble du pipeline avec gestion robuste des erreurs\"\"\"\n    try:\n        # 1. Validation et pr√©paration des donn√©es\n        print(\"\\n[√âtape 1/5] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        if not isinstance(X, pd.DataFrame): X = pd.DataFrame(X)\n        if not isinstance(y, pd.Series): y = pd.Series(y)\n        \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n        )\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        # 2. Optimisation hyperparam√©trique\n        print(\"\\n[√âtape 2/5] üöÄ Optimisation des hyperparam√®tres\")\n        study = create_study_with_checkpoints(f\"frailty_detection_v{version}\")\n        \n        study.optimize(\n            lambda trial: objective(trial, X_train, y_train),\n            n_trials=n_trials,\n            show_progress_bar=True\n        )\n\n        if not study.best_trial:\n            raise ValueError(\"Aucun essai valide dans l'√©tude Optuna. V√©rifiez les contraintes.\")\n        \n        best_params = study.best_trial.params\n        print(f\"‚öôÔ∏è Meilleurs param√®tres trouv√©s: {best_params}\")\n\n        # 3. Entra√Ænement du mod√®le final\n        print(\"\\n[√âtape 3/5] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, best_params, version=version)\n        print(f\"‚úÖ Mod√®le final entra√Æn√© et sauvegard√© dans {model_dir}\")\n\n        # 4. √âvaluation compl√®te sur le jeu de test\n        print(\"\\n[√âtape 4/5] üìà √âvaluation compl√®te sur le jeu de test\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        # 5. Sauvegarde du rapport final\n        print(\"\\n[√âtape 5/5] üìù G√©n√©ration du rapport final\")\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        \n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√© avec succ√®s! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise\ndef run_full_pipeline(X, y, n_trials=50, version=\"1.0.0\"):\n    \"\"\"Ex√©cute l'ensemble du pipeline avec gestion robuste des erreurs\"\"\"\n    try:\n        # ... (√âtapes 1 et 2 sont inchang√©es jusqu'√† study.optimize) ...\n        print(\"\\n[√âtape 1/5] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        if not isinstance(X, pd.DataFrame): X = pd.DataFrame(X)\n        if not isinstance(y, pd.Series): y = pd.Series(y)\n        \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n        )\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/5] üöÄ Optimisation des hyperparam√®tres\")\n        study = create_study_with_checkpoints(f\"frailty_detection_v{version}\")\n        \n        study.optimize(\n            lambda trial: objective(trial, X_train, y_train),\n            n_trials=n_trials,\n            show_progress_bar=True\n        )\n\n        # CORRECTION : G√©rer le cas o√π aucun essai n'a r√©ussi\n        try:\n            best_trial = study.best_trial\n            best_params = best_trial.params\n            print(f\"‚öôÔ∏è Meilleurs param√®tres trouv√©s (valeur F2 = {best_trial.value:.4f}): {best_params}\")\n        except ValueError:\n            print(\"‚ùå ERREUR D'OPTIMISATION : Aucun essai n'a pu √™tre compl√©t√© avec succ√®s.\")\n            print(\"   ‚Ü≥ Cause probable : la contrainte de rappel est trop stricte et tous les essais ont √©t√© √©lagu√©s.\")\n            print(\"   ‚Ü≥ Suggestion : Baissez le seuil `recall` dans `EvaluationConfig` ou augmentez `n_trials`.\")\n            # On arr√™te le pipeline ici car on ne peut pas continuer sans mod√®le\n            return None, None\n\n        # ... (Le reste de la fonction est inchang√©) ...\n        print(\"\\n[√âtape 3/5] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, best_params, version=version)\n        print(f\"‚úÖ Mod√®le final entra√Æn√© et sauvegard√© dans {model_dir}\")\n\n        # 4. √âvaluation compl√®te sur le jeu de test\n        print(\"\\n[√âtape 4/5] üìà √âvaluation compl√®te sur le jeu de test\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        # 5. Sauvegarde du rapport final\n        print(\"\\n[√âtape 5/5] üìù G√©n√©ration du rapport final\")\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        \n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√© avec succ√®s! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT (AVEC CORRECTIONS FINALES)\n# =============================================================================\n\nif __name__ == \"__main__\":\n    try:\n        # 1. G√©n√©ration de donn√©es synth√©tiques r√©alistes\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\")\n        print(\"=\"*80)\n\n        print(\"\\nüîç [Expert] G√©n√©ration de donn√©es synth√©tiques compatibles\")\n        print(\"üë∂ [Enfant] On cr√©e de fausses donn√©es pour tester notre machine\")\n        data = {\n            'age': np.random.normal(75, 8, 1000).clip(60, 95),\n            'heart_rate': np.random.normal(78, 12, 1000).clip(50, 120),\n            'systolic_bp': np.random.normal(130, 20, 1000).clip(90, 180),\n            'diastolic_bp': np.random.normal(80, 10, 1000).clip(60, 110),\n            'grip_strength': np.random.normal(25, 6, 1000).clip(5, 45),\n            'mobility_score': np.random.uniform(1, 10, 1000),\n            'comorbidities_count': np.random.poisson(2.5, 1000),\n            'medication_count': np.random.poisson(4, 1000),\n            'gender': np.random.choice(['M', 'F'], 1000, p=[0.4, 0.6])\n        }\n        X = pd.DataFrame(data)\n\n        # CORRECTION PRINCIPALE : Ajustement de la logique de g√©n√©ration de la cible\n        # pour garantir un m√©lange de classes.\n        # On centre le score pour avoir des probabilit√©s plus vari√©es autour de 0.5.\n        frailty_score_raw = (X['age'] * 0.05 - X['grip_strength'] * 0.1 - X['mobility_score'] * 0.1)\n        frailty_score_centered = frailty_score_raw - frailty_score_raw.mean()\n        prob = 1 / (1 + np.exp(-frailty_score_centered)) # Sigmoid plus √©quilibr√©\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n\n        print(f\"\\nüìä Donn√©es g√©n√©r√©es: {X.shape[0]} lignes, {X.shape[1]} colonnes\")\n        print(f\"üéØ Distribution de la cible (corrig√©e): \\n{y.value_counts(normalize=True).round(2)}\")\n\n        # AM√âLIORATION DE LA ROBUSTESSE : V√©rification avant de lancer le pipeline\n        if y.nunique() < 2:\n            raise ValueError(\n                \"ERREUR CRITIQUE : La variable cible ne contient qu'une seule classe. \"\n                \"Le pipeline de classification binaire ne peut pas continuer.\"\n            )\n        print(\"‚úÖ La variable cible contient bien deux classes. Lancement du pipeline.\")\n\n        # 2. Ex√©cution du pipeline complet\n        # On r√©duit le nombre d'essais pour une ex√©cution rapide\n        model, report = run_full_pipeline(X, y, n_trials=10, version=\"1.0.2\")\n\n        print(\"\\nüéâüéâüéâ PIPELINE EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n\n    except Exception as e:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        # L'erreur est d√©j√† trac√©e dans run_full_pipeline, mais on peut ajouter un log ici","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T16:22:25.331131Z","iopub.execute_input":"2025-06-22T16:22:25.331616Z","iopub.status.idle":"2025-06-22T16:23:03.040435Z","shell.execute_reply.started":"2025-06-22T16:22:25.331588Z","shell.execute_reply":"2025-06-22T16:23:03.039579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC EXPLICATIONS D√âTAILL√âES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire une machine intelligente qui peut rep√©rer les personnes fragiles\")\nprint(\"\\nüí° Alternatives :\")\nprint(\"- Pour des projets simples : utiliser sklearn sans pipeline\")\nprint(\"- Pour le big data : utiliser Spark ML ou Dask\")\nprint(\"- Pour l'IA embarqu√©e : utiliser ONNX Runtime ou TensorFlow Lite\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Importation des librairies essentielles pour le machine learning industriel\")\nprint(\"üë∂ [Enfant] On sort toutes les bo√Ætes √† outils dont on aura besoin\")\nprint(\"\\nüí° Gestion des d√©pendances :\")\nprint(\"- En entreprise : utiliser un environnement conda/pip freeze\")\nprint(\"- En production : conteneur Docker avec versions fig√©es\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\n\n# ... autres imports ...\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport optuna\n# ...\n\nprint(\"\\nüîç [Expert] Importation des composants scikit-learn pour le pipeline\")\nprint(\"üë∂ [Enfant] On prend les pi√®ces pour construire notre machine\")\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nprint(\"\\nüîç [Expert] M√©triques pour l'√©valuation robuste en milieu clinique\")\nprint(\"üë∂ [Enfant] On choisit comment juger si notre machine fonctionne bien\")\n\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score,\n                           precision_score, confusion_matrix, classification_report,\n                           precision_recall_curve, roc_curve, average_precision_score,\n                           make_scorer, balanced_accuracy_score)\n\nprint(\"\\nüîç [Expert] LightGBM pour des mod√®les performants et interpr√©tables\")\nprint(\"üë∂ [Enfant] On prend un moteur puissant mais qu'on peut comprendre\")\n\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.calibration import calibration_curve\n\nprint(\"\\nüîç [Expert] Configuration de base pour la reproductibilit√©\")\nprint(\"üë∂ [Enfant] On r√®gle notre machine pour qu'elle donne toujours les m√™mes r√©sultats\")\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n# Correction pour compatibilit√© avec diff√©rentes versions de matplotlib/seaborn\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] D√©finition des KPI m√©tier pour √©valuer le mod√®le\")\nprint(\"üë∂ [Enfant] On d√©cide comment noter notre machine\")\nprint(\"\\nüí° Choix des m√©triques :\")\nprint(\"- Probl√®me √©quilibr√© : accuracy et AUC\")\nprint(\"- D√©s√©quilibre mod√©r√© : F1-score\")\nprint(\"- Cas critique (comme ici) : F2-score privil√©giant le recall\")\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation\"\"\"\n    def __init__(self):\n        print(\"\\nüîç [Expert] Initialisation des m√©triques avec contraintes m√©tier\")\n        print(\"üë∂ [Enfant] On pr√©pare notre carte de notation\")\n        \n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        \n        print(\"\\nüîç [Expert] D√©finition des seuils d'alerte cliniques\")\n        print(\"üë∂ [Enfant] On fixe les notes en dessous desquelles c'est inqui√©tant\")\n        \n        self.alert_thresholds = {\n            'roc_auc': 0.75,\n            'f2_score': 0.6,\n            'recall': 0.5\n        }\n\n    def _specificity(self, y_true, y_pred):\n        \"\"\"Calcul sp√©cifique de la sp√©cificit√© (True Negative Rate)\"\"\"\n        # Utilisation de ravel() pour g√©rer les cas o√π la matrice est 1D\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        # Ajout d'une protection contre la division par z√©ro\n        if (tn + fp) == 0:\n            return 0.0\n        return tn / (tn + fp)\n\nprint(\"\\nüîç [Expert] Instanciation de la configuration d'√©valuation\")\nprint(\"üë∂ [Enfant] On active notre syst√®me de notation\")\n\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT (CORRIG√â)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (CORRIG√â)\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Cr√©ation de features cliniques pertinentes\")\nprint(\"üë∂ [Enfant] On ajoute des mesures utiles que notre machine pourra comprendre\")\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    CORRECTION : Ing√©nierie de caract√©ristiques robuste qui v√©rifie les colonnes.\n    Cette √©tape doit s'ex√©cuter sur un DataFrame, avant que les noms de colonnes\n    ne soient perdus par les transformateurs num√©riques/cat√©goriels.\n    \"\"\"\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Validation robuste que les colonnes n√©cessaires existent\n        required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n        if not required_cols.issubset(X_copy.columns):\n            missing = required_cols - set(X_copy.columns)\n            raise ValueError(f\"Colonnes manquantes pour FeatureEngineer: {missing}\")\n\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        \n        # CORRECTION : Retourne le DataFrame avec les nouvelles colonnes\n        return X_copy\n\nprint(\"\\nüîç [Expert] Configuration du pr√©processeur industriel\")\nprint(\"üë∂ [Enfant] On pr√©pare les √©tapes pour nettoyer et pr√©parer les donn√©es\")\n\nnumeric_features = ['age', 'heart_rate', 'systolic_bp', 'diastolic_bp',\n                   'grip_strength', 'mobility_score', 'comorbidities_count',\n                   'medication_count', 'pulse_pressure', 'bp_mobility_interaction']\ncategorical_features = ['gender']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', RobustScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# 4. üéØ FONCTIONS D'√âVALUATION INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ VALIDATION ROBUSTE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvaluation compl√®te avec validation crois√©e\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    \n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2),\n        'recall': 'recall',\n        'precision': 'precision',\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    \n    # CORRECTION : Ajout de error_score='raise' pour un d√©bogage plus facile\n    cv_results = cross_validate(\n        pipeline, X, y,\n        cv=cv,\n        scoring=scoring,\n        return_train_score=True,\n        n_jobs=-1,\n        error_score='raise'\n    )\n    \n    results_df = pd.DataFrame(cv_results)\n    print(f\"\\nüìä R√©sultats moyens de la validation crois√©e :\\n{results_df.mean().to_string()}\")\n    return results_df\n\n# 5. üìä ANALYSE DES SEUILS DE D√âCISION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üìä OPTIMISATION DES SEUILS\")\nprint(\"=\"*80)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    \"\"\"Analyse approfondie des seuils de classification\"\"\"\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    \n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        # Gestion des cas o√π une classe n'est pas pr√©dite\n        with np.errstate(divide='ignore', invalid='ignore'):\n            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n            precision = precision_score(y_true, y_pred, zero_division=0)\n        \n        metrics.append({\n            'threshold': thresh,\n            'f2': f2,\n            'recall': recall_score(y_true, y_pred, zero_division=0),\n            'precision': precision,\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    \n    metrics_df = pd.DataFrame(metrics)\n    \n    if plot:\n        try:\n            plt.figure(figsize=(12, 7))\n            plt.plot(metrics_df['threshold'], metrics_df['f2'], label='F2 Score', lw=2)\n            plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall (Sensibilit√©)', linestyle='--')\n            plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linestyle=':')\n            plt.plot(metrics_df['threshold'], metrics_df['specificity'], label='Specificity', linestyle='-.')\n            plt.title(\"Analyse des M√©triques par Seuil de D√©cision\")\n            plt.xlabel(\"Seuil de Classification\")\n            plt.ylabel(\"Score\")\n            plt.grid(True)\n            plt.legend()\n            plt.show()\n        except Exception as e:\n            print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    \n    return metrics_df\n\n# 6. üí∞ ANALYSE CO√õT DES ERREURS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üí∞ IMPACT M√âTIER\")\nprint(\"=\"*80)\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    \n    cost_matrix = {\n        'fn_cost': 1000,\n        'fp_cost': 200,\n        'tp_gain': 500,\n        'tn_gain': 50\n    }\n    \n    total_cost = (fn * cost_matrix['fn_cost'] + \n                 fp * cost_matrix['fp_cost'] - \n                 tp * cost_matrix['tp_gain'] - \n                 tn * cost_matrix['tn_gain'])\n    \n    print(\"\\nüí∞ Bilan des co√ªts :\")\n    print(f\"- Faux N√©gatifs (FN): {fn:3d} √ó {cost_matrix['fn_cost']}‚Ç¨ = {fn * cost_matrix['fn_cost']:7d}‚Ç¨\")\n    print(f\"- Faux Positifs (FP): {fp:3d} √ó {cost_matrix['fp_cost']}‚Ç¨ = {fp * cost_matrix['fp_cost']:7d}‚Ç¨\")\n    print(f\"üí∞ Bilan des gains :\")\n    print(f\"- Vrais Positifs (TP): {tp:3d} √ó {cost_matrix['tp_gain']}‚Ç¨ = {tp * cost_matrix['tp_gain']:7d}‚Ç¨\")\n    print(f\"- Vrais N√©gatifs (TN): {tn:3d} √ó {cost_matrix['tn_gain']}‚Ç¨ = {tn * cost_matrix['tn_gain']:7d}‚Ç¨\")\n    print(\"-\" * 30)\n    print(f\"‚Üí IMPACT TOTAL (Co√ªt - Gain): {total_cost:7d}‚Ç¨\")\n    \n    return {\n        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n        'cost_analysis': cost_matrix,\n        'total_cost': int(total_cost)\n    }\n    \n# 7. üöÄ OPTIMISATION AVEC CHECKPOINTS\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üöÄ OPTIMISATION DES HYPERPARAM√àTRES\")\nprint(\"=\"*80)\n\ndef create_study_with_checkpoints(study_name):\n    \"\"\"Cr√©e ou charge une √©tude Optuna\"\"\"\n    CHECKPOINT_DIR = \"optuna_checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    storage_name = f\"sqlite:///{CHECKPOINT_DIR}/{study_name}.db\"\n    \n    study = optuna.create_study(\n        direction='maximize',\n        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n        study_name=study_name,\n        storage=storage_name,\n        load_if_exists=True\n    )\n    \n    if study.trials:\n        print(f\"\\nüîç [Expert] Chargement d'une √©tude existante depuis {storage_name}\")\n        print(f\"üìö Essais pr√©c√©dents charg√©s: {len(study.trials)}\")\n    else:\n        print(f\"\\nüîç [Expert] Cr√©ation d'une nouvelle √©tude dans {storage_name}\")\n        \n    return study\n\n\"\"\"\ndef objective(trial, X, y):\n    #Fonction objective pour Optuna\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'random_state': RANDOM_STATE,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n    }\n    \n    # CORRECTION: Le FeatureEngineer est maintenant la premi√®re √©tape\n    pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(**params))\n    ])\n    \n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    \n    return mean_f2\n\n\"\"\"\n\n# =============================================================================\n# 7. üöÄ OPTIMISATION AVEC CHECKPOINTS (MODIFI√â POUR MULTI-MOD√àLES)\n# =============================================================================\n\ndef objective(trial, X, y):\n    \"\"\"\n    AM√âLIORATION : Fonction objective qui optimise √† la fois le type de mod√®le\n    et ses hyperparam√®tres.\n    \"\"\"\n    \n    # √âtape 1 : Choisir le type de classifieur\n    classifier_name = trial.suggest_categorical(\"classifier\", [\"LGBM\", \"RandomForest\", \"LogisticRegression\"])\n    \n    # √âtape 2 : D√©finir les hyperparam√®tres en fonction du classifieur choisi\n    if classifier_name == \"LGBM\":\n        params = {\n            'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1, 'random_state': RANDOM_STATE,\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n        }\n        classifier_obj = lgb.LGBMClassifier(**params)\n\n    elif classifier_name == \"RandomForest\":\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n            'random_state': RANDOM_STATE,\n            'n_jobs': -1\n        }\n        classifier_obj = RandomForestClassifier(**params)\n\n    else: # LogisticRegression\n        # La r√©gression logistique est plus sensible √† la mise √† l'√©chelle des donn√©es,\n        # d'o√π l'importance d'avoir StandardScaler dans le pr√©processeur.\n        params = {\n            'C': trial.suggest_float('C', 1e-4, 100, log=True),\n            'solver': 'liblinear', # Bon pour les petits datasets\n            'random_state': RANDOM_STATE\n        }\n        classifier_obj = LogisticRegression(**params)\n\n    # Le reste de la fonction est le m√™me, on ins√®re simplement le classifieur choisi\n    pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor), # Il est crucial que ce pr√©processeur normalise bien les donn√©es pour la r√©gression logistique\n        ('classifier', classifier_obj)\n    ])\n    \n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    # La contrainte de rappel s'applique √† tous les mod√®les\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} ({classifier_name}) √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    \n    return mean_f2\n\n# 8. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. üè≠ PIPELINE FINAL\")\nprint(\"=\"*80)\n\"\"\"\"\"\"\n# =============================================================================\n# 8. üè≠ ENTRA√éNEMENT ET VERSIONNING (MODIFI√â POUR MULTI-MOD√àLES)\n# =============================================================================\n\ndef train_final_model(X_train, y_train, best_params, version=\"1.0.0\"):\n    \"\"\"\n    AM√âLIORATION : Entra√Æne et sauvegarde le mod√®le final en fonction du type\n    de classifieur choisi par Optuna.\n    \"\"\"\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le final (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    # Extraire le nom du classifieur et ses param√®tres\n    best_params_copy = best_params.copy()\n    classifier_name = best_params_copy.pop(\"classifier\")\n\n    print(f\"ü§ñ Type de mod√®le s√©lectionn√© : {classifier_name}\")\n    \n    if classifier_name == \"LGBM\":\n        classifier_obj = lgb.LGBMClassifier(**best_params_copy, random_state=RANDOM_STATE)\n    elif classifier_name == \"RandomForest\":\n        classifier_obj = RandomForestClassifier(**best_params_copy, random_state=RANDOM_STATE, n_jobs=-1)\n    elif classifier_name == \"LogisticRegression\":\n        classifier_obj = LogisticRegression(**best_params_copy, random_state=RANDOM_STATE)\n    else:\n        raise ValueError(f\"Type de classifieur inconnu : {classifier_name}\")\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    print(f\"üíæ Mod√®le sauvegard√©: {model_path}\")\n    \n    # Le reste est inchang√©...\n    metadata = {\n        \"model_version\": version,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features_input\": list(X_train.columns),\n        \"best_params\": best_params, # On sauvegarde les params complets, incluant le nom du classifieur\n        \"metrics_config\": {\n            \"optimization_metric\": \"f2_score\",\n            \"recall_constraint\": eval_config.alert_thresholds['recall']\n        },\n        \"data_schema\": {\n            \"numeric_features\": numeric_features,\n            \"categorical_features\": categorical_features\n        }\n    }\n    \n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4, default=str)\n    print(f\"üìù M√©tadonn√©es sauvegard√©es: {metadata_path}\")\n    \n    return final_pipeline, MODEL_DIR\n# 9. üìä VISUALISATIONS CLINIQUES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. üìä ANALYSE DES PERFORMANCES\")\nprint(\"=\"*80)\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Courbe de calibration robuste\"\"\"\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6))\n        plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\")\n        plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\n# =============================================================================\n# 10. üèÅ PIPELINE COMPLET CORRIG√â ET ROBUSTE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"10. üèÅ PIPELINE COMPLET AVEC GESTION D'ERREURS\")\nprint(\"=\"*80)\n\nprint(\"\\nüîç [Expert] Orchestration robuste avec gestion des erreurs et logging\")\nprint(\"üë∂ [Enfant] On lance tout en √©tant pr√™t √† r√©parer si quelque chose casse\")\n\ndef run_full_pipeline(X, y, n_trials=50, version=\"1.0.0\"):\n    \"\"\"Ex√©cute l'ensemble du pipeline avec gestion robuste des erreurs\"\"\"\n    try:\n        # 1. Validation et pr√©paration des donn√©es\n        print(\"\\n[√âtape 1/5] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        if not isinstance(X, pd.DataFrame): X = pd.DataFrame(X)\n        if not isinstance(y, pd.Series): y = pd.Series(y)\n        \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n        )\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        # 2. Optimisation hyperparam√©trique\n        print(\"\\n[√âtape 2/5] üöÄ Optimisation des hyperparam√®tres\")\n        study = create_study_with_checkpoints(f\"frailty_detection_v{version}\")\n        \n        study.optimize(\n            lambda trial: objective(trial, X_train, y_train),\n            n_trials=n_trials,\n            show_progress_bar=True\n        )\n\n        if not study.best_trial:\n            raise ValueError(\"Aucun essai valide dans l'√©tude Optuna. V√©rifiez les contraintes.\")\n        \n        best_params = study.best_trial.params\n        print(f\"‚öôÔ∏è Meilleurs param√®tres trouv√©s: {best_params}\")\n\n        # 3. Entra√Ænement du mod√®le final\n        print(\"\\n[√âtape 3/5] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, best_params, version=version)\n        print(f\"‚úÖ Mod√®le final entra√Æn√© et sauvegard√© dans {model_dir}\")\n\n        # 4. √âvaluation compl√®te sur le jeu de test\n        print(\"\\n[√âtape 4/5] üìà √âvaluation compl√®te sur le jeu de test\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        # 5. Sauvegarde du rapport final\n        print(\"\\n[√âtape 5/5] üìù G√©n√©ration du rapport final\")\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        \n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√© avec succ√®s! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise\ndef run_full_pipeline(X, y, n_trials=50, version=\"1.0.0\"):\n    \"\"\"Ex√©cute l'ensemble du pipeline avec gestion robuste des erreurs\"\"\"\n    try:\n        # ... (√âtapes 1 et 2 sont inchang√©es jusqu'√† study.optimize) ...\n        print(\"\\n[√âtape 1/5] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        if not isinstance(X, pd.DataFrame): X = pd.DataFrame(X)\n        if not isinstance(y, pd.Series): y = pd.Series(y)\n        \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n        )\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/5] üöÄ Optimisation des hyperparam√®tres\")\n        study = create_study_with_checkpoints(f\"frailty_detection_v{version}\")\n        \n        study.optimize(\n            lambda trial: objective(trial, X_train, y_train),\n            n_trials=n_trials,\n            show_progress_bar=True\n        )\n\n        # CORRECTION : G√©rer le cas o√π aucun essai n'a r√©ussi\n        try:\n            best_trial = study.best_trial\n            best_params = best_trial.params\n            print(f\"‚öôÔ∏è Meilleurs param√®tres trouv√©s (valeur F2 = {best_trial.value:.4f}): {best_params}\")\n        except ValueError:\n            print(\"‚ùå ERREUR D'OPTIMISATION : Aucun essai n'a pu √™tre compl√©t√© avec succ√®s.\")\n            print(\"   ‚Ü≥ Cause probable : la contrainte de rappel est trop stricte et tous les essais ont √©t√© √©lagu√©s.\")\n            print(\"   ‚Ü≥ Suggestion : Baissez le seuil `recall` dans `EvaluationConfig` ou augmentez `n_trials`.\")\n            # On arr√™te le pipeline ici car on ne peut pas continuer sans mod√®le\n            return None, None\n\n        # ... (Le reste de la fonction est inchang√©) ...\n        print(\"\\n[√âtape 3/5] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, best_params, version=version)\n        print(f\"‚úÖ Mod√®le final entra√Æn√© et sauvegard√© dans {model_dir}\")\n\n        # 4. √âvaluation compl√®te sur le jeu de test\n        print(\"\\n[√âtape 4/5] üìà √âvaluation compl√®te sur le jeu de test\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        # 5. Sauvegarde du rapport final\n        print(\"\\n[√âtape 5/5] üìù G√©n√©ration du rapport final\")\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        \n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√© avec succ√®s! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT (AVEC CORRECTIONS FINALES)\n# =============================================================================\n\nif __name__ == \"__main__\":\n    try:\n        # 1. G√©n√©ration de donn√©es synth√©tiques r√©alistes\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\")\n        print(\"=\"*80)\n\n        print(\"\\nüîç [Expert] G√©n√©ration de donn√©es synth√©tiques compatibles\")\n        print(\"üë∂ [Enfant] On cr√©e de fausses donn√©es pour tester notre machine\")\n        data = {\n            'age': np.random.normal(75, 8, 1000).clip(60, 95),\n            'heart_rate': np.random.normal(78, 12, 1000).clip(50, 120),\n            'systolic_bp': np.random.normal(130, 20, 1000).clip(90, 180),\n            'diastolic_bp': np.random.normal(80, 10, 1000).clip(60, 110),\n            'grip_strength': np.random.normal(25, 6, 1000).clip(5, 45),\n            'mobility_score': np.random.uniform(1, 10, 1000),\n            'comorbidities_count': np.random.poisson(2.5, 1000),\n            'medication_count': np.random.poisson(4, 1000),\n            'gender': np.random.choice(['M', 'F'], 1000, p=[0.4, 0.6])\n        }\n        X = pd.DataFrame(data)\n\n        # CORRECTION PRINCIPALE : Ajustement de la logique de g√©n√©ration de la cible\n        # pour garantir un m√©lange de classes.\n        # On centre le score pour avoir des probabilit√©s plus vari√©es autour de 0.5.\n        frailty_score_raw = (X['age'] * 0.05 - X['grip_strength'] * 0.1 - X['mobility_score'] * 0.1)\n        frailty_score_centered = frailty_score_raw - frailty_score_raw.mean()\n        prob = 1 / (1 + np.exp(-frailty_score_centered)) # Sigmoid plus √©quilibr√©\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n\n        print(f\"\\nüìä Donn√©es g√©n√©r√©es: {X.shape[0]} lignes, {X.shape[1]} colonnes\")\n        print(f\"üéØ Distribution de la cible (corrig√©e): \\n{y.value_counts(normalize=True).round(2)}\")\n\n        # AM√âLIORATION DE LA ROBUSTESSE : V√©rification avant de lancer le pipeline\n        if y.nunique() < 2:\n            raise ValueError(\n                \"ERREUR CRITIQUE : La variable cible ne contient qu'une seule classe. \"\n                \"Le pipeline de classification binaire ne peut pas continuer.\"\n            )\n        print(\"‚úÖ La variable cible contient bien deux classes. Lancement du pipeline.\")\n\n        # 2. Ex√©cution du pipeline complet\n        # On r√©duit le nombre d'essais pour une ex√©cution rapide\n        model, report = run_full_pipeline(X, y, n_trials=10, version=\"1.0.2\")\n\n        print(\"\\nüéâüéâüéâ PIPELINE EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n\n    except Exception as e:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        # L'erreur est d√©j√† trac√©e dans run_full_pipeline, mais on peut ajouter un log ici\n\"\"\"Vous avez transform√© votre pipeline d'optimisation en un mini-syst√®me d'AutoML (Automated Machine Learning), capable de trouver non seulement les meilleurs r√©glages, mais aussi le meilleur algorithme pour votre probl√®me. C'est une √©tape majeure vers un pipeline de production plus intelligent et performant.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:07:28.225501Z","iopub.execute_input":"2025-06-23T06:07:28.225743Z","iopub.status.idle":"2025-06-23T06:07:44.930901Z","shell.execute_reply.started":"2025-06-23T06:07:28.225725Z","shell.execute_reply":"2025-06-23T06:07:44.930038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC COMPARAISON DE MOD√àLES ET EXPLICATIONS\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire la meilleure machine intelligente possible\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\nimport traceback\n\n# Imports pour les mod√®les\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport optuna\n\n# Imports pour le pipeline et le pr√©traitement\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Imports pour les m√©triques et l'√©valuation\nfrom sklearn.metrics import (\n    roc_auc_score, fbeta_score, recall_score,\n    precision_score, confusion_matrix, classification_report,\n    make_scorer, balanced_accuracy_score, calibration_curve\n)\n\n# Configuration globale pour la reproductibilit√©\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation et des seuils m√©tier.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        self.alert_thresholds = {\n            'roc_auc': 0.70,\n            'f2_score': 0.55,\n            'recall': 0.50  # Seuil r√©aliste pour commencer\n        }\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n\n    def _specificity(self, y_true, y_pred):\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\n\neval_config = EvaluationConfig()\n\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES ET PR√âTRAITEMENT\")\nprint(\"=\"*80)\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des caract√©ristiques cliniques suppl√©mentaires.\"\"\"\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n        if not required_cols.issubset(X_copy.columns):\n            raise ValueError(f\"Colonnes manquantes pour FeatureEngineer: {required_cols - set(X_copy.columns)}\")\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        return X_copy\n\nnumeric_features = [\n    'age', 'heart_rate', 'systolic_bp', 'diastolic_bp', 'grip_strength', \n    'mobility_score', 'comorbidities_count', 'medication_count', \n    'pulse_pressure', 'bp_mobility_interaction'\n]\ncategorical_features = ['gender']\n\n# Utilisation de StandardScaler pour une meilleure compatibilit√© avec LogisticRegression\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\nprint(\"‚úÖ Pr√©processeur configur√© (Imputation, Scaling, One-Hot-Encoding).\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvalue un pipeline avec une validation crois√©e stratifi√©e.\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2, zero_division=0),\n        'recall': make_scorer(recall_score, zero_division=0),\n        'precision': make_scorer(precision_score, zero_division=0),\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    \"\"\"Analyse les m√©triques pour diff√©rents seuils de d√©cision.\"\"\"\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score(y_true, y_pred, zero_division=0),\n            'precision': precision_score(y_true, y_pred, zero_division=0),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(12, 7))\n            plt.plot(metrics_df['threshold'], metrics_df['f2'], label='F2 Score', lw=2)\n            plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall', linestyle='--')\n            plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linestyle=':')\n            plt.plot(metrics_df['threshold'], metrics_df['specificity'], label='Specificity', linestyle='-.')\n            plt.title(\"Analyse des M√©triques par Seuil de D√©cision\")\n            plt.xlabel(\"Seuil de Classification\"); plt.ylabel(\"Score\")\n            plt.grid(True); plt.legend(); plt.show()\n        except Exception as e:\n            print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions du mod√®le.\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    cost_matrix = {'fn_cost': 1000, 'fp_cost': 200, 'tp_gain': 500, 'tn_gain': 50}\n    total_cost = (fn * cost_matrix['fn_cost'] + fp * cost_matrix['fp_cost'] - \n                  tp * cost_matrix['tp_gain'] - tn * cost_matrix['tn_gain'])\n    print(\"\\nüí∞ Bilan Co√ªt/B√©n√©fice :\")\n    print(f\"‚Üí IMPACT TOTAL (Co√ªt - Gain): {total_cost:7d}‚Ç¨\")\n    return {'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n            'cost_analysis': cost_matrix, 'total_cost': int(total_cost)}\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Trace la courbe de calibration des probabilit√©s du mod√®le.\"\"\"\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6))\n        plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (STRUCTURE EN TOURNOI)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    \"\"\"Cr√©e ou charge une √©tude Optuna persistante via SQLite.\"\"\"\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    storage_name = f\"sqlite:///optuna_checkpoints/{study_name}.db\"\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=storage_name, load_if_exists=True)\n\ndef run_optimization(objective_func, study, X, y, n_trials):\n    \"\"\"Lance une session d'optimisation pour une √©tude donn√©e.\"\"\"\n    try:\n        study.optimize(lambda trial: objective_func(trial, X, y), n_trials=n_trials, show_progress_bar=True)\n        return study.best_trial\n    except Exception as e:\n        print(f\"L'optimisation pour {study.study_name} a √©chou√© ou a √©t√© compl√®tement √©lagu√©e: {e}\")\n        return None\n\n# --- Fonctions objectives sp√©cifiques par mod√®le ---\ndef _generic_objective(trial, X, y, classifier_obj):\n    pipeline = Pipeline([('fe', FeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    return mean_f2\n\ndef objective_lgbm(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 40),\n        'random_state': RANDOM_STATE, 'objective': 'binary', 'verbosity': -1\n    }\n    return _generic_objective(trial, X, y, lgb.LGBMClassifier(**params))\n\ndef objective_rf(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'random_state': RANDOM_STATE, 'n_jobs': -1\n    }\n    return _generic_objective(trial, X, y, RandomForestClassifier(**params))\n\ndef objective_logreg(trial, X, y):\n    params = {'C': trial.suggest_float('C', 1e-3, 100, log=True),\n              'solver': 'liblinear', 'random_state': RANDOM_STATE}\n    return _generic_objective(trial, X, y, LogisticRegression(**params))\n\n# --- Orchestrateur de l'optimisation (le \"tournoi\") ---\ndef optimize_models(X, y, n_trials_per_model, version):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    models_to_test = {\"LGBM\": objective_lgbm, \"RandomForest\": objective_rf, \"LogisticRegression\": objective_logreg}\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- Optimisation pour le mod√®le : {name} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        best_trial = run_optimization(objective_func, study, X, y, n_trials_per_model)\n        \n        if best_trial:\n            best_trials[name] = best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {best_trial.value:.4f}\")\n        else:\n            print(f\"‚ö†Ô∏è Aucun essai valide trouv√© pour {name}.\")\n\n    if not best_trials:\n        raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    champion_trial = best_trials[champion_name]\n    \n    print(\"\\n\" + \"=\"*40)\n    print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\")\n    print(f\"   Meilleur F2-score global : {champion_trial.value:.4f}\")\n    print(\"=\"*40)\n    \n    return champion_name, champion_trial.params\n\nprint(\"‚úÖ Structure d'optimisation multi-mod√®les pr√™te.\")\n\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version):\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    if champion_name == \"LGBM\":\n        classifier_obj = lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE)\n    elif champion_name == \"RandomForest\":\n        classifier_obj = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, n_jobs=-1)\n    elif champion_name == \"LogisticRegression\":\n        classifier_obj = LogisticRegression(**best_params, random_state=RANDOM_STATE)\n    else:\n        raise ValueError(f\"Type de classifieur inconnu : {champion_name}\")\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    \n    metadata = {\n        \"model_version\": version,\n        \"champion_model\": champion_name,\n        \"best_params\": best_params,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features_input\": list(X_train.columns),\n        \"data_schema\": {'numeric_features': numeric_features, 'categorical_features': categorical_features}\n    }\n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=20, version=\"1.0.0\"):\n    try:\n        # √âtape 1: Validation et pr√©paration\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        X = pd.DataFrame(X).copy()\n        y = pd.Series(y).copy()\n        if y.nunique() < 2:\n            raise ValueError(\"ERREUR CRITIQUE : La variable cible ne contient qu'une seule classe.\")\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        # √âtape 2: Optimisation multi-mod√®les\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version)\n\n        # √âtape 3: Entra√Ænement du mod√®le champion\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version)\n\n        # √âtape 4: √âvaluation finale et rapport\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2, zero_division=0),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        traceback.print_exc()\n        raise\n\nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\")\n        print(\"=\"*80)\n\n        data = {\n            'age': np.random.normal(75, 8, 1000).clip(60, 95),\n            'heart_rate': np.random.normal(78, 12, 1000).clip(50, 120),\n            'systolic_bp': np.random.normal(130, 20, 1000).clip(90, 180),\n            'diastolic_bp': np.random.normal(80, 10, 1000).clip(60, 110),\n            'grip_strength': np.random.normal(25, 6, 1000).clip(5, 45),\n            'mobility_score': np.random.uniform(1, 10, 1000),\n            'comorbidities_count': np.random.poisson(2.5, 1000),\n            'medication_count': np.random.poisson(4, 1000),\n            'gender': np.random.choice(['M', 'F'], 1000, p=[0.4, 0.6])\n        }\n        X = pd.DataFrame(data)\n        \n        frailty_score_raw = (X['age'] * 0.05 - X['grip_strength'] * 0.1 - X['mobility_score'] * 0.1)\n        frailty_score_centered = frailty_score_raw - frailty_score_raw.mean()\n        prob = 1 / (1 + np.exp(-frailty_score_centered))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        \n        # Lancer le pipeline avec 15 essais par mod√®le\n        model, report = run_full_pipeline(X, y, n_trials_per_model=15, version=\"2.0.0-multi-model\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE MULTI-MOD√àLE EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:52:08.012101Z","iopub.execute_input":"2025-06-23T07:52:08.012409Z","iopub.status.idle":"2025-06-23T07:52:16.962497Z","shell.execute_reply.started":"2025-06-23T07:52:08.012388Z","shell.execute_reply":"2025-06-23T07:52:16.961052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC COMPARAISON DE MOD√àLES ET EXPLICATIONS\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire la meilleure machine intelligente possible\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\nimport traceback\n\n# Imports pour les mod√®les\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport optuna\n\n# Imports pour le pipeline et le pr√©traitement\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Imports pour les m√©triques et l'√©valuation\nfrom sklearn.metrics import (\n    roc_auc_score, fbeta_score, recall_score,\n    precision_score, confusion_matrix, classification_report,\n    make_scorer, balanced_accuracy_score\n)\n# CORRECTION : Import de calibration_curve depuis le bon module\nfrom sklearn.calibration import calibration_curve\n\n# Configuration globale pour la reproductibilit√©\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation et des seuils m√©tier.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        self.alert_thresholds = {\n            'roc_auc': 0.70,\n            'f2_score': 0.55,\n            'recall': 0.50  # Seuil r√©aliste pour commencer\n        }\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n\n    def _specificity(self, y_true, y_pred):\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\n\neval_config = EvaluationConfig()\n\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES ET PR√âTRAITEMENT\")\nprint(\"=\"*80)\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des caract√©ristiques cliniques suppl√©mentaires.\"\"\"\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n        if not required_cols.issubset(X_copy.columns):\n            raise ValueError(f\"Colonnes manquantes pour FeatureEngineer: {required_cols - set(X_copy.columns)}\")\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        return X_copy\n\nnumeric_features = [\n    'age', 'heart_rate', 'systolic_bp', 'diastolic_bp', 'grip_strength', \n    'mobility_score', 'comorbidities_count', 'medication_count', \n    'pulse_pressure', 'bp_mobility_interaction'\n]\ncategorical_features = ['gender']\n\n# Utilisation de StandardScaler pour une meilleure compatibilit√© avec LogisticRegression\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\nprint(\"‚úÖ Pr√©processeur configur√© (Imputation, Scaling, One-Hot-Encoding).\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvalue un pipeline avec une validation crois√©e stratifi√©e.\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2, zero_division=0),\n        'recall': make_scorer(recall_score, zero_division=0),\n        'precision': make_scorer(precision_score, zero_division=0),\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    \"\"\"Analyse les m√©triques pour diff√©rents seuils de d√©cision.\"\"\"\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score(y_true, y_pred, zero_division=0),\n            'precision': precision_score(y_true, y_pred, zero_division=0),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(12, 7))\n            plt.plot(metrics_df['threshold'], metrics_df['f2'], label='F2 Score', lw=2)\n            plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall', linestyle='--')\n            plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linestyle=':')\n            plt.plot(metrics_df['threshold'], metrics_df['specificity'], label='Specificity', linestyle='-.')\n            plt.title(\"Analyse des M√©triques par Seuil de D√©cision\")\n            plt.xlabel(\"Seuil de Classification\"); plt.ylabel(\"Score\")\n            plt.grid(True); plt.legend(); plt.show()\n        except Exception as e:\n            print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions du mod√®le.\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    cost_matrix = {'fn_cost': 1000, 'fp_cost': 200, 'tp_gain': 500, 'tn_gain': 50}\n    total_cost = (fn * cost_matrix['fn_cost'] + fp * cost_matrix['fp_cost'] - \n                  tp * cost_matrix['tp_gain'] - tn * cost_matrix['tn_gain'])\n    print(\"\\nüí∞ Bilan Co√ªt/B√©n√©fice :\")\n    print(f\"‚Üí IMPACT TOTAL (Co√ªt - Gain): {total_cost:7d}‚Ç¨\")\n    return {'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n            'cost_analysis': cost_matrix, 'total_cost': int(total_cost)}\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Trace la courbe de calibration des probabilit√©s du mod√®le.\"\"\"\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6))\n        plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (STRUCTURE EN TOURNOI)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    \"\"\"Cr√©e ou charge une √©tude Optuna persistante via SQLite.\"\"\"\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    storage_name = f\"sqlite:///optuna_checkpoints/{study_name}.db\"\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=storage_name, load_if_exists=True)\n\ndef run_optimization(objective_func, study, X, y, n_trials):\n    \"\"\"Lance une session d'optimisation pour une √©tude donn√©e.\"\"\"\n    try:\n        study.optimize(lambda trial: objective_func(trial, X, y), n_trials=n_trials, show_progress_bar=True)\n        return study.best_trial\n    except Exception as e:\n        print(f\"L'optimisation pour {study.study_name} a √©chou√© ou a √©t√© compl√®tement √©lagu√©e: {e}\")\n        return None\n\n# --- Fonctions objectives sp√©cifiques par mod√®le ---\ndef _generic_objective(trial, X, y, classifier_obj):\n    pipeline = Pipeline([('fe', FeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    return mean_f2\n\ndef objective_lgbm(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 40),\n        'random_state': RANDOM_STATE, 'objective': 'binary', 'verbosity': -1\n    }\n    return _generic_objective(trial, X, y, lgb.LGBMClassifier(**params))\n\ndef objective_rf(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'random_state': RANDOM_STATE, 'n_jobs': -1\n    }\n    return _generic_objective(trial, X, y, RandomForestClassifier(**params))\n\ndef objective_logreg(trial, X, y):\n    params = {'C': trial.suggest_float('C', 1e-3, 100, log=True),\n              'solver': 'liblinear', 'random_state': RANDOM_STATE}\n    return _generic_objective(trial, X, y, LogisticRegression(**params))\n\n\n# --- Orchestrateur de l'optimisation (le \"tournoi\") ---\ndef optimize_models(X, y, n_trials_per_model, version):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    models_to_test = {\"LGBM\": objective_lgbm, \"RandomForest\": objective_rf, \"LogisticRegression\": objective_logreg}\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- Optimisation pour le mod√®le : {name} ---\")\n        study_name = f\"frailty_{name}_v{version}\"\n        study = create_study(study_name)\n        best_trial = run_optimization(objective_func, study, X, y, n_trials_per_model)\n        \n        if best_trial:\n            best_trials[name] = best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {best_trial.value:.4f}\")\n        else:\n            print(f\"‚ö†Ô∏è Aucun essai valide trouv√© pour {name}.\")\n\n    if not best_trials:\n        raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    champion_trial = best_trials[champion_name]\n    \n    print(\"\\n\" + \"=\"*40)\n    print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\")\n    print(f\"   Meilleur F2-score global : {champion_trial.value:.4f}\")\n    print(\"=\"*40)\n    \n    return champion_name, champion_trial.params\n\nprint(\"‚úÖ Structure d'optimisation multi-mod√®les pr√™te.\")\n\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version):\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    if champion_name == \"LGBM\":\n        classifier_obj = lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE)\n    elif champion_name == \"RandomForest\":\n        classifier_obj = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, n_jobs=-1)\n    elif champion_name == \"LogisticRegression\":\n        classifier_obj = LogisticRegression(**best_params, random_state=RANDOM_STATE)\n    else:\n        raise ValueError(f\"Type de classifieur inconnu : {champion_name}\")\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    \n    metadata = {\n        \"model_version\": version,\n        \"champion_model\": champion_name,\n        \"best_params\": best_params,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features_input\": list(X_train.columns),\n        \"data_schema\": {'numeric_features': numeric_features, 'categorical_features': categorical_features}\n    }\n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=20, version=\"1.0.0\"):\n    try:\n        # √âtape 1: Validation et pr√©paration\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        X = pd.DataFrame(X).copy()\n        y = pd.Series(y).copy()\n        if y.nunique() < 2:\n            raise ValueError(\"ERREUR CRITIQUE : La variable cible ne contient qu'une seule classe.\")\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        # √âtape 2: Optimisation multi-mod√®les\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version)\n\n        # √âtape 3: Entra√Ænement du mod√®le champion\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version)\n\n        # √âtape 4: √âvaluation finale et rapport\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2, zero_division=0),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        traceback.print_exc()\n        raise\n\nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\")\n        print(\"=\"*80)\n\n        data = {\n            'age': np.random.normal(75, 8, 1000).clip(60, 95),\n            'heart_rate': np.random.normal(78, 12, 1000).clip(50, 120),\n            'systolic_bp': np.random.normal(130, 20, 1000).clip(90, 180),\n            'diastolic_bp': np.random.normal(80, 10, 1000).clip(60, 110),\n            'grip_strength': np.random.normal(25, 6, 1000).clip(5, 45),\n            'mobility_score': np.random.uniform(1, 10, 1000),\n            'comorbidities_count': np.random.poisson(2.5, 1000),\n            'medication_count': np.random.poisson(4, 1000),\n            'gender': np.random.choice(['M', 'F'], 1000, p=[0.4, 0.6])\n        }\n        X = pd.DataFrame(data)\n        \n        frailty_score_raw = (X['age'] * 0.05 - X['grip_strength'] * 0.1 - X['mobility_score'] * 0.1)\n        frailty_score_centered = frailty_score_raw - frailty_score_raw.mean()\n        prob = 1 / (1 + np.exp(-frailty_score_centered))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        \n        # Lancer le pipeline avec 15 essais par mod√®le\n        model, report = run_full_pipeline(X, y, n_trials_per_model=15, version=\"2.0.0-multi-model\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE MULTI-MOD√àLE EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:54:24.983758Z","iopub.execute_input":"2025-06-23T07:54:24.984199Z","iopub.status.idle":"2025-06-23T07:56:08.802021Z","shell.execute_reply.started":"2025-06-23T07:54:24.984170Z","shell.execute_reply":"2025-06-23T07:56:08.800520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC COMPARAISON DE MOD√àLES ET EXPLICATIONS\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare tous les outils pour construire la meilleure machine intelligente possible\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nfrom dataclasses import dataclass\nimport json\nimport traceback\n\n# Imports pour les mod√®les\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC  # NOUVEAU MOD√àLE\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier # NOUVEAU MOD√àLE\nimport optuna\n\n# Imports pour le pipeline et le pr√©traitement\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Imports pour les m√©triques et l'√©valuation\nfrom sklearn.metrics import (\n    roc_auc_score, fbeta_score, recall_score,\n    precision_score, confusion_matrix, classification_report,\n    make_scorer, balanced_accuracy_score\n)\nfrom sklearn.calibration import calibration_curve\n\n# Configuration globale pour la reproductibilit√©\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation et des seuils m√©tier.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score,\n            'precision': precision_score,\n            'specificity': self._specificity,\n            'balanced_accuracy': balanced_accuracy_score\n        }\n        self.alert_thresholds = {\n            'roc_auc': 0.70,\n            'f2_score': 0.55,\n            'recall': 0.50\n        }\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n\n    def _specificity(self, y_true, y_pred):\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\n\neval_config = EvaluationConfig()\n\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES ET PR√âTRAITEMENT\")\nprint(\"=\"*80)\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des caract√©ristiques cliniques suppl√©mentaires.\"\"\"\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n        if not required_cols.issubset(X_copy.columns):\n            raise ValueError(f\"Colonnes manquantes pour FeatureEngineer: {required_cols - set(X_copy.columns)}\")\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        return X_copy\n\nnumeric_features = [\n    'age', 'heart_rate', 'systolic_bp', 'diastolic_bp', 'grip_strength', \n    'mobility_score', 'comorbidities_count', 'medication_count', \n    'pulse_pressure', 'bp_mobility_interaction'\n]\ncategorical_features = ['gender']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\nprint(\"‚úÖ Pr√©processeur configur√© (Imputation, Scaling, One-Hot-Encoding).\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvalue un pipeline avec une validation crois√©e stratifi√©e.\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {\n        'roc_auc': 'roc_auc',\n        'f2': make_scorer(fbeta_score, beta=2, zero_division=0),\n        'recall': make_scorer(recall_score, zero_division=0),\n        'precision': make_scorer(precision_score, zero_division=0),\n        'balanced_accuracy': 'balanced_accuracy'\n    }\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    \"\"\"Analyse les m√©triques pour diff√©rents seuils de d√©cision.\"\"\"\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({\n            'threshold': thresh,\n            'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score(y_true, y_pred, zero_division=0),\n            'precision': precision_score(y_true, y_pred, zero_division=0),\n            'specificity': eval_config.metrics['specificity'](y_true, y_pred)\n        })\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(12, 7))\n            plt.plot(metrics_df['threshold'], metrics_df['f2'], label='F2 Score', lw=2)\n            plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall', linestyle='--')\n            plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linestyle=':')\n            plt.plot(metrics_df['threshold'], metrics_df['specificity'], label='Specificity', linestyle='-.')\n            plt.title(\"Analyse des M√©triques par Seuil de D√©cision\")\n            plt.xlabel(\"Seuil de Classification\"); plt.ylabel(\"Score\")\n            plt.grid(True); plt.legend(); plt.show()\n        except Exception as e:\n            print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef cost_analysis(y_true, y_pred):\n    \"\"\"Analyse l'impact financier des pr√©dictions du mod√®le.\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    cost_matrix = {'fn_cost': 1000, 'fp_cost': 200, 'tp_gain': 500, 'tn_gain': 50}\n    total_cost = (fn * cost_matrix['fn_cost'] + fp * cost_matrix['fp_cost'] - \n                  tp * cost_matrix['tp_gain'] - tn * cost_matrix['tn_gain'])\n    print(\"\\nüí∞ Bilan Co√ªt/B√©n√©fice :\")\n    print(f\"‚Üí IMPACT TOTAL (Co√ªt - Gain): {total_cost:7d}‚Ç¨\")\n    return {'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n            'cost_analysis': cost_matrix, 'total_cost': int(total_cost)}\n\ndef plot_calibration(y_true, y_probs):\n    \"\"\"Trace la courbe de calibration des probabilit√©s du mod√®le.\"\"\"\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6))\n        plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (STRUCTURE EN TOURNOI)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    \"\"\"Cr√©e ou charge une √©tude Optuna persistante via SQLite.\"\"\"\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    storage_name = f\"sqlite:///optuna_checkpoints/{study_name}.db\"\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=storage_name, load_if_exists=True)\n\ndef run_optimization(objective_func, study, X, y, n_trials):\n    \"\"\"Lance une session d'optimisation pour une √©tude donn√©e.\"\"\"\n    try:\n        study.optimize(lambda trial: objective_func(trial, X, y), n_trials=n_trials, show_progress_bar=True)\n        return study.best_trial\n    except Exception as e:\n        print(f\"L'optimisation pour {study.study_name} a √©chou√© ou a √©t√© compl√®tement √©lagu√©e: {e}\")\n        return None\n\n# --- Fonctions objectives sp√©cifiques par mod√®le ---\ndef _generic_objective(trial, X, y, classifier_obj):\n    pipeline = Pipeline([('fe', FeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    return mean_f2\n\ndef objective_lgbm(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 40),\n        'random_state': RANDOM_STATE, 'objective': 'binary', 'verbosity': -1\n    }\n    return _generic_objective(trial, X, y, lgb.LGBMClassifier(**params))\n\ndef objective_rf(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'random_state': RANDOM_STATE, 'n_jobs': -1\n    }\n    return _generic_objective(trial, X, y, RandomForestClassifier(**params))\n\ndef objective_logreg(trial, X, y):\n    params = {'C': trial.suggest_float('C', 1e-3, 100, log=True),\n              'solver': 'liblinear', 'random_state': RANDOM_STATE}\n    return _generic_objective(trial, X, y, LogisticRegression(**params))\n\n# NOUVEAU MOD√àLE : CatBoost\ndef objective_catboost(trial, X, y):\n    params = {\n        'iterations': trial.suggest_int('iterations', 100, 800),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n        'random_seed': RANDOM_STATE, 'verbose': 0\n    }\n    return _generic_objective(trial, X, y, CatBoostClassifier(**params))\n\n# NOUVEAU MOD√àLE : SVC\ndef objective_svc(trial, X, y):\n    params = {\n        'C': trial.suggest_float('C', 1e-2, 1e2, log=True),\n        'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n        'kernel': 'rbf', 'probability': True, 'random_state': RANDOM_STATE\n    }\n    return _generic_objective(trial, X, y, SVC(**params))\n\n\n# --- Orchestrateur de l'optimisation (le \"tournoi\") ---\ndef optimize_models(X, y, n_trials_per_model, version):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    # NOUVEAU MOD√àLE : Ajout au dictionnaire du tournoi\n    models_to_test = {\n        \"LGBM\": objective_lgbm,\n        \"RandomForest\": objective_rf,\n        \"LogisticRegression\": objective_logreg,\n        \"CatBoost\": objective_catboost,\n        \"SVC\": objective_svc\n    }\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- Optimisation pour le mod√®le : {name} ---\")\n        study_name = f\"frailty_{name}_v{version}\"\n        study = create_study(study_name)\n        best_trial = run_optimization(objective_func, study, X, y, n_trials_per_model)\n        \n        if best_trial:\n            best_trials[name] = best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {best_trial.value:.4f}\")\n        else:\n            print(f\"‚ö†Ô∏è Aucun essai valide trouv√© pour {name}.\")\n\n    if not best_trials:\n        raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    champion_trial = best_trials[champion_name]\n    \n    print(\"\\n\" + \"=\"*40)\n    print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\")\n    print(f\"   Meilleur F2-score global : {champion_trial.value:.4f}\")\n    print(\"=\"*40)\n    \n    return champion_name, champion_trial.params\n\nprint(\"‚úÖ Structure d'optimisation multi-mod√®les pr√™te.\")\n\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version):\n    print(f\"\\nüîç [Expert] Entra√Ænement du mod√®le champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    # NOUVEAU MOD√àLE : Ajout de la logique d'instanciation\n    if champion_name == \"LGBM\":\n        classifier_obj = lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE)\n    elif champion_name == \"RandomForest\":\n        classifier_obj = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, n_jobs=-1)\n    elif champion_name == \"LogisticRegression\":\n        classifier_obj = LogisticRegression(**best_params, random_state=RANDOM_STATE)\n    elif champion_name == \"CatBoost\":\n        classifier_obj = CatBoostClassifier(**best_params, random_seed=RANDOM_STATE, verbose=0)\n    elif champion_name == \"SVC\":\n        classifier_obj = SVC(**best_params, probability=True, random_state=RANDOM_STATE)\n    else:\n        raise ValueError(f\"Type de classifieur inconnu : {champion_name}\")\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    model_path = os.path.join(MODEL_DIR, 'model.joblib')\n    joblib.dump(final_pipeline, model_path)\n    \n    metadata = {\n        \"model_version\": version,\n        \"champion_model\": champion_name,\n        \"best_params\": best_params,\n        \"training_date\": pd.Timestamp.now().isoformat(),\n        \"features_input\": list(X_train.columns),\n        \"data_schema\": {'numeric_features': numeric_features, 'categorical_features': categorical_features}\n    }\n    metadata_path = os.path.join(MODEL_DIR, 'metadata.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=20, version=\"1.0.0\"):\n    try:\n        # √âtape 1: Validation et pr√©paration\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Validation et pr√©paration des donn√©es\")\n        X = pd.DataFrame(X).copy()\n        y = pd.Series(y).copy()\n        if y.nunique() < 2:\n            raise ValueError(\"ERREUR CRITIQUE : La variable cible ne contient qu'une seule classe.\")\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        # √âtape 2: Optimisation multi-mod√®les\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version)\n\n        # √âtape 3: Entra√Ænement du mod√®le champion\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version)\n\n        # √âtape 4: √âvaluation finale et rapport\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]\n        y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\")\n        print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        cost_results = cost_analysis(y_test, y_pred)\n        plot_calibration(y_test, y_probs)\n\n        report = {\n            \"model_info\": json.load(open(os.path.join(model_dir, 'metadata.json'))),\n            \"test_metrics\": {\n                \"roc_auc\": roc_auc_score(y_test, y_probs),\n                \"f2_score_default_threshold\": fbeta_score(y_test, y_pred, beta=2, zero_division=0),\n                \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n            },\n            \"cost_analysis\": cost_results,\n            \"optimal_threshold_for_f2\": threshold_metrics.loc[threshold_metrics['f2'].idxmax()].to_dict(),\n        }\n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n            \n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {report_path}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå\")\n        print(f\"Message d'erreur: {e}\")\n        traceback.print_exc()\n        # Ne pas propager l'erreur pour permettre au script de finir proprement dans un notebook\n        # Dans un script de production, on pourrait vouloir 'raise'\n        \nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\")\n        print(\"=\"*80)\n\n        data = {\n            'age': np.random.normal(75, 8, 1000).clip(60, 95),\n            'heart_rate': np.random.normal(78, 12, 1000).clip(50, 120),\n            'systolic_bp': np.random.normal(130, 20, 1000).clip(90, 180),\n            'diastolic_bp': np.random.normal(80, 10, 1000).clip(60, 110),\n            'grip_strength': np.random.normal(25, 6, 1000).clip(5, 45),\n            'mobility_score': np.random.uniform(1, 10, 1000),\n            'comorbidities_count': np.random.poisson(2.5, 1000),\n            'medication_count': np.random.poisson(4, 1000),\n            'gender': np.random.choice(['M', 'F'], 1000, p=[0.4, 0.6])\n        }\n        X = pd.DataFrame(data)\n        \n        frailty_score_raw = (X['age'] * 0.05 - X['grip_strength'] * 0.1 - X['mobility_score'] * 0.1)\n        frailty_score_centered = frailty_score_raw - frailty_score_raw.mean()\n        prob = 1 / (1 + np.exp(-frailty_score_centered))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        \n        # Lancer le pipeline avec 10 essais par mod√®le pour un test rapide\n        model, report = run_full_pipeline(X, y, n_trials_per_model=10, version=\"3.0.0-full-tournament\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE MULTI-MOD√àLE EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T08:23:49.424403Z","iopub.execute_input":"2025-06-23T08:23:49.425215Z","iopub.status.idle":"2025-06-23T08:26:48.892780Z","shell.execute_reply.started":"2025-06-23T08:23:49.425185Z","shell.execute_reply":"2025-06-23T08:26:48.891738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET AVEC TOURNOI DE 16 MOD√àLES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline industriel pour la d√©tection de fragilit√©\")\nprint(\"üë∂ [Enfant] On pr√©pare un grand tournoi pour trouver la meilleure machine intelligente !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\n\n# --- Imports pour le Machine Learning ---\n# Outil d'optimisation\nimport optuna\n\n# Mod√®les pour le tournoi (16 au total)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Outils de pipeline et de pr√©traitement\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Outils d'√©valuation\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\nprint(\"‚úÖ Environnement configur√© avec 16 mod√®les pr√™ts pour le tournoi.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation et des seuils m√©tier.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score, 'precision': precision_score,\n            'specificity': self._specificity, 'balanced_accuracy': balanced_accuracy_score\n        }\n        self.alert_thresholds = {'recall': 0.50} # Seuil principal pour l'√©lagage\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n\n    def _specificity(self, y_true, y_pred):\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\n\neval_config = EvaluationConfig()\n\n\n# 3. üõ†Ô∏è PIPELINE DE PR√âTRAITEMENT\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES ET PR√âTRAITEMENT\")\nprint(\"=\"*80)\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des caract√©ristiques cliniques suppl√©mentaires.\"\"\"\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy(); required_cols = {'systolic_bp', 'diastolic_bp', 'mobility_score'}\n        if not required_cols.issubset(X_copy.columns):\n            raise ValueError(f\"Colonnes manquantes: {required_cols - set(X_copy.columns)}\")\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['bp_mobility_interaction'] = X_copy['systolic_bp'] * X_copy['mobility_score']\n        return X_copy\n\nnumeric_features = ['age', 'heart_rate', 'systolic_bp', 'diastolic_bp', 'grip_strength', \n                    'mobility_score', 'comorbidities_count', 'medication_count', \n                    'pulse_pressure', 'bp_mobility_interaction']\ncategorical_features = ['gender']\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_features),\n    ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_features)],\n    remainder='passthrough'\n)\nprint(\"‚úÖ Pr√©processeur configur√©.\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE (INCHANG√âES)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvalue un pipeline avec une validation crois√©e stratifi√©e.\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0),\n               'recall': make_scorer(recall_score, zero_division=0)}\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    # (Code inchang√©)\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({'threshold': thresh,\n                        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n                        'recall': recall_score(y_true, y_pred, zero_division=0),\n                        'precision': precision_score(y_true, y_pred, zero_division=0),\n                        'specificity': eval_config.metrics['specificity'](y_true, y_pred)})\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(10, 6)); metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'], ax=plt.gca())\n            plt.title(\"Analyse des M√©triques par Seuil\"); plt.xlabel(\"Seuil\"); plt.ylabel(\"Score\"); plt.grid(True); plt.show()\n        except Exception as e: print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\n# ... (les autres fonctions d'analyse comme cost_analysis et plot_calibration sont suppos√©es exister ici)\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI √âLARGI)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI DE 16)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    storage = f\"sqlite:///optuna_checkpoints/{study_name}.db\"\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=storage, load_if_exists=True)\n\ndef _generic_objective(trial, X, y, classifier_obj):\n    pipeline = Pipeline([('fe', FeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall < seuil\")\n        raise optuna.TrialPruned()\n    return mean_f2\n\n# --- Fonctions objectives pour chaque mod√®le ---\ndef objective_lgbm(t, X, y):\n    p = {'n_estimators': t.suggest_int('n_e', 100, 800), 'learning_rate': t.suggest_float('lr', 1e-3, 0.2, log=True),\n         'num_leaves': t.suggest_int('nl', 10, 40), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    return _generic_objective(t, X, y, lgb.LGBMClassifier(**p))\n\ndef objective_rf(t, X, y):\n    p = {'n_estimators': t.suggest_int('n_e', 100, 800), 'max_depth': t.suggest_int('md', 4, 15),\n         'min_samples_split': t.suggest_int('mss', 2, 20), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    return _generic_objective(t, X, y, RandomForestClassifier(**p))\n\ndef objective_logreg(t, X, y):\n    p = {'C': t.suggest_float('C', 1e-3, 100, log=True), 'solver': 'liblinear', 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, LogisticRegression(**p))\n\ndef objective_xgb(t, X, y):\n    p = {'n_estimators': t.suggest_int('n_e', 100, 800), 'learning_rate': t.suggest_float('lr', 0.01, 0.3),\n         'max_depth': t.suggest_int('md', 3, 10), 'subsample': t.suggest_float('ss', 0.6, 1.0), 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, xgb.XGBClassifier(**p, use_label_encoder=False, eval_metric='logloss'))\n\ndef objective_catboost(t, X, y):\n    p = {'iterations': t.suggest_int('it', 100, 800), 'depth': t.suggest_int('d', 4, 10),\n         'learning_rate': t.suggest_float('lr', 1e-3, 0.2, log=True), 'random_seed': RANDOM_STATE, 'verbose': 0}\n    return _generic_objective(t, X, y, CatBoostClassifier(**p))\n\ndef objective_svc(t, X, y):\n    p = {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'gamma': t.suggest_float('g', 1e-4, 1e-1, log=True),\n         'kernel': 'rbf', 'probability': True, 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, SVC(**p))\n\ndef objective_knn(t, X, y):\n    p = {'n_neighbors': t.suggest_int('k', 3, 30), 'weights': t.suggest_categorical('w', ['uniform', 'distance'])}\n    return _generic_objective(t, X, y, KNeighborsClassifier(**p))\n\ndef objective_gnb(t, X, y):\n    p = {'var_smoothing': t.suggest_float('vs', 1e-10, 1e-8, log=True)}\n    return _generic_objective(t, X, y, GaussianNB(**p))\n\ndef objective_dt(t, X, y):\n    p = {'max_depth': t.suggest_int('md', 3, 20), 'min_samples_split': t.suggest_int('mss', 2, 20), 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, DecisionTreeClassifier(**p))\n\ndef objective_adaboost(t, X, y):\n    p = {'n_estimators': t.suggest_int('n_e', 50, 500), 'learning_rate': t.suggest_float('lr', 0.01, 1.0), 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, AdaBoostClassifier(**p))\n\ndef objective_gb(t, X, y):\n    p = {'n_estimators': t.suggest_int('n_e', 100, 500), 'learning_rate': t.suggest_float('lr', 0.01, 0.2),\n         'max_depth': t.suggest_int('md', 3, 8), 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, GradientBoostingClassifier(**p))\n\ndef objective_et(t, X, y):\n    p = {'n_estimators': t.suggest_int('n_e', 100, 800), 'max_depth': t.suggest_int('md', 4, 15),\n         'min_samples_split': t.suggest_int('mss', 2, 20), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    return _generic_objective(t, X, y, ExtraTreesClassifier(**p))\n\ndef objective_lda(t, X, y):\n    p = {'solver': 'lsqr', 'shrinkage': t.suggest_float('s', 0.0, 1.0)}\n    return _generic_objective(t, X, y, LinearDiscriminantAnalysis(**p))\n\ndef objective_qda(t, X, y):\n    p = {'reg_param': t.suggest_float('rp', 0.0, 1.0)}\n    return _generic_objective(t, X, y, QuadraticDiscriminantAnalysis(**p))\n\ndef objective_mlp(t, X, y):\n    p = {'hidden_layer_sizes': (t.suggest_int('hls', 50, 200),), 'alpha': t.suggest_float('a', 1e-5, 1e-1, log=True),\n         'learning_rate_init': t.suggest_float('lri', 1e-4, 1e-2, log=True), 'max_iter': 500, 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, MLPClassifier(**p))\n    \ndef objective_sgd(t, X, y):\n    p = {'loss': 'log_loss', 'penalty': 'elasticnet', 'alpha': t.suggest_float('a', 1e-5, 1e-1, log=True),\n         'l1_ratio': t.suggest_float('l1r', 0, 1), 'max_iter': 1000, 'tol': 1e-3, 'random_state': RANDOM_STATE}\n    return _generic_objective(t, X, y, SGDClassifier(**p))\n\n# --- Orchestrateur du tournoi ---\ndef optimize_models(X, y, n_trials_per_model, version):\n    print(\"\\nüîç Lancement du grand tournoi des mod√®les...\")\n    models_to_test = {\n        \"LogisticRegression\": objective_logreg, \"RandomForest\": objective_rf, \"LGBM\": objective_lgbm,\n        \"XGB\": objective_xgb, \"CatBoost\": objective_catboost, \"SVC\": objective_svc,\n        \"KNN\": objective_knn, \"GaussianNB\": objective_gnb, \"DecisionTree\": objective_dt,\n        \"AdaBoost\": objective_adaboost, \"GradientBoosting\": objective_gb, \"ExtraTrees\": objective_et,\n        \"LDA\": objective_lda, \"QDA\": objective_qda, \"MLP\": objective_mlp, \"SGD\": objective_sgd,\n    }\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        best_trial = run_optimization(objective_func, study, X, y, n_trials_per_model)\n        if best_trial:\n            best_trials[name] = best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {best_trial.value:.4f}\")\n        else:\n            print(f\"‚ö†Ô∏è {name} n'a pas pu produire d'essai valide.\")\n\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    champion_trial = best_trials[champion_name]\n    \n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\"); print(f\"   Meilleur F2-score : {champion_trial.value:.4f}\"); print(\"=\"*40)\n    \n    return champion_name, champion_trial.params\n\nprint(\"‚úÖ Structure d'optimisation pour 16 mod√®les pr√™te.\")\n\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU CHAMPION\n# =============================================================================\n\"\"\"\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    model_map = {\n        \"LGBM\": lgb.LGBMClassifier(**best_params, random_state=RANDOM_STATE),\n        \"RandomForest\": RandomForestClassifier(**best_params, random_state=RANDOM_STATE, n_jobs=-1),\n        \"LogisticRegression\": LogisticRegression(**best_params, random_state=RANDOM_STATE),\n        \"XGB\": xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE),\n        \"CatBoost\": CatBoostClassifier(**best_params, random_seed=RANDOM_STATE, verbose=0),\n        \"SVC\": SVC(**best_params, probability=True, random_state=RANDOM_STATE),\n        \"KNN\": KNeighborsClassifier(**best_params),\n        \"GaussianNB\": GaussianNB(**best_params),\n        \"DecisionTree\": DecisionTreeClassifier(**best_params, random_state=RANDOM_STATE),\n        \"AdaBoost\": AdaBoostClassifier(**best_params, random_state=RANDOM_STATE),\n        \"GradientBoosting\": GradientBoostingClassifier(**best_params, random_state=RANDOM_STATE),\n        \"ExtraTrees\": ExtraTreesClassifier(**best_params, random_state=RANDOM_STATE, n_jobs=-1),\n        \"LDA\": LinearDiscriminantAnalysis(**best_params),\n        \"QDA\": QuadraticDiscriminantAnalysis(**best_params),\n        \"MLP\": MLPClassifier(**best_params, max_iter=500, random_state=RANDOM_STATE),\n        \"SGD\": SGDClassifier(**best_params, loss='log_loss', random_state=RANDOM_STATE),\n    }\n\n    classifier_obj = model_map.get(champion_name)\n    if classifier_obj is None: raise ValueError(f\"Classifieur inconnu : {champion_name}\")\n        \n    final_pipeline = Pipeline([('feature_engineering', FeatureEngineer()), ('preprocessor', preprocessor), ('classifier', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    \n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    \n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params, \"training_date\": pd.Timestamp.now().isoformat()}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\"\"\"\n# =============================================================================\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU CHAMPION (CORRIG√â)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params_short, version):\n    \"\"\"\n    CORRECTION: Instancie UNIQUEMENT le mod√®le champion et traduit les noms\n    courts des hyperparam√®tres en noms longs attendus par sklearn.\n    \"\"\"\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    # Dictionnaire de traduction des param√®tres\n    param_map = {\n        'n_e': 'n_estimators', 'lr': 'learning_rate', 'nl': 'num_leaves', 'md': 'max_depth',\n        'mss': 'min_samples_split', 'it': 'iterations', 'd': 'depth', 'g': 'gamma',\n        'k': 'n_neighbors', 'w': 'weights', 'vs': 'var_smoothing', 's': 'shrinkage',\n        'rp': 'reg_param', 'hls': 'hidden_layer_sizes', 'a': 'alpha', 'lri': 'learning_rate_init',\n        'l1r': 'l1_ratio'\n    }\n\n    # Traduire les noms courts en noms longs pour le mod√®le champion\n    best_params_long = {param_map.get(k, k): v for k, v in best_params_short.items()}\n    \n    # Dictionnaire des constructeurs de mod√®les\n    model_constructors = {\n        \"LGBM\": lambda p: lgb.LGBMClassifier(**p, random_state=RANDOM_STATE),\n        \"RandomForest\": lambda p: RandomForestClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1),\n        \"LogisticRegression\": lambda p: LogisticRegression(**p, random_state=RANDOM_STATE),\n        \"XGB\": lambda p: xgb.XGBClassifier(**p, use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE),\n        \"CatBoost\": lambda p: CatBoostClassifier(**p, random_seed=RANDOM_STATE, verbose=0),\n        \"SVC\": lambda p: SVC(**p, probability=True, random_state=RANDOM_STATE),\n        \"KNN\": lambda p: KNeighborsClassifier(**p),\n        \"GaussianNB\": lambda p: GaussianNB(**p),\n        \"DecisionTree\": lambda p: DecisionTreeClassifier(**p, random_state=RANDOM_STATE),\n        \"AdaBoost\": lambda p: AdaBoostClassifier(**p, random_state=RANDOM_STATE),\n        \"GradientBoosting\": lambda p: GradientBoostingClassifier(**p, random_state=RANDOM_STATE),\n        \"ExtraTrees\": lambda p: ExtraTreesClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1),\n        \"LDA\": lambda p: LinearDiscriminantAnalysis(**p),\n        \"QDA\": lambda p: QuadraticDiscriminantAnalysis(**p),\n        \"MLP\": lambda p: MLPClassifier(**p, max_iter=500, random_state=RANDOM_STATE),\n        \"SGD\": lambda p: SGDClassifier(**p, loss='log_loss', random_state=RANDOM_STATE),\n    }\n\n    constructor = model_constructors.get(champion_name)\n    if constructor is None:\n        raise ValueError(f\"Constructeur de classifieur inconnu : {champion_name}\")\n    \n    classifier_obj = constructor(best_params_long)\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    \n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    \n    # Sauvegarder les param√®tres courts (d'Optuna) et longs (du mod√®le)\n    metadata = {\n        \"model_version\": version,\n        \"champion_model\": champion_name,\n        \"optuna_params\": best_params_short,\n        \"model_params\": classifier_obj.get_params(),\n        \"training_date\": pd.Timestamp.now().isoformat()\n    }\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f:\n        json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final corrig√©e et pr√™te.\")\n\n# Le reste du script, notamment la fonction `run_full_pipeline` et le bloc `if __name__ == \"__main__\"`,\n# n'a pas besoin d'√™tre modifi√© car il appelle d√©j√† `train_final_model` avec les bons arguments.\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=20, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Validation et pr√©paration\"); X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        #cost_analysis(y_test, y_pred) # D√©sactiv√© pour la concision de la sortie\n        #plot_calibration(y_test, y_probs) # D√©sactiv√© pour la concision de la sortie\n\n        report = {\"champion\": champion_name, \"test_roc_auc\": roc_auc_score(y_test, y_probs), **best_params}\n        with open(os.path.join(model_dir, 'final_report.json'), 'w') as f: json.dump(report, f, indent=4, default=str)\n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {os.path.join(model_dir, 'final_report.json')}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n        \nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\"); print(\"=\"*80)\n\n        data = {'age': np.random.normal(75, 8, 1000).clip(60, 95), 'heart_rate': np.random.normal(78, 12, 1000).clip(50, 120),\n                'systolic_bp': np.random.normal(130, 20, 1000).clip(90, 180), 'diastolic_bp': np.random.normal(80, 10, 1000).clip(60, 110),\n                'grip_strength': np.random.normal(25, 6, 1000).clip(5, 45), 'mobility_score': np.random.uniform(1, 10, 1000),\n                'comorbidities_count': np.random.poisson(2.5, 1000), 'medication_count': np.random.poisson(4, 1000),\n                'gender': np.random.choice(['M', 'F'], 1000, p=[0.4, 0.6])}\n        X = pd.DataFrame(data)\n        \n        score_raw = (X['age'] * 0.05 - X['grip_strength'] * 0.1 - X['mobility_score'] * 0.1)\n        prob = 1 / (1 + np.exp(-(score_raw - score_raw.mean())))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        \n        # Lancer le pipeline avec 5 essais par mod√®le pour un test rapide\n        model, report = run_full_pipeline(X, y, n_trials_per_model=5, version=\"4.0.0-tournament-16\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE MULTI-MOD√àLE EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T08:37:24.996143Z","iopub.execute_input":"2025-06-23T08:37:24.996643Z","iopub.status.idle":"2025-06-23T08:40:12.924769Z","shell.execute_reply.started":"2025-06-23T08:37:24.996609Z","shell.execute_reply":"2025-06-23T08:40:12.923850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V5 - TOURNOI DE 16 MOD√àLES & FEATURE ENGINEERING AVANC√â\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V5 : Tournoi de 16 mod√®les avec Feature Engineering.\")\nprint(\"üë∂ [Enfant] On organise un grand tournoi pour trouver la meilleure machine intelligente !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√© avec 16 mod√®les pr√™ts pour le tournoi.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.metrics = {'f2_score': lambda y,p: fbeta_score(y,p,beta=2,zero_division=0), 'specificity': self._specificity}\n        self.alert_thresholds = {'recall': 0.60}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n    def _specificity(self, y, p):\n        tn, fp, _, _ = confusion_matrix(y, p, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â\n# =============================================================================\n\"\"\"\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES AVANC√âE\")\nprint(\"=\"*80)\n\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self, rolling_window='6H'):\n        self.rolling_window = rolling_window\n        self.gender_means = {}\n\n    def fit(self, X, y=None):\n        if 'gender' in X.columns and 'grip_strength' in X.columns:\n            self.gender_means = X.groupby('gender')['grip_strength'].mean().to_dict()\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Business-driven features\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X_copy.columns:\n            X_copy['bmi_age_interaction'] = X_copy['bmi'] * X_copy['age']\n        \n        if self.gender_means and 'gender' in X.columns and 'grip_strength' in X.columns:\n            mean_grip = X_copy['gender'].map(self.gender_means).fillna(X['grip_strength'].mean())\n            X_copy['grip_strength_normalized'] = X_copy['grip_strength'] / mean_grip\n        \n        # Temporal features\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            rolling_cols = ['heart_rate', 'pulse_pressure', 'grip_strength']\n            grouped = X_copy.groupby('patient_id')\n            \n            for col in rolling_cols:\n                rolling_mean = grouped[col].rolling(self.rolling_window, on='timestamp', min_periods=1).mean()\n                rolling_std = grouped[col].rolling(self.rolling_window, on='timestamp', min_periods=1).std()\n                X_copy[f'{col}_rolling_mean'] = rolling_mean.reset_index(level=0, drop=True)\n                X_copy[f'{col}_rolling_std'] = rolling_std.reset_index(level=0, drop=True)\n        \n        # Remplir les NaN restants qui pourraient √™tre cr√©√©s par le rolling\n        X_copy.fillna(method='bfill', inplace=True)\n        X_copy.fillna(method='ffill', inplace=True)\n        \n        return X_copy\n\nprint(\"‚úÖ Classe AdvancedFeatureEngineer pr√™te.\")\n\"\"\"\n# =============================================================================\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â (CORRIG√â)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES AVANC√âE\")\nprint(\"=\"*80)\n\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    AM√âLIORATION : Cr√©e des features cliniques et temporelles avanc√©es.\n    Cette classe doit recevoir des donn√©es tri√©es par patient et par temps.\n    \"\"\"\n    def __init__(self, rolling_window='6H'):\n        self.rolling_window = rolling_window\n        self.gender_means = {}\n\n    def fit(self, X, y=None):\n        if 'gender' in X.columns and 'grip_strength' in X.columns:\n            self.gender_means = X.groupby('gender')['grip_strength'].mean().to_dict()\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Business-driven features\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X_copy.columns:\n            X_copy['bmi_age_interaction'] = X_copy['bmi'] * X_copy['age']\n        \n        if self.gender_means and 'gender' in X.columns and 'grip_strength' in X.columns:\n            mean_grip = X_copy['gender'].map(self.gender_means).fillna(X['grip_strength'].mean())\n            X_copy['grip_strength_normalized'] = X_copy['grip_strength'] / mean_grip\n        \n        # Temporal features\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            rolling_cols = ['heart_rate', 'pulse_pressure', 'grip_strength']\n            \n            # CORRECTION : Appliquer .rolling sur le DataFrame group√©, pas sur la S√©rie\n            # Cela permet √† .rolling d'acc√©der √† la colonne 'timestamp'\n            grouped = X_copy.groupby('patient_id')\n            \n            for col in rolling_cols:\n                # Appliquer rolling sur le groupe de DataFrame, puis s√©lectionner la colonne\n                rolling_stats = grouped.rolling(self.rolling_window, on='timestamp', min_periods=1)[col]\n                \n                rolling_mean = rolling_stats.mean()\n                rolling_std = rolling_stats.std()\n                \n                # R√©-aligner les index et assigner\n                X_copy[f'{col}_rolling_mean'] = rolling_mean.reset_index(level=0, drop=True)\n                X_copy[f'{col}_rolling_std'] = rolling_std.reset_index(level=0, drop=True)\n        \n        X_copy.fillna(method='bfill', inplace=True)\n        X_copy.fillna(method='ffill', inplace=True)\n        \n        return X_copy\n\nprint(\"‚úÖ Classe AdvancedFeatureEngineer (corrig√©e) pr√™te.\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), 'recall': make_scorer(recall_score, zero_division=0)}\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({'threshold': thresh,\n                        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n                        'recall': recall_score(y_true, y_pred, zero_division=0),\n                        'precision': precision_score(y_true, y_pred, zero_division=0),\n                        'specificity': eval_config.metrics['specificity'](y_true, y_pred)})\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(10, 6)); metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'], ax=plt.gca())\n            plt.title(\"Analyse des M√©triques par Seuil\"); plt.xlabel(\"Seuil\"); plt.ylabel(\"Score\"); plt.grid(True); plt.legend(); plt.show()\n        except Exception as e: print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef plot_calibration(y_true, y_probs):\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6)); plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e: print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI DE 16)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, classifier_obj):\n    pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({cv_results['test_recall'].mean():.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n    return mean_f2\n\ndef get_model_objectives(preprocessor):\n    objectives = {}\n    obj_fn = partial(_generic_objective, preprocessor=preprocessor)\n\n    # LGBM\n    def obj_lgbm(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n             'num_leaves': t.suggest_int('num_leaves', 10, 40), 'random_state': RANDOM_STATE, 'verbosity': -1}\n        return obj_fn(t, X, y, classifier_obj=lgb.LGBMClassifier(**p))\n    objectives[\"LGBM\"] = obj_lgbm\n\n    # RandomForest\n    def obj_rf(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'max_depth': t.suggest_int('max_depth', 4, 15),\n             'min_samples_split': t.suggest_int('min_samples_split', 2, 20), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n        return obj_fn(t, X, y, classifier_obj=RandomForestClassifier(**p))\n    objectives[\"RandomForest\"] = obj_rf\n\n    # LogisticRegression\n    def obj_logreg(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-3, 100, log=True), 'solver': 'liblinear', 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=LogisticRegression(**p))\n    objectives[\"LogisticRegression\"] = obj_logreg\n\n    # XGBoost\n    def obj_xgb(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.3),\n             'max_depth': t.suggest_int('max_depth', 3, 10), 'subsample': t.suggest_float('subsample', 0.6, 1.0), 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=xgb.XGBClassifier(**p, use_label_encoder=False, eval_metric='logloss'))\n    objectives[\"XGB\"] = obj_xgb\n\n    # CatBoost\n    def obj_catboost(t, X, y):\n        p = {'iterations': t.suggest_int('iterations', 100, 800), 'depth': t.suggest_int('depth', 4, 10),\n             'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'random_seed': RANDOM_STATE, 'verbose': 0}\n        return obj_fn(t, X, y, classifier_obj=CatBoostClassifier(**p))\n    objectives[\"CatBoost\"] = obj_catboost\n\n    # SVC\n    def obj_svc(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'gamma': t.suggest_float('gamma', 1e-4, 1e-1, log=True),\n             'kernel': 'rbf', 'probability': True, 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=SVC(**p))\n    objectives[\"SVC\"] = obj_svc\n\n    # KNeighborsClassifier\n    def obj_knn(t, X, y):\n        p = {'n_neighbors': t.suggest_int('n_neighbors', 3, 30), 'weights': t.suggest_categorical('weights', ['uniform', 'distance'])}\n        return obj_fn(t, X, y, classifier_obj=KNeighborsClassifier(**p))\n    objectives[\"KNN\"] = obj_knn\n\n    # GaussianNB\n    def obj_gnb(t, X, y):\n        p = {'var_smoothing': t.suggest_float('var_smoothing', 1e-10, 1e-8, log=True)}\n        return obj_fn(t, X, y, classifier_obj=GaussianNB(**p))\n    objectives[\"GaussianNB\"] = obj_gnb\n    \n    # DecisionTreeClassifier\n    def obj_dt(t, X, y):\n        p = {'max_depth': t.suggest_int('max_depth', 3, 20), 'min_samples_split': t.suggest_int('min_samples_split', 2, 20), 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=DecisionTreeClassifier(**p))\n    objectives[\"DecisionTree\"] = obj_dt\n\n    # AdaBoostClassifier\n    def obj_adaboost(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 50, 500), 'learning_rate': t.suggest_float('learning_rate', 0.01, 1.0), 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=AdaBoostClassifier(**p))\n    objectives[\"AdaBoost\"] = obj_adaboost\n\n    # GradientBoostingClassifier\n    def obj_gb(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.2),\n             'max_depth': t.suggest_int('max_depth', 3, 8), 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=GradientBoostingClassifier(**p))\n    objectives[\"GradientBoosting\"] = obj_gb\n\n    # ExtraTreesClassifier\n    def obj_et(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'max_depth': t.suggest_int('max_depth', 4, 15),\n             'min_samples_split': t.suggest_int('min_samples_split', 2, 20), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n        return obj_fn(t, X, y, classifier_obj=ExtraTreesClassifier(**p))\n    objectives[\"ExtraTrees\"] = obj_et\n\n    # LinearDiscriminantAnalysis\n    def obj_lda(t, X, y):\n        p = {'solver': 'lsqr', 'shrinkage': t.suggest_float('shrinkage', 0.0, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=LinearDiscriminantAnalysis(**p))\n    objectives[\"LDA\"] = obj_lda\n\n    # QuadraticDiscriminantAnalysis\n    def obj_qda(t, X, y):\n        p = {'reg_param': t.suggest_float('reg_param', 0.0, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=QuadraticDiscriminantAnalysis(**p))\n    objectives[\"QDA\"] = obj_qda\n\n    # MLPClassifier\n    def obj_mlp(t, X, y):\n        p = {'hidden_layer_sizes': (t.suggest_int('hls_1', 30, 100), t.suggest_int('hls_2', 10, 50)),\n             'alpha': t.suggest_float('alpha', 1e-5, 1e-1, log=True), 'learning_rate_init': t.suggest_float('lr_init', 1e-4, 1e-2, log=True),\n             'max_iter': 300, 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=MLPClassifier(**p))\n    objectives[\"MLP\"] = obj_mlp\n    \n    # SGDClassifier\n    def obj_sgd(t, X, y):\n        p = {'loss': 'log_loss', 'penalty': 'elasticnet', 'alpha': t.suggest_float('alpha', 1e-5, 1e-1, log=True),\n             'l1_ratio': t.suggest_float('l1_ratio', 0, 1), 'max_iter': 1000, 'tol': 1e-3, 'random_state': RANDOM_STATE}\n        return obj_fn(t, X, y, classifier_obj=SGDClassifier(**p))\n    objectives[\"SGD\"] = obj_sgd\n    \n    return objectives\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du grand tournoi des mod√®les...\")\n    models_to_test = get_model_objectives(preprocessor)\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        # Use partial to pass X and y to the objective function\n        study.optimize(partial(objective_func, X=X, y=y), n_trials=n_trials_per_model, show_progress_bar=True)\n        if study.best_trial:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        else:\n            print(f\"‚ö†Ô∏è {name} n'a pas pu produire d'essai valide.\")\n\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\"); print(f\"   Meilleur F2-score : {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    \n    return champion_name, best_trials[champion_name].params\n\nprint(\"‚úÖ Structure d'optimisation pour 16 mod√®les pr√™te.\")\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    model_constructors = {\n        \"LGBM\": lgb.LGBMClassifier, \"RandomForest\": RandomForestClassifier, \"LogisticRegression\": LogisticRegression,\n        \"XGB\": xgb.XGBClassifier, \"CatBoost\": CatBoostClassifier, \"SVC\": SVC, \"KNN\": KNeighborsClassifier,\n        \"GaussianNB\": GaussianNB, \"DecisionTree\": DecisionTreeClassifier, \"AdaBoost\": AdaBoostClassifier,\n        \"GradientBoosting\": GradientBoostingClassifier, \"ExtraTrees\": ExtraTreesClassifier,\n        \"LDA\": LinearDiscriminantAnalysis, \"QDA\": QuadraticDiscriminantAnalysis, \"MLP\": MLPClassifier, \"SGD\": SGDClassifier\n    }\n    \n    default_params = {\n        'random_state': RANDOM_STATE, 'n_jobs': -1, 'use_label_encoder': False, 'eval_metric': 'logloss',\n        'verbose': 0, 'random_seed': RANDOM_STATE, 'probability': True, 'loss': 'log_loss', 'max_iter': 300\n    }\n    \n    constructor = model_constructors.get(champion_name)\n    if not constructor: raise ValueError(f\"Constructeur inconnu: {champion_name}\")\n\n    # Filtrer les param√®tres pour ne garder que ceux pertinents pour le constructeur\n    valid_params = {k: v for k, v in best_params.items() if k in constructor().get_params()}\n    # Ajouter les param√®tres par d√©faut\n    for k, v in default_params.items():\n        if k in constructor().get_params() and k not in valid_params:\n            valid_params[k] = v\n            \n    classifier_obj = constructor(**valid_params)\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', AdvancedFeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    \n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params, \n                \"training_date\": pd.Timestamp.now().isoformat()}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        # Cr√©ation d'une instance du FE pour d√©terminer les colonnes finales\n        fe = AdvancedFeatureEngineer()\n        X_engineered = fe.fit_transform(X) # fit_transform sur toutes les donn√©es pour obtenir toutes les colonnes possibles\n        \n        numeric_cols = list(X_engineered.select_dtypes(include=np.number).columns)\n        categorical_cols = list(X_engineered.select_dtypes(include=['object', 'category']).columns)\n        if 'patient_id' in numeric_cols: numeric_cols.remove('patient_id')\n        if 'patient_id' in categorical_cols: categorical_cols.remove('patient_id')\n        \n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√© pour {len(numeric_cols)} features num√©riques et {len(categorical_cols)} cat√©gorielles.\")\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        analyze_decision_threshold(y_test, y_probs)\n        # cost_analysis et plot_calibration sont appelables ici si besoin\n\n        report = {\"champion\": champion_name, \"final_test_f2_score\": fbeta_score(y_test, y_pred, beta=2), **best_params}\n        with open(os.path.join(model_dir, 'final_report.json'), 'w') as f: json.dump(report, f, indent=4, default=str)\n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {os.path.join(model_dir, 'final_report.json')}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n        \nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\"); print(\"=\"*80)\n        \n        # G√©n√©ration de donn√©es temporelles plus r√©alistes\n        n_patients = 100\n        records_per_patient = 24 * 3 # 3 jours de donn√©es horaires\n        total_records = n_patients * records_per_patient\n\n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='H'))\n        \n        data = {\n            'patient_id': patient_ids, 'timestamp': timestamps,\n            'age': np.repeat(np.random.randint(65, 95, n_patients), records_per_patient),\n            'gender': np.repeat(np.random.choice(['M', 'F'], n_patients, p=[0.4, 0.6]), records_per_patient),\n            'bmi': np.repeat(np.random.normal(26, 4, n_patients), records_per_patient)\n        }\n        X = pd.DataFrame(data)\n        \n        # Ajouter des signes vitaux avec variations intra-patient\n        noise = np.random.normal(0, 0.5, total_records)\n        X['heart_rate'] = np.repeat(np.random.normal(75, 8, n_patients), records_per_patient) + np.sin(X['timestamp'].dt.hour * 2 * np.pi / 24) * 5 + noise\n        X['systolic_bp'] = np.repeat(np.random.normal(130, 15, n_patients), records_per_patient) + np.random.normal(0, 3, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.repeat(np.random.normal(50, 5, n_patients), records_per_patient) + np.random.normal(0, 2, total_records)\n        X['grip_strength'] = np.repeat(np.random.normal(25, 6, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        X['mobility_score'] = np.repeat(np.random.uniform(2, 9, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        X['comorbidities_count'] = np.repeat(np.random.poisson(2, n_patients), records_per_patient)\n        X['medication_count'] = np.repeat(np.random.poisson(4, n_patients), records_per_patient)\n        \n        # Cr√©er une cible corr√©l√©e aux variations temporelles et aux caract√©ristiques de base\n        patient_frailty_level = {pid: np.random.rand() * 0.5 for pid in X['patient_id'].unique()}\n        X['frailty_base'] = X['patient_id'].map(patient_frailty_level)\n        score_raw = (X['frailty_base'] + X['age']/100 - X['grip_strength']/50 - X['mobility_score']/50 + np.random.randn(len(X))*0.05)\n        prob = 1 / (1 + np.exp(-(score_raw - score_raw.mean())*5))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        X = X.drop('frailty_base', axis=1) # Ne pas donner la r√©ponse au mod√®le\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Lancer le pipeline avec 5 essais par mod√®le pour un test rapide\n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=5, version=\"5.0-tournament-16\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE DU GRAND TOURNOI EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T13:10:32.860778Z","iopub.execute_input":"2025-06-23T13:10:32.861437Z","iopub.status.idle":"2025-06-23T13:11:13.396276Z","shell.execute_reply.started":"2025-06-23T13:10:32.861402Z","shell.execute_reply":"2025-06-23T13:11:13.395520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V6 - TOURNOI ROBUSTE DE 16 MOD√àLES & FEATURE ENGINEERING\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V6 : Tournoi r√©silient et code nettoy√©.\")\nprint(\"üë∂ [Enfant] Notre tournoi est maintenant plus intelligent : m√™me si personne n'est tr√®s fort, il trouvera le moins faible !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Outils de pipeline et de pr√©traitement\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Outils d'√©valuation\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√© avec 16 mod√®les pr√™ts pour le tournoi.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation et des seuils m√©tier.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score, 'precision': precision_score,\n            'specificity': self._specificity, 'balanced_accuracy': balanced_accuracy_score\n        }\n        self.alert_thresholds = {'recall': 0.60}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n\n    def _specificity(self, y_true, y_pred):\n        tn, fp, _, _ = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\neval_config = EvaluationConfig()\n\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES AVANC√âE (NETTOY√âE)\")\nprint(\"=\"*80)\n\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des caract√©ristiques cliniques et temporelles avanc√©es.\"\"\"\n    def __init__(self, rolling_window='6h'): # CORRECTION : 'H' -> 'h'\n        self.rolling_window = rolling_window\n        self.gender_means = {}\n\n    def fit(self, X, y=None):\n        if 'gender' in X.columns and 'grip_strength' in X.columns:\n            self.gender_means = X.groupby('gender')['grip_strength'].mean().to_dict()\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Business-driven features\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X_copy.columns:\n            X_copy['bmi_age_interaction'] = X_copy['bmi'] * X_copy['age']\n        \n        if self.gender_means and 'gender' in X.columns and 'grip_strength' in X_copy.columns:\n            mean_grip = X_copy['gender'].map(self.gender_means).fillna(X_copy['grip_strength'].mean())\n            X_copy['grip_strength_normalized'] = X_copy['grip_strength'] / mean_grip\n        \n        # Temporal features\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            rolling_cols = ['heart_rate', 'pulse_pressure', 'grip_strength']\n            grouped = X_copy.groupby('patient_id')\n            \n            for col in rolling_cols:\n                rolling_stats = grouped.rolling(self.rolling_window, on='timestamp', min_periods=1)[col]\n                X_copy[f'{col}_rolling_mean'] = rolling_stats.mean().reset_index(level=0, drop=True)\n                X_copy[f'{col}_rolling_std'] = rolling_stats.std().reset_index(level=0, drop=True)\n        \n        # CORRECTION : Utilisation de la nouvelle syntaxe pour fillna\n        X_copy.bfill(inplace=True)\n        X_copy.ffill(inplace=True)\n        \n        return X_copy\n\nprint(\"‚úÖ Classe AdvancedFeatureEngineer (nettoy√©e) pr√™te.\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), 'recall': make_scorer(recall_score, zero_division=0)}\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({'threshold': thresh,\n                        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n                        'recall': recall_score(y_true, y_pred, zero_division=0),\n                        'precision': precision_score(y_true, y_pred, zero_division=0),\n                        'specificity': eval_config.metrics['specificity'](y_true, y_pred)})\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(10, 6)); metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'], ax=plt.gca())\n            plt.title(\"Analyse des M√©triques par Seuil\"); plt.xlabel(\"Seuil\"); plt.ylabel(\"Score\"); plt.grid(True); plt.legend(); plt.show()\n        except Exception as e: print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef plot_calibration(y_true, y_probs):\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6)); plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e: print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI DE 16)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, classifier_obj):\n    pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_f2 = cv_results['test_f2'].mean()\n    if cv_results['test_recall'].mean() < eval_config.alert_thresholds['recall']:\n        raise optuna.TrialPruned()\n    return mean_f2\n\n# --- D√©finition des objectifs pour chaque mod√®le ---\ndef get_model_objectives(preprocessor):\n    obj_fn = partial(_generic_objective, preprocessor=preprocessor)\n    \n    objectives = {}\n    \n    def obj_lgbm(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n             'num_leaves': t.suggest_int('num_leaves', 10, 40)}\n        return obj_fn(t, X, y, classifier_obj=lgb.LGBMClassifier(**p, random_state=RANDOM_STATE, verbosity=-1))\n    objectives[\"LGBM\"] = obj_lgbm\n\n    def obj_rf(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'max_depth': t.suggest_int('max_depth', 4, 15),\n             'min_samples_split': t.suggest_int('min_samples_split', 2, 20)}\n        return obj_fn(t, X, y, classifier_obj=RandomForestClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1))\n    objectives[\"RandomForest\"] = obj_rf\n\n    def obj_logreg(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-3, 100, log=True), 'solver': 'liblinear'}\n        return obj_fn(t, X, y, classifier_obj=LogisticRegression(**p, random_state=RANDOM_STATE))\n    objectives[\"LogisticRegression\"] = obj_logreg\n\n    def obj_xgb(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.3),\n             'max_depth': t.suggest_int('max_depth', 3, 10), 'subsample': t.suggest_float('subsample', 0.6, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=xgb.XGBClassifier(**p, random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'))\n    objectives[\"XGB\"] = obj_xgb\n\n    def obj_catboost(t, X, y):\n        p = {'iterations': t.suggest_int('iterations', 100, 800), 'depth': t.suggest_int('depth', 4, 10),\n             'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True)}\n        return obj_fn(t, X, y, classifier_obj=CatBoostClassifier(**p, random_seed=RANDOM_STATE, verbose=0))\n    objectives[\"CatBoost\"] = obj_catboost\n\n    def obj_svc(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'gamma': t.suggest_float('gamma', 1e-4, 1e-1, log=True), 'kernel': 'rbf'}\n        return obj_fn(t, X, y, classifier_obj=SVC(**p, probability=True, random_state=RANDOM_STATE))\n    objectives[\"SVC\"] = obj_svc\n\n    def obj_knn(t, X, y):\n        p = {'n_neighbors': t.suggest_int('n_neighbors', 3, 30), 'weights': t.suggest_categorical('weights', ['uniform', 'distance'])}\n        return obj_fn(t, X, y, classifier_obj=KNeighborsClassifier(**p))\n    objectives[\"KNN\"] = obj_knn\n\n    def obj_gnb(t, X, y):\n        p = {'var_smoothing': t.suggest_float('var_smoothing', 1e-10, 1e-8, log=True)}\n        return obj_fn(t, X, y, classifier_obj=GaussianNB(**p))\n    objectives[\"GaussianNB\"] = obj_gnb\n    \n    def obj_dt(t, X, y):\n        p = {'max_depth': t.suggest_int('max_depth', 3, 20), 'min_samples_split': t.suggest_int('min_samples_split', 2, 20)}\n        return obj_fn(t, X, y, classifier_obj=DecisionTreeClassifier(**p, random_state=RANDOM_STATE))\n    objectives[\"DecisionTree\"] = obj_dt\n\n    def obj_adaboost(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 50, 500), 'learning_rate': t.suggest_float('learning_rate', 0.01, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=AdaBoostClassifier(**p, random_state=RANDOM_STATE))\n    objectives[\"AdaBoost\"] = obj_adaboost\n\n    def obj_gb(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.2),\n             'max_depth': t.suggest_int('max_depth', 3, 8)}\n        return obj_fn(t, X, y, classifier_obj=GradientBoostingClassifier(**p, random_state=RANDOM_STATE))\n    objectives[\"GradientBoosting\"] = obj_gb\n\n    def obj_et(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'max_depth': t.suggest_int('max_depth', 4, 15),\n             'min_samples_split': t.suggest_int('min_samples_split', 2, 20)}\n        return obj_fn(t, X, y, classifier_obj=ExtraTreesClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1))\n    objectives[\"ExtraTrees\"] = obj_et\n\n    def obj_lda(t, X, y):\n        p = {'solver': 'lsqr', 'shrinkage': t.suggest_float('shrinkage', 0.0, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=LinearDiscriminantAnalysis(**p))\n    objectives[\"LDA\"] = obj_lda\n\n    def obj_qda(t, X, y):\n        p = {'reg_param': t.suggest_float('reg_param', 0.0, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=QuadraticDiscriminantAnalysis(**p))\n    objectives[\"QDA\"] = obj_qda\n\n    def obj_mlp(t, X, y):\n        p = {'hidden_layer_sizes': (t.suggest_int('hls_1', 30, 100), t.suggest_int('hls_2', 10, 50)),\n             'alpha': t.suggest_float('alpha', 1e-5, 1e-1, log=True), 'learning_rate_init': t.suggest_float('lr_init', 1e-4, 1e-2, log=True)}\n        return obj_fn(t, X, y, classifier_obj=MLPClassifier(**p, max_iter=300, random_state=RANDOM_STATE))\n    objectives[\"MLP\"] = obj_mlp\n    \n    def obj_sgd(t, X, y):\n        p = {'penalty': 'elasticnet', 'alpha': t.suggest_float('alpha', 1e-5, 1e-1, log=True),\n             'l1_ratio': t.suggest_float('l1_ratio', 0, 1)}\n        return obj_fn(t, X, y, classifier_obj=SGDClassifier(**p, loss='log_loss', random_state=RANDOM_STATE))\n    objectives[\"SGD\"] = obj_sgd\n    \n    return objectives\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du grand tournoi des mod√®les...\")\n    models_to_test = get_model_objectives(preprocessor)\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        study.optimize(partial(objective_func, X=X, y=y), n_trials=n_trials_per_model, show_progress_bar=True)\n        \n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s. Ce mod√®le est disqualifi√©.\")\n\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\"); print(f\"   Meilleur F2-score : {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    \n    return champion_name, best_trials[champion_name].params\n\nprint(\"‚úÖ Structure d'optimisation pour 16 mod√®les pr√™te.\")\n\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    model_constructors = {\n        \"LGBM\": lgb.LGBMClassifier, \"RandomForest\": RandomForestClassifier, \"LogisticRegression\": LogisticRegression,\n        \"XGB\": xgb.XGBClassifier, \"CatBoost\": CatBoostClassifier, \"SVC\": SVC, \"KNN\": KNeighborsClassifier,\n        \"GaussianNB\": GaussianNB, \"DecisionTree\": DecisionTreeClassifier, \"AdaBoost\": AdaBoostClassifier,\n        \"GradientBoosting\": GradientBoostingClassifier, \"ExtraTrees\": ExtraTreesClassifier,\n        \"LDA\": LinearDiscriminantAnalysis, \"QDA\": QuadraticDiscriminantAnalysis, \"MLP\": MLPClassifier, \"SGD\": SGDClassifier\n    }\n    \n    constructor = model_constructors.get(champion_name)\n    if not constructor: raise ValueError(f\"Constructeur inconnu: {champion_name}\")\n            \n    classifier_obj = constructor(**best_params, random_state=RANDOM_STATE)\n    \n    # G√©rer les param√®tres sp√©cifiques non-standard\n    if champion_name in [\"RandomForest\", \"ExtraTrees\"]: classifier_obj.set_params(n_jobs=-1)\n    if champion_name == \"LGBM\": classifier_obj.set_params(verbosity=-1)\n    if champion_name == \"CatBoost\": classifier_obj.set_params(verbose=0)\n    if champion_name == \"XGB\": classifier_obj.set_params(use_label_encoder=False, eval_metric='logloss')\n    if champion_name == \"SVC\": classifier_obj.set_params(probability=True)\n    if champion_name == \"SGD\": classifier_obj.set_params(loss='log_loss')\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', AdvancedFeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    \n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params, \n                \"training_date\": pd.Timestamp.now().isoformat()}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = AdvancedFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = list(X_engineered.select_dtypes(include=np.number).columns.drop(['patient_id'], errors='ignore'))\n        categorical_cols = list(X_engineered.select_dtypes(include=['object', 'category']).columns.drop(['patient_id'], errors='ignore'))\n        \n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√© pour {len(numeric_cols)} features num√©riques et {len(categorical_cols)} cat√©gorielles.\")\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        analyze_decision_threshold(y_test, y_probs)\n        plot_calibration(y_test, y_probs)\n\n        report = {\"champion\": champion_name, \"final_test_f2_score\": fbeta_score(y_test, y_pred, beta=2), **best_params}\n        with open(os.path.join(model_dir, 'final_report.json'), 'w') as f: json.dump(report, f, indent=4, default=str)\n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {os.path.join(model_dir, 'final_report.json')}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n        \nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\"); print(\"=\"*80)\n        \n        n_patients = 100\n        records_per_patient = 24 * 3\n        total_records = n_patients * records_per_patient\n\n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='h'))\n        \n        data = {'patient_id': patient_ids, 'timestamp': timestamps,\n                'age': np.repeat(np.random.randint(65, 95, n_patients), records_per_patient),\n                'gender': np.repeat(np.random.choice(['M', 'F'], n_patients, p=[0.4, 0.6]), records_per_patient),\n                'bmi': np.repeat(np.random.normal(26, 4, n_patients), records_per_patient),\n                'comorbidities_count': np.repeat(np.random.poisson(2, n_patients), records_per_patient),\n                'medication_count': np.repeat(np.random.poisson(4, n_patients), records_per_patient)}\n        X = pd.DataFrame(data)\n        \n        noise = np.random.normal(0, 0.5, total_records)\n        X['heart_rate'] = np.repeat(np.random.normal(75, 8, n_patients), records_per_patient) + np.sin(X['timestamp'].dt.hour * 2 * np.pi / 24) * 5 + noise\n        X['systolic_bp'] = np.repeat(np.random.normal(130, 15, n_patients), records_per_patient) + np.random.normal(0, 3, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.repeat(np.random.normal(50, 5, n_patients), records_per_patient)\n        X['grip_strength'] = np.repeat(np.random.normal(25, 6, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        X['mobility_score'] = np.repeat(np.random.uniform(2, 9, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        \n        patient_frailty_level = {pid: np.random.rand() * 0.5 for pid in X['patient_id'].unique()}\n        X['frailty_base'] = X['patient_id'].map(patient_frailty_level)\n        score_raw = (X['frailty_base'] + X['age']/100 - X['grip_strength']/50 - X['mobility_score']/50 + np.random.randn(len(X))*0.05)\n        prob = 1 / (1 + np.exp(-(score_raw - score_raw.mean())*5))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        X = X.drop('frailty_base', axis=1)\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Lancer le pipeline avec un petit nombre d'essais pour un test rapide\n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=5, version=\"6.0-full-tournament\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE DU GRAND TOURNOI EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T13:17:26.520042Z","iopub.execute_input":"2025-06-23T13:17:26.520772Z","iopub.status.idle":"2025-06-23T13:25:00.607399Z","shell.execute_reply.started":"2025-06-23T13:17:26.520745Z","shell.execute_reply":"2025-06-23T13:25:00.606530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V7 - TOURNOI ANTI-TRICHE & FEATURE ENGINEERING\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V7 : Ajout d'une contrainte AUC pour un tournoi plus juste.\")\nprint(\"üë∂ [Enfant] On a ajout√© un arbitre pour v√©rifier que les machines ne trichent pas en donnant toujours la m√™me r√©ponse !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Outils de pipeline et de pr√©traitement\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Outils d'√©valuation\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√© avec 16 mod√®les pr√™ts pour le tournoi.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    \"\"\"Configuration centralis√©e des m√©triques d'√©valuation et des seuils m√©tier.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'roc_auc': roc_auc_score,\n            'f2_score': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n            'recall': recall_score, 'precision': precision_score,\n            'specificity': self._specificity, 'balanced_accuracy': balanced_accuracy_score\n        }\n        self.alert_thresholds = {'recall': 0.60, 'min_auc': 0.55}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n\n    def _specificity(self, y_true, y_pred):\n        tn, fp, _, _ = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n        return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\neval_config = EvaluationConfig()\n\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES AVANC√âE (NETTOY√âE)\")\nprint(\"=\"*80)\n\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des caract√©ristiques cliniques et temporelles avanc√©es.\"\"\"\n    def __init__(self, rolling_window='6h'):\n        self.rolling_window = rolling_window\n        self.gender_means = {}\n\n    def fit(self, X, y=None):\n        if 'gender' in X.columns and 'grip_strength' in X.columns:\n            self.gender_means = X.groupby('gender')['grip_strength'].mean().to_dict()\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Business-driven features\n        X_copy['pulse_pressure'] = X_copy['systolic_bp'] - X_copy['diastolic_bp']\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X_copy.columns:\n            X_copy['bmi_age_interaction'] = X_copy['bmi'] * X_copy['age']\n        \n        if self.gender_means and 'gender' in X.columns and 'grip_strength' in X_copy.columns:\n            mean_grip = X_copy['gender'].map(self.gender_means).fillna(X_copy['grip_strength'].mean())\n            X_copy['grip_strength_normalized'] = X_copy['grip_strength'] / mean_grip\n        \n        # Temporal features\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            rolling_cols = ['heart_rate', 'pulse_pressure', 'grip_strength']\n            grouped = X_copy.groupby('patient_id')\n            \n            for col in rolling_cols:\n                rolling_stats = grouped.rolling(self.rolling_window, on='timestamp', min_periods=1)[col]\n                X_copy[f'{col}_rolling_mean'] = rolling_stats.mean().reset_index(level=0, drop=True)\n                X_copy[f'{col}_rolling_std'] = rolling_stats.std().reset_index(level=0, drop=True)\n        \n        X_copy.bfill(inplace=True)\n        X_copy.ffill(inplace=True)\n        \n        return X_copy\n\nprint(\"‚úÖ Classe AdvancedFeatureEngineer (nettoy√©e) pr√™te.\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\")\nprint(\"=\"*80)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"MODIFI√â : √âvalue et retourne toujours le rappel ET l'AUC ROC.\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {\n        'f2': make_scorer(fbeta_score, beta=2, zero_division=0),\n        'recall': make_scorer(recall_score, zero_division=0),\n        'roc_auc': 'roc_auc'\n    }\n    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise')\n    return pd.DataFrame(cv_results)\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    thresholds = np.linspace(0.1, 0.9, 20)\n    metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({'threshold': thresh,\n                        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n                        'recall': recall_score(y_true, y_pred, zero_division=0),\n                        'precision': precision_score(y_true, y_pred, zero_division=0),\n                        'specificity': eval_config.metrics['specificity'](y_true, y_pred)})\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(10, 6)); metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'], ax=plt.gca())\n            plt.title(\"Analyse des M√©triques par Seuil\"); plt.xlabel(\"Seuil\"); plt.ylabel(\"Score\"); plt.grid(True); plt.legend(); plt.show()\n        except Exception as e: print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef plot_calibration(y_true, y_probs):\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6)); plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e: print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\nprint(\"‚úÖ Fonctions d'√©valuation et d'analyse pr√™tes.\")\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES (AVEC DOUBLE CONTRAINTE)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI ROBUSTE)\")\nprint(\"=\"*80)\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, classifier_obj):\n    pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    \n    mean_recall = cv_results['test_recall'].mean()\n    mean_auc = cv_results['test_roc_auc'].mean()\n    mean_f2 = cv_results['test_f2'].mean()\n    \n    # CONTRAINTE 1 : Le mod√®le doit avoir une capacit√© de discrimination minimale\n    if mean_auc < eval_config.alert_thresholds['min_auc']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - AUC ({mean_auc:.2f}) < {eval_config.alert_thresholds['min_auc']}. Le mod√®le ne discrimine pas.\")\n        raise optuna.TrialPruned()\n        \n    # CONTRAINTE 2 : Le mod√®le doit respecter le seuil m√©tier de rappel\n    if mean_recall < eval_config.alert_thresholds['recall']:\n        print(f\"‚ö†Ô∏è Essai {trial.number} √©lagu√© - recall ({mean_recall:.2f}) < seuil ({eval_config.alert_thresholds['recall']})\")\n        raise optuna.TrialPruned()\n        \n    return mean_f2\n\ndef get_model_objectives(preprocessor):\n    obj_fn = partial(_generic_objective, preprocessor=preprocessor)\n    objectives = {}\n    def obj_lgbm(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n             'num_leaves': t.suggest_int('num_leaves', 10, 40)}\n        return obj_fn(t, X, y, classifier_obj=lgb.LGBMClassifier(**p, random_state=RANDOM_STATE, verbosity=-1))\n    objectives[\"LGBM\"] = obj_lgbm\n    def obj_rf(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'max_depth': t.suggest_int('max_depth', 4, 15),\n             'min_samples_split': t.suggest_int('min_samples_split', 2, 20)}\n        return obj_fn(t, X, y, classifier_obj=RandomForestClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1))\n    objectives[\"RandomForest\"] = obj_rf\n    def obj_logreg(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-3, 100, log=True), 'solver': 'liblinear'}\n        return obj_fn(t, X, y, classifier_obj=LogisticRegression(**p, random_state=RANDOM_STATE))\n    objectives[\"LogisticRegression\"] = obj_logreg\n    def obj_xgb(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.3),\n             'max_depth': t.suggest_int('max_depth', 3, 10), 'subsample': t.suggest_float('subsample', 0.6, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=xgb.XGBClassifier(**p, random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'))\n    objectives[\"XGB\"] = obj_xgb\n    def obj_catboost(t, X, y):\n        p = {'iterations': t.suggest_int('iterations', 100, 800), 'depth': t.suggest_int('depth', 4, 10),\n             'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True)}\n        return obj_fn(t, X, y, classifier_obj=CatBoostClassifier(**p, random_seed=RANDOM_STATE, verbose=0))\n    objectives[\"CatBoost\"] = obj_catboost\n    def obj_svc(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'gamma': t.suggest_float('gamma', 1e-4, 1e-1, log=True), 'kernel': 'rbf'}\n        return obj_fn(t, X, y, classifier_obj=SVC(**p, probability=True, random_state=RANDOM_STATE))\n    objectives[\"SVC\"] = obj_svc\n    def obj_knn(t, X, y):\n        p = {'n_neighbors': t.suggest_int('n_neighbors', 3, 30), 'weights': t.suggest_categorical('weights', ['uniform', 'distance'])}\n        return obj_fn(t, X, y, classifier_obj=KNeighborsClassifier(**p, n_jobs=-1))\n    objectives[\"KNN\"] = obj_knn\n    def obj_gnb(t, X, y):\n        p = {'var_smoothing': t.suggest_float('var_smoothing', 1e-10, 1e-8, log=True)}\n        return obj_fn(t, X, y, classifier_obj=GaussianNB(**p))\n    objectives[\"GaussianNB\"] = obj_gnb\n    def obj_dt(t, X, y):\n        p = {'max_depth': t.suggest_int('max_depth', 3, 20), 'min_samples_split': t.suggest_int('min_samples_split', 2, 20)}\n        return obj_fn(t, X, y, classifier_obj=DecisionTreeClassifier(**p, random_state=RANDOM_STATE))\n    objectives[\"DecisionTree\"] = obj_dt\n    def obj_adaboost(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 50, 500), 'learning_rate': t.suggest_float('learning_rate', 0.01, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=AdaBoostClassifier(**p, random_state=RANDOM_STATE))\n    objectives[\"AdaBoost\"] = obj_adaboost\n    def obj_gb(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.2),\n             'max_depth': t.suggest_int('max_depth', 3, 8)}\n        return obj_fn(t, X, y, classifier_obj=GradientBoostingClassifier(**p, random_state=RANDOM_STATE))\n    objectives[\"GradientBoosting\"] = obj_gb\n    def obj_et(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'max_depth': t.suggest_int('max_depth', 4, 15),\n             'min_samples_split': t.suggest_int('min_samples_split', 2, 20)}\n        return obj_fn(t, X, y, classifier_obj=ExtraTreesClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1))\n    objectives[\"ExtraTrees\"] = obj_et\n    def obj_lda(t, X, y):\n        p = {'solver': 'lsqr', 'shrinkage': t.suggest_float('shrinkage', 0.0, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=LinearDiscriminantAnalysis(**p))\n    objectives[\"LDA\"] = obj_lda\n    def obj_qda(t, X, y):\n        p = {'reg_param': t.suggest_float('reg_param', 0.0, 1.0)}\n        return obj_fn(t, X, y, classifier_obj=QuadraticDiscriminantAnalysis(**p))\n    objectives[\"QDA\"] = obj_qda\n    def obj_mlp(t, X, y):\n        p = {'hidden_layer_sizes': (t.suggest_int('hls_1', 30, 100), t.suggest_int('hls_2', 10, 50)),\n             'alpha': t.suggest_float('alpha', 1e-5, 1e-1, log=True), 'learning_rate_init': t.suggest_float('lr_init', 1e-4, 1e-2, log=True)}\n        return obj_fn(t, X, y, classifier_obj=MLPClassifier(**p, max_iter=300, random_state=RANDOM_STATE))\n    objectives[\"MLP\"] = obj_mlp\n    def obj_sgd(t, X, y):\n        p = {'penalty': 'elasticnet', 'alpha': t.suggest_float('alpha', 1e-5, 1e-1, log=True),\n             'l1_ratio': t.suggest_float('l1_ratio', 0, 1)}\n        return obj_fn(t, X, y, classifier_obj=SGDClassifier(**p, loss='log_loss', random_state=RANDOM_STATE))\n    objectives[\"SGD\"] = obj_sgd\n    return objectives\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du grand tournoi des mod√®les...\")\n    models_to_test = get_model_objectives(preprocessor)\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        study.optimize(partial(objective_func, X=X, y=y), n_trials=n_trials_per_model, show_progress_bar=True)\n        \n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s. Ce mod√®le est disqualifi√©.\")\n\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\"); print(f\"   Meilleur F2-score : {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    \n    return champion_name, best_trials[champion_name].params\n\nprint(\"‚úÖ Structure d'optimisation pour 16 mod√®les pr√™te.\")\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. üè≠ ENTRA√éNEMENT ET VERSIONNING DU MOD√àLE CHAMPION\")\nprint(\"=\"*80)\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    model_constructors = {\n        \"LGBM\": lgb.LGBMClassifier, \"RandomForest\": RandomForestClassifier, \"LogisticRegression\": LogisticRegression,\n        \"XGB\": xgb.XGBClassifier, \"CatBoost\": CatBoostClassifier, \"SVC\": SVC, \"KNN\": KNeighborsClassifier,\n        \"GaussianNB\": GaussianNB, \"DecisionTree\": DecisionTreeClassifier, \"AdaBoost\": AdaBoostClassifier,\n        \"GradientBoosting\": GradientBoostingClassifier, \"ExtraTrees\": ExtraTreesClassifier,\n        \"LDA\": LinearDiscriminantAnalysis, \"QDA\": QuadraticDiscriminantAnalysis, \"MLP\": MLPClassifier, \"SGD\": SGDClassifier\n    }\n    \n    constructor = model_constructors.get(champion_name)\n    if not constructor: raise ValueError(f\"Constructeur inconnu: {champion_name}\")\n            \n    # Instanciation avec les param√®tres optimis√©s\n    classifier_obj = constructor(**best_params, random_state=RANDOM_STATE)\n    \n    # Gestion des param√®tres sp√©cifiques non inclus dans l'optimisation\n    if champion_name in [\"RandomForest\", \"ExtraTrees\", \"KNN\"]: classifier_obj.set_params(n_jobs=-1)\n    if champion_name == \"LGBM\": classifier_obj.set_params(verbosity=-1)\n    if champion_name == \"CatBoost\": classifier_obj.set_params(verbose=0)\n    if champion_name == \"XGB\": classifier_obj.set_params(use_label_encoder=False, eval_metric='logloss')\n    if champion_name == \"SVC\": classifier_obj.set_params(probability=True)\n    if champion_name == \"SGD\": classifier_obj.set_params(loss='log_loss')\n    if champion_name == \"MLP\": classifier_obj.set_params(max_iter=300)\n        \n    final_pipeline = Pipeline([\n        ('feature_engineering', AdvancedFeatureEngineer()),\n        ('preprocessor', preprocessor),\n        ('classifier', classifier_obj)\n    ])\n    \n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    \n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params, \n                \"training_date\": pd.Timestamp.now().isoformat()}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    \n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\nprint(\"‚úÖ Fonction d'entra√Ænement final pr√™te.\")\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. üèÅ PIPELINE COMPLET ORCHESTR√â\")\nprint(\"=\"*80)\n\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = AdvancedFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [col for col in X_engineered.select_dtypes(include=np.number).columns if col != 'patient_id']\n        categorical_cols = [col for col in X_engineered.select_dtypes(include=['object', 'category']).columns if col != 'patient_id']\n\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√© pour {len(numeric_cols)} features num√©riques et {len(categorical_cols)} cat√©gorielles.\")\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        \n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        threshold_metrics = analyze_decision_threshold(y_test, y_probs)\n        plot_calibration(y_test, y_probs)\n\n        report = {\"champion\": champion_name, \"final_test_f2_score\": fbeta_score(y_test, y_pred, beta=2, zero_division=0), **best_params}\n        with open(os.path.join(model_dir, 'final_report.json'), 'w') as f: json.dump(report, f, indent=4, default=str)\n        print(f\"\\n‚úÖ Pipeline termin√©! Rapport sauvegard√©: {os.path.join(model_dir, 'final_report.json')}\")\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n        \nprint(\"‚úÖ Pipeline principal pr√™t √† √™tre ex√©cut√©.\")\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE\"); print(\"=\"*80)\n        \n        n_patients = 100; records_per_patient = 24 * 3; total_records = n_patients * records_per_patient\n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='h'))\n        \n        data = {'patient_id': patient_ids, 'timestamp': timestamps,\n                'age': np.repeat(np.random.randint(65, 95, n_patients), records_per_patient),\n                'gender': np.repeat(np.random.choice(['M', 'F'], n_patients, p=[0.4, 0.6]), records_per_patient),\n                'bmi': np.repeat(np.random.normal(26, 4, n_patients), records_per_patient),\n                'comorbidities_count': np.repeat(np.random.poisson(2, n_patients), records_per_patient),\n                'medication_count': np.repeat(np.random.poisson(4, n_patients), records_per_patient)}\n        X = pd.DataFrame(data)\n        \n        noise = np.random.normal(0, 0.5, total_records)\n        X['heart_rate'] = np.repeat(np.random.normal(75, 8, n_patients), records_per_patient) + np.sin(X['timestamp'].dt.hour * 2 * np.pi / 24) * 5 + noise\n        X['systolic_bp'] = np.repeat(np.random.normal(130, 15, n_patients), records_per_patient) + np.random.normal(0, 3, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.repeat(np.random.normal(50, 5, n_patients), records_per_patient)\n        X['grip_strength'] = np.repeat(np.random.normal(25, 6, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        X['mobility_score'] = np.repeat(np.random.uniform(2, 9, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        \n        patient_frailty_level = {pid: np.random.rand() * 0.5 for pid in X['patient_id'].unique()}\n        X['frailty_base'] = X['patient_id'].map(patient_frailty_level)\n        score_raw = (X['frailty_base'] + X['age']/100 - X['grip_strength']/50 - X['mobility_score']/50 + np.random.randn(len(X))*0.05)\n        prob = 1 / (1 + np.exp(-(score_raw - score_raw.mean())*5))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n        X = X.drop('frailty_base', axis=1)\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Lancer le pipeline avec un petit nombre d'essais pour un test rapide\n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=5, version=\"7.0-anti-cheat-tournament\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE DU GRAND TOURNOI EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T13:53:00.535253Z","iopub.execute_input":"2025-06-23T13:53:00.535619Z","iopub.status.idle":"2025-06-23T14:00:49.351771Z","shell.execute_reply.started":"2025-06-23T13:53:00.535591Z","shell.execute_reply":"2025-06-23T14:00:49.350680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V9 - MODE DEBUG : SIMPLIFICATION RADICALE POUR RETROUVER LE SIGNAL\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V9 : Mode D√©bogage. Simplification radicale pour valider les hypoth√®ses de base.\")\nprint(\"üë∂ [Enfant] Notre machine n'apprend rien. On va lui donner un probl√®me beaucoup plus simple pour voir si elle comprend.\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION (INCHANG√â)\n# =============================================================================\n# ... (Imports identiques √† la version pr√©c√©dente) ...\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import roc_auc_score, fbeta_score, recall_score, precision_score, confusion_matrix, classification_report, make_scorer, balanced_accuracy_score\nfrom sklearn.calibration import calibration_curve\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n# ... (Configuration de style inchang√©e) ...\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES (SIMPLIFI√âE POUR DEBUG)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES (MODE DEBUG)\")\nprint(\"=\"*80)\nclass EvaluationConfigDebug:\n    def __init__(self):\n        # On ne met plus de contraintes pour voir ce que les mod√®les arrivent √† faire\n        self.alert_thresholds = {'recall': 0.0, 'min_auc': 0.0}\n        print(f\"Seuils d'alerte D√âSACTIV√âS pour le d√©bogage : {self.alert_thresholds}\")\neval_config = EvaluationConfigDebug()\n\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (SIMPLIFI√â POUR DEBUG)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (SIMPLIFI√âE POUR DEBUG)\")\nprint(\"=\"*80)\n\nclass BasicFeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Version simplifi√©e qui ne fait que le minimum vital.\"\"\"\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X_copy = X.copy()\n        # On ne cr√©e qu'une seule feature simple et fiable\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        return X_copy\n\nprint(\"‚úÖ Classe FeatureEngineer revenue √† sa version de base.\")\n\n# Le reste du code reste identique jusqu'√† l'ex√©cution principale\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    \"\"\"√âvalue et retourne toujours le rappel ET l'AUC ROC.\"\"\"\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {\n        'f2': make_scorer(fbeta_score, beta=2, zero_division=0),\n        'recall': make_scorer(recall_score, zero_division=0),\n        'roc_auc': 'roc_auc'\n    }\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\ndef _generic_objective(trial, X, y, preprocessor, classifier_obj):\n    # Utilisation du Feature Engineer de base\n    pipeline = Pipeline([('fe', BasicFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    \n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']:\n        raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) too low.\")\n        \n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']:\n        raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) too low.\")\n        \n    return cv_results['test_f2'].mean()\n    \n# ... (Le reste des fonctions est inchang√©, mais utilisera implicitement les nouvelles versions) ...\ndef get_model_objectives(preprocessor):\n    obj_fn = partial(_generic_objective, preprocessor=preprocessor)\n    objectives = {}\n    def obj_lgbm(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'learning_rate': t.suggest_float('learning_rate', 1e-2, 0.2, log=True)}\n        return obj_fn(t, X, y, classifier_obj=lgb.LGBMClassifier(**p, random_state=RANDOM_STATE, verbosity=-1))\n    objectives[\"LGBM\"] = obj_lgbm\n    def obj_rf(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'max_depth': t.suggest_int('max_depth', 5, 15)}\n        return obj_fn(t, X, y, classifier_obj=RandomForestClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1))\n    objectives[\"RandomForest\"] = obj_rf\n    def obj_logreg(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear'}\n        return obj_fn(t, X, y, classifier_obj=LogisticRegression(**p, random_state=RANDOM_STATE))\n    objectives[\"LogisticRegression\"] = obj_logreg\n    return objectives\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    models_to_test = get_model_objectives(preprocessor)\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        study.optimize(partial(objective_func, X=X, y=y), n_trials=n_trials_per_model, show_progress_bar=True)\n        \n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s. Ce mod√®le est disqualifi√©.\")\n\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\"); print(f\"   Meilleur F2-score : {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    \n    return champion_name, best_trials[champion_name].params\n\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    model_constructors = {\"LGBM\": lgb.LGBMClassifier, \"RandomForest\": RandomForestClassifier, \"LogisticRegression\": LogisticRegression}\n    constructor = model_constructors.get(champion_name)\n    classifier_obj = constructor(**best_params, random_state=RANDOM_STATE)\n    \n    final_pipeline = Pipeline([('fe', BasicFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    \n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    # ... metadata\n    return final_pipeline, MODEL_DIR\n\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        \n        fe = BasicFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [col for col in X_engineered.select_dtypes(include=np.number) if col != 'patient_id']\n        categorical_cols = [col for col in X_engineered.select_dtypes(include='object') if col != 'patient_id']\n\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        return model, {}\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT (MODE DEBUG)\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE (MODE DEBUG)\"); print(\"=\"*80)\n        \n        # --- G√âN√âRATION DE DONN√âES AVEC UN SIGNAL VOLONTAIREMENT SIMPLE ET FORT ---\n        print(\"\\nüî¨ Cr√©ation de donn√©es avec un signal simple et clair...\")\n        n_records = 2000\n        data = {'age': np.random.randint(65, 95, n_records),\n                'mobility_score': np.random.uniform(1, 10, n_records),\n                'grip_strength': np.random.normal(25, 8, n_records),\n                'systolic_bp': np.random.normal(130, 20, n_records),\n                'diastolic_bp': np.random.normal(80, 10, n_records),\n                'heart_rate': np.random.normal(75, 10, n_records),\n                'gender': np.random.choice(['M', 'F'], n_records, p=[0.4, 0.6])}\n        X = pd.DataFrame(data)\n\n        # La cible d√©pend TRES fortement de 3 variables, avec un peu de bruit\n        score = (X['age']/20) - (X['mobility_score']/2) - (X['grip_strength']/5) + np.random.randn(n_records)*0.5\n        prob = 1 / (1 + np.exp(-(score - score.mean())))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Lancer le pipeline avec un tournoi plus petit pour le debug\n        model, report = run_full_pipeline(X, y, n_trials_per_model=10, version=\"9.0-debug-signal\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE EN MODE DEBUG EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T16:39:41.120863Z","iopub.execute_input":"2025-06-23T16:39:41.121631Z","iopub.status.idle":"2025-06-23T16:40:36.885221Z","shell.execute_reply.started":"2025-06-23T16:39:41.121600Z","shell.execute_reply":"2025-06-23T16:40:36.884428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V9 - MODE DEBUG : SIMPLIFICATION RADICALE (CORRIG√â)\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V9 : Mode D√©bogage. Simplification radicale pour valider les hypoth√®ses de base.\")\nprint(\"üë∂ [Enfant] Notre machine n'apprend rien. On va lui donner un probl√®me beaucoup plus simple pour voir si elle comprend.\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    sns.set_palette(\"husl\")\nexcept:\n    print(\"‚ö†Ô∏è Style 'seaborn-v0_8-whitegrid' non trouv√©. Utilisation du style par d√©faut.\")\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES (MODE DEBUG)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES (MODE DEBUG)\")\nprint(\"=\"*80)\nclass EvaluationConfigDebug:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.0, 'min_auc': 0.0}\n        print(f\"Seuils d'alerte D√âSACTIV√âS pour le d√©bogage : {self.alert_thresholds}\")\neval_config = EvaluationConfigDebug()\n\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (SIMPLIFI√â POUR DEBUG)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (SIMPLIFI√âE POUR DEBUG)\")\nprint(\"=\"*80)\n\nclass BasicFeatureEngineer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        return X_copy\nprint(\"‚úÖ Classe FeatureEngineer revenue √† sa version de base.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0),\n               'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, classifier_obj):\n    pipeline = Pipeline([('fe', BasicFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) too low.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) too low.\")\n    return cv_results['test_f2'].mean()\n\ndef get_model_objectives(preprocessor):\n    obj_fn = partial(_generic_objective, preprocessor=preprocessor)\n    objectives = {}\n    def obj_lgbm(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'learning_rate': t.suggest_float('learning_rate', 1e-2, 0.2, log=True)}\n        return obj_fn(t, X, y, classifier_obj=lgb.LGBMClassifier(**p, random_state=RANDOM_STATE, verbosity=-1))\n    objectives[\"LGBM\"] = obj_lgbm\n    def obj_rf(t, X, y):\n        p = {'n_estimators': t.suggest_int('n_estimators', 100, 500), 'max_depth': t.suggest_int('max_depth', 5, 15)}\n        return obj_fn(t, X, y, classifier_obj=RandomForestClassifier(**p, random_state=RANDOM_STATE, n_jobs=-1))\n    objectives[\"RandomForest\"] = obj_rf\n    def obj_logreg(t, X, y):\n        p = {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear'}\n        return obj_fn(t, X, y, classifier_obj=LogisticRegression(**p, random_state=RANDOM_STATE))\n    objectives[\"LogisticRegression\"] = obj_logreg\n    return objectives\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    models_to_test = get_model_objectives(preprocessor)\n    best_trials = {}\n\n    for name, objective_func in models_to_test.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        study.optimize(partial(objective_func, X=X, y=y), n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s. Ce mod√®le est disqualifi√©.\")\n\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n\n    champion_name = max(best_trials, key=lambda name: best_trials[name].value)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION DU TOURNOI : {champion_name} üèÜ\"); print(f\"   Meilleur F2-score : {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    \n    return champion_name, best_trials[champion_name].params\n\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (version {version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    \n    model_constructors = {\"LGBM\": lgb.LGBMClassifier, \"RandomForest\": RandomForestClassifier, \"LogisticRegression\": LogisticRegression}\n    constructor = model_constructors.get(champion_name)\n    \n    classifier_obj = constructor(**best_params, random_state=RANDOM_STATE)\n    \n    final_pipeline = Pipeline([('fe', BasicFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    \n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    \n    return final_pipeline, MODEL_DIR\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        \n        fe = BasicFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [col for col in X_engineered.select_dtypes(include=np.number) if col != 'patient_id']\n        categorical_cols = [col for col in X_engineered.select_dtypes(include='object') if col != 'patient_id']\n\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        # CORRECTION DU BUG : Retourner un rapport non vide\n        report = {\n            \"champion_model\": champion_name,\n            \"best_params\": best_params,\n            \"final_test_f2_score\": fbeta_score(y_test, y_pred, beta=2, zero_division=0)\n        }\n        \n        report_path = os.path.join(model_dir, 'final_report.json')\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=4, default=str)\n        print(f\"\\n‚úÖ Rapport de d√©bogage sauvegard√©: {report_path}\")\n\n        return model, report\n\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT (MODE DEBUG)\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION PRINCIPALE (MODE DEBUG)\"); print(\"=\"*80)\n        \n        # --- G√âN√âRATION DE DONN√âES AVEC UN SIGNAL VOLONTAIREMENT SIMPLE ET FORT ---\n        print(\"\\nüî¨ Cr√©ation de donn√©es avec un signal simple et clair...\")\n        n_records = 2000\n        data = {'age': np.random.randint(65, 95, n_records),\n                'mobility_score': np.random.uniform(1, 10, n_records),\n                'grip_strength': np.random.normal(25, 8, n_records),\n                'systolic_bp': np.random.normal(130, 20, n_records),\n                'diastolic_bp': np.random.normal(80, 10, n_records),\n                'heart_rate': np.random.normal(75, 10, n_records),\n                'gender': np.random.choice(['M', 'F'], n_records, p=[0.4, 0.6])}\n        X = pd.DataFrame(data)\n\n        score = (X['age']/20) - (X['mobility_score']/2) - (X['grip_strength']/5) + np.random.randn(n_records)*0.5\n        prob = 1 / (1 + np.exp(-(score - score.mean())))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Lancer le pipeline avec un tournoi de 3 mod√®les pour le debug\n        model, report = run_full_pipeline(X, y, n_trials_per_model=10, version=\"9.1-debug-fix\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE EN MODE DEBUG EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:12:25.897696Z","iopub.execute_input":"2025-06-23T17:12:25.899725Z","iopub.status.idle":"2025-06-23T17:13:20.608028Z","shell.execute_reply.started":"2025-06-23T17:12:25.899681Z","shell.execute_reply":"2025-06-23T17:13:20.606075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V11.5 - CORRECTION FINALE DE LA LOGIQUE DE G√âN√âRATION DE DONN√âES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V11.5 : Correction de l'appel √† .rolling() dans la g√©n√©ration de donn√©es.\")\nprint(\"üë∂ [Enfant] On a compris ! Pour regarder l'heure, il faut garder la montre avec soi !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\nimport numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import joblib; import os; import json; import traceback; from functools import partial; import warnings\nimport optuna; from sklearn.ensemble import *; from sklearn.linear_model import *; from sklearn.svm import *; from sklearn.neighbors import *; from sklearn.naive_bayes import *; from sklearn.tree import *; from sklearn.discriminant_analysis import *; from sklearn.neural_network import *; import lightgbm as lgb; import xgboost as xgb; from catboost import CatBoostClassifier\nfrom sklearn.model_selection import *; from sklearn.pipeline import *; from sklearn.compose import *; from sklearn.preprocessing import *; from sklearn.impute import *; from sklearn.base import *; from sklearn.metrics import *; from sklearn.calibration import *\nRANDOM_STATE = 42; np.random.seed(RANDOM_STATE); warnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry: plt.style.use('seaborn-v0_8-whitegrid')\nexcept: plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.70, 'min_auc': 0.75}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES AVANC√âE\")\nprint(\"=\"*80)\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self, rolling_window='12h'): self.rolling_window = rolling_window\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        if 'comorbidities_count' in X.columns: X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            for col in ['heart_rate', 'pulse_pressure', 'mobility_score']:\n                if col in X_copy.columns:\n                    rolling_group = X_copy.groupby('patient_id').rolling(self.rolling_window, on='timestamp', min_periods=1)\n                    X_copy[f'{col}_roll_mean'] = rolling_group[col].mean().reset_index(level=0, drop=True)\n                    X_copy[f'{col}_roll_std'] = rolling_group[col].std().reset_index(level=0, drop=True)\n        return X_copy.bfill().ffill()\nprint(\"‚úÖ Classe AdvancedFeatureEngineer pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), 'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI ROBUSTE)\")\nprint(\"=\"*80)\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    return models\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (v{version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = AdvancedFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', QuantileTransformer(output_distribution='normal'))]), numeric_cols),\n            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√©.\")\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        return model, {\"champion\": champion_name, \"params\": best_params}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V11.5 (CORRIG√âE)\"); print(\"=\"*80)\n        \n        n_patients = 150; records_per_patient = 24 * 7; total_records = n_patients * records_per_patient\n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='h'))\n        \n        data = {'patient_id': patient_ids, 'timestamp': timestamps,\n                'age': np.repeat(np.random.randint(65, 95, n_patients), records_per_patient),\n                'gender': np.repeat(np.random.choice(['M', 'F'], n_patients, p=[0.4, 0.6]), records_per_patient),\n                'bmi': np.repeat(np.random.normal(26, 4, n_patients), records_per_patient),\n                'comorbidities_count': np.repeat(np.random.poisson(2, n_patients), records_per_patient),\n                'medication_count': np.repeat(np.random.poisson(4, n_patients), records_per_patient)}\n        X = pd.DataFrame(data)\n        \n        noise = np.random.normal(0, 2, total_records)\n        X['heart_rate'] = np.repeat(np.random.normal(75, 8, n_patients), records_per_patient) + np.sin(X['timestamp'].dt.hour * 2 * np.pi / 24) * 5 + noise\n        X['systolic_bp'] = np.repeat(np.random.normal(130, 15, n_patients), records_per_patient) + np.random.normal(0, 5, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.repeat(np.random.normal(50, 5, n_patients), records_per_patient)\n        X['grip_strength'] = np.repeat(np.random.normal(25, 6, n_patients), records_per_patient) * (1 - np.sin(X['timestamp'].dt.dayofweek * 2 * np.pi / 7) * 0.05 - np.random.rand(total_records)*0.1)\n        X['mobility_score'] = np.repeat(np.random.uniform(2, 9, n_patients), records_per_patient) * (1 - np.random.rand(total_records)*0.1)\n        \n        # --- G√©n√©ration de la cible avec une logique temporelle corrig√©e ---\n        # D√©finition d'une fonction pour √™tre utilis√©e avec .apply()\n        def calculate_rolling_std(df_group):\n            return df_group.rolling('12h', on='timestamp', min_periods=1)['heart_rate'].std()\n\n        # CORRECTION : Utiliser .apply() qui passe un sous-DataFrame\n        hr_instability = X.groupby('patient_id', group_keys=False).apply(calculate_rolling_std).bfill().ffill()\n        \n        base_risk = (X['age']/100) + (X['comorbidities_count']/10) - (X['mobility_score']/20)\n        score_raw = base_risk + (hr_instability / 10)\n        prob = 1 / (1 + np.exp(-(score_raw.fillna(0) - score_raw.mean())*5))\n        y = pd.Series(np.random.binomial(1, prob, size=len(X)), name=\"is_frail\")\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Lancement du pipeline\n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=10, version=\"11.5-final\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE CANDIDAT PRODUCTION EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:43:07.048618Z","iopub.execute_input":"2025-06-23T17:43:07.049679Z","iopub.status.idle":"2025-06-23T17:50:18.392703Z","shell.execute_reply.started":"2025-06-23T17:43:07.049640Z","shell.execute_reply":"2025-06-23T17:50:18.391815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V12 - TEST DE L'HYPOTH√àSE FINALE SUR UN SIGNAL CLAIR\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V12 : Test de l'hypoth√®se finale avec un signal inject√© non-ambigu.\")\nprint(\"üë∂ [Enfant] On va donner un exercice tr√®s facile √† nos machines pour √™tre s√ªr qu'elles savent lire !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\nimport warnings\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.70, 'min_auc': 0.75}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES AVANC√âE\")\nprint(\"=\"*80)\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self, rolling_window='12h'): self.rolling_window = rolling_window\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        if 'comorbidities_count' in X.columns: X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            for col in ['heart_rate', 'pulse_pressure', 'mobility_score']:\n                if col in X_copy.columns:\n                    rolling_group = X_copy.groupby('patient_id').rolling(self.rolling_window, on='timestamp', min_periods=1)\n                    X_copy[f'{col}_roll_mean'] = rolling_group[col].mean().reset_index(level=0, drop=True)\n                    X_copy[f'{col}_roll_std'] = rolling_group[col].std().reset_index(level=0, drop=True)\n        return X_copy.bfill().ffill()\nprint(\"‚úÖ Classe AdvancedFeatureEngineer pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI ROBUSTE)\")\nprint(\"=\"*80)\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\n\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    # Ici, nous ne mettons que 3 mod√®les pour un test rapide, mais les 13 autres peuvent √™tre ajout√©s sur le m√™me principe.\n    return models\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (v{version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        fe = AdvancedFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', QuantileTransformer(output_distribution='normal'))]), numeric_cols),\n            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√©.\")\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        return model, {\"champion\": champion_name, \"params\": best_params}\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V12 (TEST FINAL)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        n_patients = 200\n        records_per_patient = 50\n        total_records = n_patients * records_per_patient\n        \n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='h'))\n        \n        # Cr√©ation des profils de base\n        is_frail_profile = np.repeat([0, 1], n_patients // 2)\n        np.random.shuffle(is_frail_profile)\n        patient_profiles = pd.DataFrame({'patient_id': [f'P{i:03d}' for i in range(n_patients)],\n                                         'is_frail_profile': is_frail_profile})\n        \n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_hr_std'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(1, 3, n_patients), np.random.uniform(4, 8, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n\n        X = pd.DataFrame({'patient_id': patient_ids, 'timestamp': timestamps})\n        X = pd.merge(X, patient_profiles, on='patient_id')\n        \n        X['heart_rate'] = np.repeat(np.random.normal(75, 5, n_patients), records_per_patient) + np.repeat(X.groupby('patient_id')['base_hr_std'].first(), records_per_patient).values * np.random.randn(total_records)\n        X['mobility_score'] = np.repeat(X.groupby('patient_id')['base_mobility'].first(), records_per_patient) - np.random.rand(total_records) * 0.5\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(np.where(X['is_frail_profile']==1, 3, 1), total_records)\n        X['grip_strength'] = np.random.normal(np.where(X['is_frail_profile']==1, 20, 35), 5, total_records)\n        \n        y = pd.Series(X['is_frail_profile'], name=\"is_frail\")\n        X = X.drop('is_frail_profile', axis=1)\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        # Assouplir les contraintes pour ce test final\n        eval_config.alert_thresholds = {'recall': 0.65, 'min_auc': 0.70}\n        print(f\"\\n[DEBUG] Seuils temporairement assouplis √† : {eval_config.alert_thresholds}\")\n\n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=10, version=\"12.0-final-test\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ HYPOTH√àSE VALID√âE : LE PIPELINE FONCTIONNE SUR DES DONN√âES AVEC SIGNAL ! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî HYPOTH√àSE INVALID√âE : M√äME AVEC UN SIGNAL CLAIR, LE PIPELINE √âCHOUE.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:57:21.468389Z","iopub.execute_input":"2025-06-23T17:57:21.468903Z","iopub.status.idle":"2025-06-23T17:57:21.587578Z","shell.execute_reply.started":"2025-06-23T17:57:21.468862Z","shell.execute_reply":"2025-06-23T17:57:21.586544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V12.1 - CORRECTION DE L'ASSIGNATION D'INDEX PANDAS\n# =============================================================================\nprint(\"\\nüîç [Expert] Initialisation du pipeline V12.1 : Correction de l'alignement d'index dans la g√©n√©ration de donn√©es.\")\nprint(\"üë∂ [Enfant] On a appris √† la machine √† bien coller les √©tiquettes sur les bonnes bo√Ætes !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# (INCHANG√â)\nimport numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import joblib; import os; import json; import traceback; from functools import partial; import warnings\nimport optuna; from sklearn.ensemble import *; from sklearn.linear_model import *; from sklearn.svm import *; from sklearn.neighbors import *; from sklearn.naive_bayes import *; from sklearn.tree import *; from sklearn.discriminant_analysis import *; from sklearn.neural_network import *; import lightgbm as lgb; import xgboost as xgb; from catboost import CatBoostClassifier\nfrom sklearn.model_selection import *; from sklearn.pipeline import *; from sklearn.compose import *; from sklearn.preprocessing import *; from sklearn.impute import *; from sklearn.base import *; from sklearn.metrics import *; from sklearn.calibration import *\nRANDOM_STATE = 42; np.random.seed(RANDOM_STATE); warnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry: plt.style.use('seaborn-v0_8-whitegrid')\nexcept: plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES\n# (INCHANG√â)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.70, 'min_auc': 0.75}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING AVANC√â\n# (INCHANG√â)\nclass AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self, rolling_window='12h'): self.rolling_window = rolling_window\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        if 'comorbidities_count' in X.columns: X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            for col in ['heart_rate', 'pulse_pressure', 'mobility_score']:\n                if col in X_copy.columns:\n                    rolling_group = X_copy.groupby('patient_id').rolling(self.rolling_window, on='timestamp', min_periods=1)\n                    X_copy[f'{col}_roll_mean'] = rolling_group[col].mean().reset_index(level=0, drop=True)\n                    X_copy[f'{col}_roll_std'] = rolling_group[col].std().reset_index(level=0, drop=True)\n        return X_copy.bfill().ffill()\nprint(\"‚úÖ Classe AdvancedFeatureEngineer pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET 5, 6, 7 (INCHANG√âS)\n# (Le reste du pipeline est robuste et n'a pas besoin de modifications)\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), 'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    return models\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', AdvancedFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    return final_pipeline, MODEL_DIR\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        fe = AdvancedFeatureEngineer()\n        X_engineered = fe.fit_transform(X)\n        numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n        preprocessor_dynamic = ColumnTransformer(transformers=[('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', QuantileTransformer(output_distribution='normal'))]), numeric_cols),('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)], remainder='drop')\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        return model, {\"champion\": champion_name, \"params\": best_params}\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V12.1 (CORRIG√âE)\"); print(\"=\"*80)\n        \n        # --- G√âN√âRATION DE DONN√âES AVEC PROFILS ET SIGNAL CLAIR ---\n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        n_patients = 200\n        records_per_patient = 50\n        total_records = n_patients * records_per_patient\n        \n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='h'))\n        \n        is_frail_profile = np.repeat([0, 1], n_patients // 2)\n        np.random.shuffle(is_frail_profile)\n        patient_profiles = pd.DataFrame({'patient_id': [f'P{i:03d}' for i in range(n_patients)], 'is_frail_profile': is_frail_profile})\n        \n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X = pd.DataFrame({'patient_id': patient_ids, 'timestamp': timestamps})\n        X = pd.merge(X, patient_profiles, on='patient_id')\n        \n        # CORRECTION : Utiliser .values pour assigner des tableaux NumPy et √©viter les erreurs d'index\n        X['mobility_score'] = np.repeat(X.groupby('patient_id')['base_mobility'].first().values, records_per_patient) - np.random.rand(total_records) * 0.5\n        X['grip_strength'] = np.repeat(X.groupby('patient_id')['base_grip'].first().values, records_per_patient) + np.random.normal(0, 1, total_records)\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['heart_rate'] = np.random.normal(75, 10, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(np.where(X['is_frail_profile']==1, 3, 1), total_records)\n        \n        y = pd.Series(X['is_frail_profile'], name=\"is_frail\")\n        X = X.drop(['is_frail_profile', 'base_mobility', 'base_grip'], axis=1)\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        eval_config.alert_thresholds = {'recall': 0.85, 'min_auc': 0.90}\n        print(f\"\\n[INFO] Seuils r√©-ajust√©s pour un probl√®me plus facile : {eval_config.alert_thresholds}\")\n\n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=10, version=\"12.1-final-test\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ HYPOTH√àSE VALID√âE : LE PIPELINE FONCTIONNE SUR DES DONN√âES AVEC SIGNAL ! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:01:11.513761Z","iopub.execute_input":"2025-06-23T18:01:11.514241Z","iopub.status.idle":"2025-06-23T18:05:31.488147Z","shell.execute_reply.started":"2025-06-23T18:01:11.514209Z","shell.execute_reply":"2025-06-23T18:05:31.486875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V13 - TEST DE LA SIMPLICIT√â (FE MINIMAL)\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V13 : Test de l'hypoth√®se de la simplicit√©. Feature Engineering minimal.\")\nprint(\"üë∂ [Enfant] On a enlev√© les pi√®ces compliqu√©es de la machine pour voir si elle marche mieux avec juste l'essentiel.\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\nimport warnings\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.85, 'min_auc': 0.90}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (SIMPLIFI√â)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (MINIMALISTE POUR TEST)\")\nprint(\"=\"*80)\nclass SimpleFeatureEngineer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X_copy.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        return X_copy\nprint(\"‚úÖ Classe SimpleFeatureEngineer pr√™te.\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI ROBUSTE)\")\nprint(\"=\"*80)\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', SimpleFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\n\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    \n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    \n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    \n    # On inclut tous les mod√®les pour un test complet\n    def get_xgb_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 800), 'learning_rate': t.suggest_float('learning_rate', 0.01, 0.3), 'max_depth': t.suggest_int('max_depth', 3, 10), 'scale_pos_weight': scale_pos_weight}\n    models[\"XGB\"] = {'constructor': xgb.XGBClassifier, 'params': get_xgb_params}\n    \n    return models\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüîç Lancement du tournoi des mod√®les...\")\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(f\"\\nüîç Entra√Ænement du champion '{champion_name}' (v{version})\")\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', SimpleFeatureEngineer()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = SimpleFeatureEngineer() # Utilisation du FE simple\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [c for c in X_engineered.select_dtypes(include=np.number) if c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√©.\")\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        return model, {\"champion\": champion_name, \"params\": best_params}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V13 (TEST DE LA SIMPLICIT√â)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        n_patients = 200\n        total_records = n_patients * 50 # 50 enregistrements par patient\n        \n        patient_profiles = pd.DataFrame({\n            'patient_id': [f'P{i:03d}' for i in range(n_patients)],\n            'is_frail_profile': np.repeat([0, 1], n_patients // 2)\n        })\n        np.random.shuffle(patient_profiles['is_frail_profile'].values)\n        \n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X_list = []\n        for _, patient in patient_profiles.iterrows():\n            records = pd.DataFrame({'patient_id': [patient['patient_id']] * records_per_patient})\n            records['age'] = patient['age']\n            records['mobility_score'] = np.random.normal(patient['base_mobility'], 0.5, records_per_patient).clip(0, 10)\n            records['grip_strength'] = np.random.normal(patient['base_grip'], 2, records_per_patient).clip(0, 50)\n            X_list.append(records)\n        \n        X = pd.concat(X_list, ignore_index=True)\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['heart_rate'] = np.random.normal(75, 10, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(2, total_records)\n\n        y = pd.merge(X[['patient_id']], patient_profiles[['patient_id', 'is_frail_profile']], on='patient_id')['is_frail_profile']\n        \n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=10, version=\"13.0-simple-fe-test\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ HYPOTH√àSE VALID√âE : LE PIPELINE FONCTIONNE SUR DES DONN√âES AVEC SIGNAL ! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline s'est termin√© mais n'a pas pu produire de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:12:26.485048Z","iopub.execute_input":"2025-06-23T18:12:26.485430Z","iopub.status.idle":"2025-06-23T18:14:37.399682Z","shell.execute_reply.started":"2025-06-23T18:12:26.485405Z","shell.execute_reply":"2025-06-23T18:14:37.398573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V14 - TEST D'IMPACT DES FEATURES D'INTERACTION\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V14 : Test de l'impact des features d'interaction.\")\nprint(\"üë∂ [Enfant] On a une machine qui marche. Maintenant, on essaie de lui ajouter une nouvelle pi√®ce pour voir si elle devient plus forte !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\nimport warnings\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.85, 'min_auc': 0.90}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (INCR√âMENTAL)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V14 - INTERACTIONS)\")\nprint(\"=\"*80)\nclass FeatureEngineerV14(BaseEstimator, TransformerMixin):\n    \"\"\"Cr√©e des features de base ET des interactions m√©tier.\"\"\"\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        # Features de base (valid√©es dans la V13)\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        \n        # NOUVELLES FEATURES : Interactions\n        if 'bmi' in X_copy.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        if 'comorbidities_count' in X_copy.columns: X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n        \n        return X_copy\nprint(\"‚úÖ Classe FeatureEngineerV14 (avec interactions) pr√™te.\")\n\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI ROBUSTE)\")\nprint(\"=\"*80)\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    # MODIFICATION : Utilisation du Feature Engineer V14\n    pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\n\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    return models\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    # MODIFICATION : Utilisation du Feature Engineer V14\n    final_pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = FeatureEngineerV14() # Utilisation du FE V14\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n\n        preprocessor_dynamic = ColumnTransformer(transformers=[\n            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),\n            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)\n        ], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√© pour {len(numeric_cols)} features.\")\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        return model, {\"champion\": champion_name, \"params\": best_params}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V14 (TEST DES INTERACTIONS)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        n_patients = 200\n        total_records = n_patients * 50\n        \n        patient_profiles = pd.DataFrame({\n            'patient_id': [f'P{i:03d}' for i in range(n_patients)],\n            'is_frail_profile': np.repeat([0, 1], n_patients // 2)\n        })\n        np.random.shuffle(patient_profiles['is_frail_profile'].values)\n        \n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X_list = []\n        for _, patient in patient_profiles.iterrows():\n            records = pd.DataFrame({'patient_id': [patient['patient_id']] * records_per_patient})\n            records['age'] = patient['age']\n            records['mobility_score'] = np.random.normal(patient['base_mobility'], 0.5, records_per_patient).clip(0, 10)\n            records['grip_strength'] = np.random.normal(patient['base_grip'], 2, records_per_patient).clip(0, 50)\n            X_list.append(records)\n        \n        X = pd.concat(X_list, ignore_index=True)\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['heart_rate'] = np.random.normal(75, 10, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(np.where(pd.merge(X[['patient_id']], patient_profiles, on='patient_id')['is_frail_profile']==1, 3, 1), total_records)\n        \n        y = pd.merge(X[['patient_id']], patient_profiles[['patient_id', 'is_frail_profile']], on='patient_id')['is_frail_profile']\n        \n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=10, version=\"14.0-interaction-test\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE V14 EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline V14 a √©chou√© ou n'a pas produit de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:21:55.106308Z","iopub.execute_input":"2025-06-23T18:21:55.107556Z","iopub.status.idle":"2025-06-23T18:23:55.457750Z","shell.execute_reply.started":"2025-06-23T18:21:55.107465Z","shell.execute_reply":"2025-06-23T18:23:55.456729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V15 - CANDIDAT PRODUCTION AVEC FE COMPLET\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V15 : Synth√®se de toutes les le√ßons apprises.\")\nprint(\"üë∂ [Enfant] On a construit notre meilleure machine ! On lance le grand championnat final !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport json\nimport traceback\nfrom functools import partial\nimport warnings\n\n# --- Imports pour le Machine Learning ---\nimport optuna\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import (roc_auc_score, fbeta_score, recall_score, precision_score, \n                           confusion_matrix, classification_report, make_scorer, \n                           balanced_accuracy_score)\nfrom sklearn.calibration import calibration_curve\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES (MODE PRODUCTION)\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.80, 'min_auc': 0.85}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n    def _specificity(self, y, p):\n        try:\n            tn, fp, _, _ = confusion_matrix(y, p, labels=[0, 1]).ravel()\n            return 0.0 if (tn + fp) == 0 else tn / (tn + fp)\n        except ValueError:\n            return 0.0\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING COMPLET (V15)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V15 - COMPLET)\")\nprint(\"=\"*80)\nclass FeatureEngineerV15(BaseEstimator, TransformerMixin):\n    def __init__(self, rolling_window='12h'): self.rolling_window = rolling_window\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        # Features de base\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        # Features d'interaction\n        if 'bmi' in X_copy.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        if 'comorbidities_count' in X.columns: X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n        # Features temporelles\n        if 'timestamp' in X_copy.columns and 'patient_id' in X_copy.columns:\n            X_copy = X_copy.sort_values(by=['patient_id', 'timestamp'])\n            for col in ['heart_rate', 'pulse_pressure', 'mobility_score']:\n                if col in X_copy.columns:\n                    rolling_group = X_copy.groupby('patient_id').rolling(self.rolling_window, on='timestamp', min_periods=1)\n                    X_copy[f'{col}_roll_mean'] = rolling_group[col].mean().reset_index(level=0, drop=True)\n                    X_copy[f'{col}_roll_std'] = rolling_group[col].std().reset_index(level=0, drop=True)\n        return X_copy.bfill().ffill()\nprint(\"‚úÖ Classe FeatureEngineerV15 (compl√®te) pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET D'ANALYSE\n# =============================================================================\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), 'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\ndef analyze_decision_threshold(y_true, y_probs, plot=True):\n    thresholds = np.linspace(0.1, 0.9, 20); metrics = []\n    for thresh in thresholds:\n        y_pred = (y_probs >= thresh).astype(int)\n        metrics.append({'threshold': thresh, 'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n                        'recall': recall_score(y_true, y_pred, zero_division=0), 'precision': precision_score(y_true, y_pred, zero_division=0),\n                        'specificity': eval_config._specificity(y_true, y_pred)})\n    metrics_df = pd.DataFrame(metrics)\n    if plot:\n        try:\n            plt.figure(figsize=(10, 6)); metrics_df.plot(x='threshold', y=['f2', 'recall', 'precision', 'specificity'], ax=plt.gca())\n            plt.title(\"Analyse des M√©triques par Seuil\"); plt.xlabel(\"Seuil\"); plt.ylabel(\"Score\"); plt.grid(True); plt.show()\n        except Exception as e: print(f\"‚ö†Ô∏è La visualisation des seuils a √©chou√© : {e}\")\n    return metrics_df\n\ndef plot_calibration(y_true, y_probs):\n    try:\n        prob_true, prob_pred = calibration_curve(y_true, y_probs, n_bins=10)\n        plt.figure(figsize=(8, 6)); plt.plot(prob_pred, prob_true, marker='o', label='Mod√®le')\n        plt.plot([0, 1], [0, 1], linestyle='--', label='Parfaitement calibr√©')\n        plt.xlabel(\"Probabilit√© Moyenne Pr√©dite\"); plt.ylabel(\"Fraction de Positifs R√©elle\")\n        plt.title(\"Courbe de Calibration\"); plt.legend(); plt.grid(True); plt.show()\n    except Exception as e: print(f\"‚ö†Ô∏è La visualisation de la calibration a √©chou√©: {e}\")\n\n# 5. üöÄ OPTIMISATION MULTI-MOD√àLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. üöÄ OPTIMISATION MULTI-MOD√àLES (TOURNOI ROBUSTE)\")\nprint(\"=\"*80)\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', FeatureEngineerV15()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\n\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    return models\n\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 6. üè≠ ENTRA√éNEMENT ET VERSIONNING\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', FeatureEngineerV15()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/4] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        fe = FeatureEngineerV15()\n        X_engineered = fe.fit_transform(X)\n        numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n        preprocessor_dynamic = ColumnTransformer(transformers=[('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', QuantileTransformer(output_distribution='normal'))]), numeric_cols),('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)], remainder='drop')\n        print(f\"‚úÖ Pr√©processeur dynamique cr√©√©.\")\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n        print(\"\\n[√âtape 2/4] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n        print(\"\\n[√âtape 3/4] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n        print(\"\\n[√âtape 4/4] üìà √âvaluation compl√®te et rapport final\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        analyze_decision_threshold(y_test, y_probs)\n        plot_calibration(y_test, y_probs)\n        return model, {\"champion\": champion_name, \"params\": best_params}\n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V15 (CANDIDAT PRODUCTION)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils patients et un signal temporel clair...\")\n        n_patients = 200; records_per_patient = 50; total_records = n_patients * records_per_patient\n        patient_ids = np.repeat([f'P{i:03d}' for i in range(n_patients)], records_per_patient)\n        timestamps = pd.to_datetime(pd.date_range('2024-01-01', periods=total_records, freq='h'))\n        \n        is_frail_profile = np.repeat([0, 1], n_patients // 2); np.random.shuffle(is_frail_profile)\n        patient_profiles = pd.DataFrame({'patient_id': [f'P{i:03d}' for i in range(n_patients)], 'is_frail_profile': is_frail_profile})\n        \n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_hr_std'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(1, 3, n_patients), np.random.uniform(4, 8, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X_list = []\n        for _, patient in patient_profiles.iterrows():\n            records = pd.DataFrame({'patient_id': [patient['patient_id']] * records_per_patient})\n            records['age'] = patient['age']\n            records['mobility_score'] = np.random.normal(patient['base_mobility'], 0.5, records_per_patient).clip(0, 10)\n            records['grip_strength'] = np.random.normal(patient['base_grip'], 2, records_per_patient).clip(0, 50)\n            records['heart_rate'] = np.random.normal(75, patient['base_hr_std'], records_per_patient)\n            X_list.append(records)\n        \n        X = pd.concat(X_list, ignore_index=True)\n        X['timestamp'] = timestamps # Assigner les timestamps apr√®s la concat√©nation\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(np.where(pd.merge(X[['patient_id']], patient_profiles, on='patient_id')['is_frail_profile']==1, 3, 1), total_records)\n        \n        y = pd.merge(X[['patient_id']], patient_profiles[['patient_id', 'is_frail_profile']], on='patient_id')['is_frail_profile']\n\n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        model, report = run_full_pipeline(X, y.values, n_trials_per_model=15, version=\"15.0-full-fe-test\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE V15 EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline V15 a √©chou√© ou n'a pas produit de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T19:10:28.354233Z","iopub.execute_input":"2025-06-23T19:10:28.354752Z","iopub.status.idle":"2025-06-23T19:16:59.699095Z","shell.execute_reply.started":"2025-06-23T19:10:28.354722Z","shell.execute_reply":"2025-06-23T19:16:59.698063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V14.1 - AVEC EXPLICABILIT√â (XAI) VIA SHAP\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V14.1 : Ajout de l'explicabilit√© pour comprendre le 'pourquoi'.\")\nprint(\"üë∂ [Enfant] On va demander √† notre machine de nous expliquer comment elle r√©fl√©chit !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\n# --- Imports standards ---\nimport numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import joblib; import os; import json; import traceback; from functools import partial; import warnings\n# --- Imports pour le Machine Learning ---\nimport optuna; from sklearn.ensemble import *; from sklearn.linear_model import *; from sklearn.svm import *; from sklearn.neighbors import *; from sklearn.naive_bayes import *; from sklearn.tree import *; from sklearn.discriminant_analysis import *; from sklearn.neural_network import *; import lightgbm as lgb; import xgboost as xgb; from catboost import CatBoostClassifier\nfrom sklearn.model_selection import *; from sklearn.pipeline import *; from sklearn.compose import *; from sklearn.preprocessing import *; from sklearn.impute import *; from sklearn.base import *; from sklearn.metrics import *; from sklearn.calibration import *\n# --- NOUVEL IMPORT POUR L'EXPLICABILIT√â ---\nimport shap\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42; np.random.seed(RANDOM_STATE); warnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry: plt.style.use('seaborn-v0_8-whitegrid')\nexcept: plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.85, 'min_auc': 0.90}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (V14)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V14 - INTERACTIONS)\")\nprint(\"=\"*80)\nclass FeatureEngineerV14(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_copy = X.copy()\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        if 'bmi' in X.columns: X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n        if 'comorbidities_count' in X.columns: X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n        return X_copy\nprint(\"‚úÖ Classe FeatureEngineerV14 (avec interactions) pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION ET 5, 6... (logique inchang√©e)\n# ...\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n# ...\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\ndef get_model_definitions():\n    models = {}\n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), 'num_leaves': t.suggest_int('num_leaves', 10, 50), 'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), 'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    def get_rf_params(t, y):\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), 'max_depth': t.suggest_int('max_depth', 5, 20), 'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), 'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    def get_logreg_params(t,y):\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    return models\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# NOUVELLE FONCTION : Analyse de l'explicabilit√©\n# =============================================================================\ndef analyze_explicability(pipeline, X_data, champion_name):\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß† ANALYSE DE L'EXPLICABILIT√â (XAI) AVEC SHAP\")\n    print(\"=\"*80)\n    \n    try:\n        # Extraire les √©tapes du pipeline\n        feature_engineer = pipeline.named_steps['fe']\n        preprocessor = pipeline.named_steps['pre']\n        model = pipeline.named_steps['clf']\n        \n        # Transformer les donn√©es comme le ferait le pipeline\n        X_engineered = feature_engineer.transform(X_data)\n        X_processed = preprocessor.transform(X_engineered)\n        \n        # R√©cup√©rer les noms de features finaux\n        feature_names = preprocessor.get_feature_names_out()\n        \n        # Cr√©er l'explainer SHAP\n        # Utiliser TreeExplainer pour les mod√®les √† base d'arbres, KernelExplainer pour les autres\n        if champion_name in [\"LGBM\", \"RandomForest\", \"XGB\", \"CatBoost\", \"DecisionTree\", \"ExtraTrees\"]:\n            explainer = shap.TreeExplainer(model)\n        else:\n            # KernelExplainer est plus lent, on utilise un sous-√©chantillon\n            X_summary = shap.sample(X_processed, 100)\n            explainer = shap.KernelExplainer(model.predict_proba, X_summary)\n            \n        print(\"‚öôÔ∏è Calcul des valeurs SHAP... (peut prendre un moment)\")\n        shap_values = explainer.shap_values(X_processed)\n        \n        # Pour les classifieurs binaires, shap_values peut √™tre une liste de 2 arrays\n        # On s'int√©resse √† l'impact sur la classe positive (classe 1)\n        if isinstance(shap_values, list):\n            shap_values = shap_values[1]\n            \n        # Cr√©er le DataFrame pour la visualisation\n        X_processed_df = pd.DataFrame(X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed, columns=feature_names)\n\n        print(\"üìä Affichage du SHAP Summary Plot...\")\n        shap.summary_plot(shap_values, X_processed_df, plot_type=\"bar\", show=False)\n        plt.title(f\"Importance Globale des Features (SHAP) - Mod√®le {champion_name}\")\n        plt.show()\n        \n        shap.summary_plot(shap_values, X_processed_df, show=False)\n        plt.title(f\"Distribution de l'Impact des Features (SHAP) - Mod√®le {champion_name}\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"‚ùå Erreur lors de l'analyse SHAP : {e}\")\n        traceback.print_exc()\n\n# 7. üèÅ PIPELINE COMPLET ORCHESTR√â (MODIFI√â)\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    try:\n        print(\"\\n[√âtape 1/5] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = FeatureEngineerV14()\n        X_engineered = fe.fit_transform(X)\n        \n        numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n        categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n        preprocessor_dynamic = ColumnTransformer(transformers=[('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols),('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))]), categorical_cols)], remainder='drop')\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/5] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/5] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/5] üìà √âvaluation compl√®te\")\n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        # NOUVELLE √âTAPE : EXPLICABILIT√â\n        print(\"\\n[√âtape 5/5] üß† Analyse de l'explicabilit√© du mod√®le\")\n        analyze_explicability(model, X_test, champion_name)\n        \n        return model, {\"champion\": champion_name, \"params\": best_params}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\"); traceback.print_exc()\n\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V14.1 (AVEC XAI)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        n_patients = 200; records_per_patient = 50; total_records = n_patients * records_per_patient\n        \n        patient_profiles = pd.DataFrame({'patient_id': [f'P{i:03d}' for i in range(n_patients)], 'is_frail_profile': np.repeat([0, 1], n_patients // 2)})\n        np.random.shuffle(patient_profiles['is_frail_profile'].values)\n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X_list = []\n        for _, patient in patient_profiles.iterrows():\n            records = pd.DataFrame({'patient_id': [patient['patient_id']] * records_per_patient})\n            records['age'] = patient['age']\n            records['mobility_score'] = np.random.normal(patient['base_mobility'], 0.5, records_per_patient).clip(0, 10)\n            records['grip_strength'] = np.random.normal(patient['base_grip'], 2, records_per_patient).clip(0, 50)\n            X_list.append(records)\n        X = pd.concat(X_list, ignore_index=True)\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['heart_rate'] = np.random.normal(75, 10, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(np.where(pd.merge(X[['patient_id']], patient_profiles, on='patient_id')['is_frail_profile']==1, 3, 1), total_records)\n        \n        y = pd.merge(X[['patient_id']], patient_profiles[['patient_id', 'is_frail_profile']], on='patient_id')['is_frail_profile']\n        \n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        model, report = run_full_pipeline(X.drop('patient_id', axis=1), y.values, n_trials_per_model=10, version=\"14.1-xai-test\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE V14.1 AVEC XAI EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n        else:\n            print(\"\\nüíî Le pipeline V14.1 a √©chou√© ou n'a pas produit de mod√®le valide.\")\n\n    except Exception:\n        print(\"\\nüí• Le pipeline a √©chou√©. Voir les erreurs ci-dessus.\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:45:41.962136Z","iopub.execute_input":"2025-06-24T16:45:41.962446Z","iopub.status.idle":"2025-06-24T16:47:59.874672Z","shell.execute_reply.started":"2025-06-24T16:45:41.962419Z","shell.execute_reply":"2025-06-24T16:47:59.873545Z"}},"outputs":[{"name":"stdout","text":"\nüîç [Expert] Initialisation du pipeline V14.1 : Ajout de l'explicabilit√© pour comprendre le 'pourquoi'.\nüë∂ [Enfant] On va demander √† notre machine de nous expliquer comment elle r√©fl√©chit !\n\n================================================================================\n1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\n================================================================================\n‚úÖ Environnement configur√©.\n\n================================================================================\n2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\n================================================================================\nSeuils d'alerte configur√©s : {'recall': 0.85, 'min_auc': 0.9}\n\n================================================================================\n3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V14 - INTERACTIONS)\n================================================================================\n‚úÖ Classe FeatureEngineerV14 (avec interactions) pr√™te.\n\n================================================================================\nüöÄ D√âMARRAGE DE L'EX√âCUTION V14.1 (AVEC XAI)\n================================================================================\n\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\nüìä Donn√©es g√©n√©r√©es : 10000 enregistrements pour 200 patients.\nüéØ Distribution de la cible :\nis_frail_profile\n0    0.5\n1    0.5\nName: proportion, dtype: float64\n\n[√âtape 1/5] üõ°Ô∏è Configuration du pipeline dynamique\n‚úÖ Donn√©es s√©par√©es (train: 8000, test: 2000)\n\n[√âtape 2/5] üöÄ Optimisation et s√©lection du meilleur mod√®le\n\n---      Manche du tournoi : LGBM       ---\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-24 16:46:04,451] A new study created in RDB with name: frailty_LGBM_v14.1-xai-test\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab9021d7f22439a8964b2138cb66b05"}},"metadata":{}},{"name":"stdout","text":"[I 2025-06-24 16:46:11,321] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 437, 'learning_rate': 0.1540359659501924, 'num_leaves': 40, 'scale_pos_weight': 1.2993292418740214}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:12,557] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 240, 'learning_rate': 0.002285325525633921, 'num_leaves': 12, 'scale_pos_weight': 1.4330880725626516}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:14,520] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 641, 'learning_rate': 0.04258888210290081, 'num_leaves': 10, 'scale_pos_weight': 1.484954925717281}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:18,719] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 850, 'learning_rate': 0.0030803400529839683, 'num_leaves': 17, 'scale_pos_weight': 1.0917022548579403}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:20,594] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 374, 'learning_rate': 0.016124278458562614, 'num_leaves': 27, 'scale_pos_weight': 1.14561456998981}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:23,976] Trial 5 finished with value: 1.0 and parameters: {'n_estimators': 651, 'learning_rate': 0.002094013887393744, 'num_leaves': 21, 'scale_pos_weight': 1.1831809215094602}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:25,586] Trial 6 finished with value: 1.0 and parameters: {'n_estimators': 510, 'learning_rate': 0.06407866261851015, 'num_leaves': 18, 'scale_pos_weight': 1.257117219013968}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:29,175] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 633, 'learning_rate': 0.0012790390175145834, 'num_leaves': 34, 'scale_pos_weight': 1.0852620617796993}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:29,888] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 158, 'learning_rate': 0.15255065745117383, 'num_leaves': 49, 'scale_pos_weight': 1.4041986737550816}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:31,546] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 374, 'learning_rate': 0.001677824238407793, 'num_leaves': 38, 'scale_pos_weight': 1.2200762467047435}. Best is trial 0 with value: 1.0.\n‚úÖ Meilleur F2-score pour LGBM: 1.0000\n\n---  Manche du tournoi : RandomForest   ---\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-24 16:46:31,860] A new study created in RDB with name: frailty_RandomForest_v14.1-xai-test\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23db2f8a823b46f98eed241ad1e2336b"}},"metadata":{}},{"name":"stdout","text":"[I 2025-06-24 16:46:37,917] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 437, 'max_depth': 20, 'class_weight': 'balanced'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:43,037] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 240, 'max_depth': 7, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:46:55,359] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 641, 'max_depth': 16, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:11,456] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 850, 'max_depth': 8, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:17,284] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 374, 'max_depth': 13, 'class_weight': 'balanced'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:29,587] Trial 5 finished with value: 1.0 and parameters: {'n_estimators': 651, 'max_depth': 7, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:39,323] Trial 6 finished with value: 1.0 and parameters: {'n_estimators': 510, 'max_depth': 17, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:47,860] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 633, 'max_depth': 5, 'class_weight': 'balanced'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:50,278] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 158, 'max_depth': 20, 'class_weight': 'balanced'}. Best is trial 0 with value: 1.0.\n[I 2025-06-24 16:47:55,316] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 374, 'max_depth': 6, 'class_weight': 'balanced'}. Best is trial 0 with value: 1.0.\n‚úÖ Meilleur F2-score pour RandomForest: 1.0000\n\n--- Manche du tournoi : LogisticRegression ---\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-24 16:47:55,856] A new study created in RDB with name: frailty_LogisticRegression_v14.1-xai-test\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfde977aa5fb4364b2ed271d031ff9cd"}},"metadata":{}},{"name":"stdout","text":"[I 2025-06-24 16:47:56,131] Trial 0 finished with value: 0.9997999499874968 and parameters: {'C': 0.31489116479568624}. Best is trial 0 with value: 0.9997999499874968.\n[I 2025-06-24 16:47:56,405] Trial 1 finished with value: 1.0 and parameters: {'C': 63.512210106407046}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:56,675] Trial 2 finished with value: 1.0 and parameters: {'C': 8.471801418819979}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:56,949] Trial 3 finished with value: 1.0 and parameters: {'C': 2.481040974867813}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:57,239] Trial 4 finished with value: 0.9995998999749938 and parameters: {'C': 0.04207988669606638}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:57,490] Trial 5 finished with value: 0.9995998999749938 and parameters: {'C': 0.042070539502879395}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:57,727] Trial 6 finished with value: 0.9995998999749938 and parameters: {'C': 0.017073967431528128}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:57,999] Trial 7 finished with value: 1.0 and parameters: {'C': 29.154431891537552}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:58,257] Trial 8 finished with value: 1.0 and parameters: {'C': 2.5378155082656657}. Best is trial 1 with value: 1.0.\n[I 2025-06-24 16:47:58,532] Trial 9 finished with value: 1.0 and parameters: {'C': 6.79657809075816}. Best is trial 1 with value: 1.0.\n‚úÖ Meilleur F2-score pour LogisticRegression: 1.0000\n\n========================================\nüèÜ CHAMPION : LGBM | F2-Score: 1.0000\n========================================\n\n[√âtape 3/5] üéì Entra√Ænement du mod√®le final\n[LightGBM] [Info] Number of positive: 4000, number of negative: 4000\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000693 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2517\n[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\nüíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : model_v14.1-xai-test\n\n[√âtape 4/5] üìà √âvaluation compl√®te\n\nüìä Rapport de classification :\n              precision    recall  f1-score   support\n\n Non fragile       1.00      1.00      1.00      1000\n     Fragile       1.00      1.00      1.00      1000\n\n    accuracy                           1.00      2000\n   macro avg       1.00      1.00      1.00      2000\nweighted avg       1.00      1.00      1.00      2000\n\n\n[√âtape 5/5] üß† Analyse de l'explicabilit√© du mod√®le\n\n================================================================================\nüß† ANALYSE DE L'EXPLICABILIT√â (XAI) AVEC SHAP\n================================================================================\n‚öôÔ∏è Calcul des valeurs SHAP... (peut prendre un moment)\nüìä Affichage du SHAP Summary Plot...\n","output_type":"stream"},{"name":"stderr","text":"LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x670 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x670 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nüéâüéâüéâ PIPELINE V14.1 AVEC XAI EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V14.2 - AVEC EXPLICABILIT√â (XAI) ET ALTERNATIVES SCIENTIFIQUES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V14.2 : Ajout de l'explicabilit√© et des alternatives scientifiques.\")\nprint(\"üë∂ [Enfant] On va demander √† notre machine de nous expliquer comment elle r√©fl√©chit et quelles autres m√©thodes existent !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"üî¨ [Science] Configuration choisie : Seed fixe pour la reproductibilit√©\")\nprint(\"üìö [Alternatives] D'autres approches existent :\")\nprint(\"   ‚Ä¢ Seed al√©atoire : Pour tester la robustesse statistique\")\nprint(\"   ‚Ä¢ Cross-validation stratifi√©e r√©p√©t√©e : Pour validation plus rigoureuse\")\nprint(\"   ‚Ä¢ Bootstrap sampling : Pour estimation d'intervalles de confiance\")\n\n# --- Imports standards ---\nimport numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import joblib; import os; import json; import traceback; from functools import partial; import warnings\n# --- Imports pour le Machine Learning ---\nimport optuna; from sklearn.ensemble import *; from sklearn.linear_model import *; from sklearn.svm import *; from sklearn.neighbors import *; from sklearn.naive_bayes import *; from sklearn.tree import *; from sklearn.discriminant_analysis import *; from sklearn.neural_network import *; import lightgbm as lgb; import xgboost as xgb; from catboost import CatBoostClassifier\nfrom sklearn.model_selection import *; from sklearn.pipeline import *; from sklearn.compose import *; from sklearn.preprocessing import *; from sklearn.impute import *; from sklearn.base import *; from sklearn.metrics import *; from sklearn.calibration import *\n# --- IMPORT POUR L'EXPLICABILIT√â ---\nimport shap\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42; np.random.seed(RANDOM_STATE); warnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry: plt.style.use('seaborn-v0_8-whitegrid')\nexcept: plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"üî¨ [Science] M√©triques choisies : Recall (sensibilit√©) + AUC pour d√©s√©quilibre de classes\")\nprint(\"üìö [Alternatives] Autres m√©triques selon le contexte :\")\nprint(\"   ‚Ä¢ F1-Score : √âquilibre pr√©cision/rappel (quand co√ªts √©gaux)\")\nprint(\"   ‚Ä¢ Precision-Recall AUC : Mieux que ROC-AUC pour classes tr√®s d√©s√©quilibr√©es\")\nprint(\"   ‚Ä¢ Matthews Correlation Coefficient : Robuste aux d√©s√©quilibres extr√™mes\")\nprint(\"   ‚Ä¢ Balanced Accuracy : (Sensibilit√© + Sp√©cificit√©)/2\")\nprint(\"   ‚Ä¢ Cohen's Kappa : Accord inter-√©valuateurs ajust√© par le hasard\")\n\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.85, 'min_auc': 0.90}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n        print(\"‚ö†Ô∏è [Standard] Seuils industriels typiques :\")\n        print(\"   ‚Ä¢ M√©dical/S√©curit√© : Recall > 0.95, AUC > 0.95\")\n        print(\"   ‚Ä¢ Finance/Fraude : Precision > 0.80, AUC > 0.85\")\n        print(\"   ‚Ä¢ Marketing : F1 > 0.70, AUC > 0.75\")\n        \neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (V14)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V14 - INTERACTIONS)\")\nprint(\"=\"*80)\n\nprint(\"üî¨ [Science] Approche choisie : Feature engineering manuel avec interactions m√©dicales\")\nprint(\"üìö [Alternatives] Autres techniques de cr√©ation de features :\")\nprint(\"   ‚Ä¢ Automated Feature Engineering : Featuretools, tsfresh\")\nprint(\"   ‚Ä¢ Polynomial Features : sklearn.preprocessing.PolynomialFeatures\")\nprint(\"   ‚Ä¢ Feature Crosses : TensorFlow Feature Columns\")\nprint(\"   ‚Ä¢ Deep Feature Synthesis : Relations automatiques entre variables\")\nprint(\"   ‚Ä¢ Domain-Specific Transforms : Log, sqrt, reciprocal selon distribution\")\n\nclass FeatureEngineerV14(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None): \n        print(\"üß¨ [Medical] Features d√©riv√©es bas√©es sur la litt√©rature clinique :\")\n        print(\"   ‚Ä¢ Pulse Pressure = SBP - DBP (rigidit√© art√©rielle)\")\n        print(\"   ‚Ä¢ Shock Index = HR/SBP (√©tat h√©modynamique)\")\n        return self\n        \n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Features m√©dicales valid√©es\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        \n        # Interactions sp√©cifiques\n        if 'bmi' in X.columns: \n            X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n            print(\"üí° [Alternative] Autres interactions BMI possibles :\")\n            print(\"   ‚Ä¢ BMI categorization : Normal/Overweight/Obese\")\n            print(\"   ‚Ä¢ Waist-Hip ratio si disponible\")\n            \n        if 'comorbidities_count' in X.columns: \n            X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n            print(\"üí° [Alternative] Autres approches comorbidit√©s :\")\n            print(\"   ‚Ä¢ Charlson Comorbidity Index : Score pond√©r√©\")\n            print(\"   ‚Ä¢ Elixhauser Comorbidity Index : 30 conditions sp√©cifiques\")\n            \n        return X_copy\n\nprint(\"‚úÖ Classe FeatureEngineerV14 (avec interactions) pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\nprint(\"\\nüî¨ [Science] Validation crois√©e stratifi√©e : Pr√©serve la distribution des classes\")\nprint(\"üìö [Alternatives] Autres strat√©gies de validation :\")\nprint(\"   ‚Ä¢ Time Series Split : Pour donn√©es temporelles\")\nprint(\"   ‚Ä¢ Group K-Fold : Pour donn√©es group√©es (patients, r√©gions)\")\nprint(\"   ‚Ä¢ Leave-One-Out : Pour tr√®s petits datasets\")\nprint(\"   ‚Ä¢ Nested CV : Validation + s√©lection hyperparam√®tres simultan√©es\")\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n# 5. OPTIMISATION HYPERPARAM√àTRES\n# =============================================================================\nprint(\"\\nüî¨ [Science] Optimisation Bay√©sienne avec Optuna (Tree-structured Parzen Estimator)\")\nprint(\"üìö [Alternatives] Autres m√©thodes d'optimisation :\")\nprint(\"   ‚Ä¢ Grid Search : Exhaustif mais co√ªteux\")\nprint(\"   ‚Ä¢ Random Search : Plus efficace que Grid pour hautes dimensions\")\nprint(\"   ‚Ä¢ Hyperopt : Alternative √† Optuna (TPE + Adaptive)\")\nprint(\"   ‚Ä¢ BOHB : Bandit-based + Bayesian Optimization\")\nprint(\"   ‚Ä¢ Population-based Training : Pour deep learning\")\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\n\n# 6. D√âFINITION DES MOD√àLES\n# =============================================================================\ndef get_model_definitions():\n    print(\"\\nüî¨ [Science] S√©lection d'algorithmes pour classification binaire d√©s√©quilibr√©e\")\n    print(\"üìö [Alternatives] Autres familles d'algorithmes :\")\n    print(\"   ‚Ä¢ Deep Learning : Neural Networks, Transformers\")\n    print(\"   ‚Ä¢ Kernel Methods : SVM avec diff√©rents noyaux\")\n    print(\"   ‚Ä¢ Bayesian Methods : Gaussian Processes, Bayesian Neural Nets\")\n    print(\"   ‚Ä¢ Ensemble Methods : Voting, Stacking, Blending\")\n    print(\"   ‚Ä¢ Meta-learning : MAML, Few-shot learning\")\n    \n    models = {}\n    \n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        print(\"üå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\")\n        print(\"üí° [Alternatives LightGBM] :\")\n        print(\"   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\")\n        print(\"   ‚Ä¢ dart mode : Dropout regularization\")\n        print(\"   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\")\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), \n                'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), \n                'num_leaves': t.suggest_int('num_leaves', 10, 50), \n                'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), \n                'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    \n    def get_rf_params(t, y):\n        print(\"üå≤ [RandomForest] Ensemble de d√©cision avec bootstrap\")\n        print(\"üí° [Alternatives RandomForest] :\")\n        print(\"   ‚Ä¢ ExtraTreesClassifier : Plus al√©atoire, moins overfitting\")\n        print(\"   ‚Ä¢ IsolationForest : Pour d√©tection d'anomalies\")\n        print(\"   ‚Ä¢ Balanced RandomForest : SMOTE int√©gr√©\")\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), \n                'max_depth': t.suggest_int('max_depth', 5, 20), \n                'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), \n                'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    \n    def get_logreg_params(t,y):\n        print(\"üìà [LogisticRegression] Classification lin√©aire avec r√©gularisation\")\n        print(\"üí° [Alternatives LogisticRegression] :\")\n        print(\"   ‚Ä¢ ElasticNet : Combinaison L1 + L2\")\n        print(\"   ‚Ä¢ SGDClassifier : Pour tr√®s gros datasets\")\n        print(\"   ‚Ä¢ Multinomial regression : Pour multi-classes\")\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), \n                'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    \n    return models\n\n# 7. OPTIMISATION GLOBALE\n# =============================================================================\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüî¨ [Science] Multi-objective optimization avec early stopping\")\n    print(\"üìö [Alternatives] Autres strat√©gies d'optimisation :\")\n    print(\"   ‚Ä¢ Multi-objective : NSGA-II, MOEA/D\")\n    print(\"   ‚Ä¢ AutoML : Auto-sklearn, TPOT, H2O AutoML\")\n    print(\"   ‚Ä¢ Neural Architecture Search : DARTS, ENAS\")\n    \n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, \n                                    model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 8. TRAINING FINAL\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(\"\\nüî¨ [Science] Training final avec validation et sauvegarde versionn√©e\")\n    print(\"üìö [Alternatives] Autres approches de training :\")\n    print(\"   ‚Ä¢ Online Learning : Apprentissage incr√©mental\")\n    print(\"   ‚Ä¢ Transfer Learning : Pr√©-training + fine-tuning\")\n    print(\"   ‚Ä¢ Meta-Learning : Learning to learn\")\n    print(\"   ‚Ä¢ Federated Learning : Training distribu√©\")\n    \n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 9. ANALYSE D'EXPLICABILIT√â\n# =============================================================================\ndef analyze_explicability(pipeline, X_data, champion_name):\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß† ANALYSE DE L'EXPLICABILIT√â (XAI) AVEC SHAP\")\n    print(\"=\"*80)\n    \n    print(\"üî¨ [Science] SHAP : Valeurs de Shapley pour attribution causale\")\n    print(\"üìö [Alternatives XAI] Autres m√©thodes d'explicabilit√© :\")\n    print(\"   ‚Ä¢ LIME : Local Interpretable Model-agnostic Explanations\")\n    print(\"   ‚Ä¢ Permutation Importance : Impact par permutation\")\n    print(\"   ‚Ä¢ Anchors : R√®gles locales suffisantes\")\n    print(\"   ‚Ä¢ Counterfactuals : Sc√©narios 'what-if'\")\n    print(\"   ‚Ä¢ Integrated Gradients : Pour deep learning\")\n    print(\"   ‚Ä¢ DeepLIFT : Attribution pour r√©seaux profonds\")\n    \n    try:\n        # Extraire les √©tapes du pipeline\n        feature_engineer = pipeline.named_steps['fe']\n        preprocessor = pipeline.named_steps['pre']\n        model = pipeline.named_steps['clf']\n        \n        # Transformer les donn√©es comme le ferait le pipeline\n        X_engineered = feature_engineer.transform(X_data)\n        X_processed = preprocessor.transform(X_engineered)\n        \n        # R√©cup√©rer les noms de features finaux\n        feature_names = preprocessor.get_feature_names_out()\n        \n        # Cr√©er l'explainer SHAP selon le type de mod√®le\n        if champion_name in [\"LGBM\", \"RandomForest\", \"XGB\", \"CatBoost\", \"DecisionTree\", \"ExtraTrees\"]:\n            print(\"üå≥ [SHAP] Utilisation de TreeExplainer (exact pour mod√®les √† base d'arbres)\")\n            explainer = shap.TreeExplainer(model)\n        else:\n            print(\"üîç [SHAP] Utilisation de KernelExplainer (approximation par √©chantillonnage)\")\n            print(\"üí° [Alternative] LinearExplainer pour mod√®les lin√©aires (plus rapide)\")\n            # KernelExplainer est plus lent, on utilise un sous-√©chantillon\n            X_summary = shap.sample(X_processed, 100)\n            explainer = shap.KernelExplainer(model.predict_proba, X_summary)\n            \n        print(\"‚öôÔ∏è Calcul des valeurs SHAP... (peut prendre un moment)\")\n        shap_values = explainer.shap_values(X_processed)\n        \n        # Pour les classifieurs binaires, shap_values peut √™tre une liste de 2 arrays\n        # On s'int√©resse √† l'impact sur la classe positive (classe 1)\n        if isinstance(shap_values, list):\n            shap_values = shap_values[1]\n            \n        # Cr√©er le DataFrame pour la visualisation\n        X_processed_df = pd.DataFrame(X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed, columns=feature_names)\n\n        print(\"üìä Affichage du SHAP Summary Plot...\")\n        print(\"üí° [Visualisation] Autres types de plots SHAP :\")\n        print(\"   ‚Ä¢ Waterfall : Contribution individuelle\")\n        print(\"   ‚Ä¢ Force : Impact local d√©taill√©\")\n        print(\"   ‚Ä¢ Dependence : Relation feature-target\")\n        print(\"   ‚Ä¢ Decision : Arbre de d√©cision\")\n        \n        shap.summary_plot(shap_values, X_processed_df, plot_type=\"bar\", show=False)\n        plt.title(f\"Importance Globale des Features (SHAP) - Mod√®le {champion_name}\")\n        plt.show()\n        \n        shap.summary_plot(shap_values, X_processed_df, show=False)\n        plt.title(f\"Distribution de l'Impact des Features (SHAP) - Mod√®le {champion_name}\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"‚ùå Erreur lors de l'analyse SHAP : {e}\")\n        print(\"üîß [Debug] Solutions alternatives :\")\n        print(\"   ‚Ä¢ R√©duire taille √©chantillon pour KernelExplainer\")\n        print(\"   ‚Ä¢ Utiliser explainer sp√©cifique au mod√®le\")\n        print(\"   ‚Ä¢ Fallback vers permutation importance\")\n        traceback.print_exc()\n\n# 10. PREPROCESSING AVANC√â\n# =============================================================================\ndef create_advanced_preprocessor(X_engineered):\n    print(\"\\nüî¨ [Science] Preprocessing choisi : StandardScaler + OneHotEncoder\")\n    print(\"üìö [Alternatives] Autres techniques de preprocessing :\")\n    print(\"   ‚Ä¢ Normalization : MinMaxScaler, RobustScaler, QuantileTransformer\")\n    print(\"   ‚Ä¢ Encoding : Target Encoding, Binary Encoding, Hash Encoding\")\n    print(\"   ‚Ä¢ Imputation : KNN, Iterative, Forward/Backward Fill\")\n    print(\"   ‚Ä¢ Feature Selection : SelectKBest, RFE, L1-based\")\n    print(\"   ‚Ä¢ Dimensionality Reduction : PCA, t-SNE, UMAP\")\n    \n    numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n    categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n    \n    print(f\"üìä Features num√©riques d√©tect√©es : {len(numeric_cols)}\")\n    print(f\"üìä Features cat√©gorielles d√©tect√©es : {len(categorical_cols)}\")\n    \n    return ColumnTransformer(\n        transformers=[\n            ('num', Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy='median')), \n                ('scaler', StandardScaler())\n            ]), numeric_cols),\n            ('cat', Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy='most_frequent')), \n                ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n            ]), categorical_cols)\n        ], \n        remainder='drop'\n    )\n\n# 11. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    print(\"\\nüî¨ [Science] Architecture choisie : Pipeline end-to-end avec validation\")\n    print(\"üìö [Alternatives] Autres architectures ML :\")\n    print(\"   ‚Ä¢ MLOps : Kubeflow, MLflow, Airflow\")\n    print(\"   ‚Ä¢ AutoML : Auto-sklearn, TPOT, H2O\")\n    print(\"   ‚Ä¢ Streaming ML : Kafka + Spark MLlib\")\n    print(\"   ‚Ä¢ Edge ML : TensorFlow Lite, ONNX\")\n    \n    try:\n        print(\"\\n[√âtape 1/6] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = FeatureEngineerV14()\n        X_engineered = fe.fit_transform(X)\n        \n        preprocessor_dynamic = create_advanced_preprocessor(X_engineered)\n        \n        print(\"\\nüî¨ [Science] Split choisi : 80/20 stratifi√©\")\n        print(\"üìö [Alternatives] Autres strat√©gies de split :\")\n        print(\"   ‚Ä¢ Time-based split : Pour donn√©es temporelles\")\n        print(\"   ‚Ä¢ Group split : Par patient/r√©gion\")\n        print(\"   ‚Ä¢ Hold-out validation : 60/20/20 train/val/test\")\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/6] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/6] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/6] üìà √âvaluation compl√®te\")\n        print(\"üî¨ [Science] M√©triques choisies : Classification report + matrice confusion\")\n        print(\"üìö [Alternatives] Autres √©valuations :\")\n        print(\"   ‚Ä¢ Learning curves : Sur/sous-apprentissage\")\n        print(\"   ‚Ä¢ Calibration plots : Fiabilit√© probabilit√©s\")\n        print(\"   ‚Ä¢ Fairness metrics : √âquit√© entre groupes\")\n        \n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        print(\"\\n[√âtape 5/6] üß† Analyse de l'explicabilit√© du mod√®le\")\n        analyze_explicability(model, X_test, champion_name)\n        \n        print(\"\\n[√âtape 6/6] üìã Validation et documentation finale\")\n        print(\"üî¨ [Science] Documentation automatique g√©n√©r√©e\")\n        print(\"üìö [Standards] Autres validations industrielles :\")\n        print(\"   ‚Ä¢ A/B Testing : Performance en production\")\n        print(\"   ‚Ä¢ Shadow Mode : Validation sans impact\")\n        print(\"   ‚Ä¢ Model Monitoring : Drift detection\")\n        print(\"   ‚Ä¢ Compliance : GDPR, FDA, SOX\")\n        \n        return model, {\"champion\": champion_name, \"params\": best_params}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\")\n        print(\"üîß [Debug] Solutions de r√©cup√©ration :\")\n        print(\"   ‚Ä¢ R√©duire complexit√© mod√®les\")\n        print(\"   ‚Ä¢ Augmenter donn√©es d'entra√Ænement\")\n        print(\"   ‚Ä¢ Simplifier feature engineering\")\n        traceback.print_exc()\n\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V14.2 (AVEC ALTERNATIVES SCIENTIFIQUES)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        print(\"üìö [Alternatives] Autres sources de donn√©es :\")\n        print(\"   ‚Ä¢ Synthetic Data : GAN, VAE pour augmentation\")\n        print(\"   ‚Ä¢ Public Datasets : UCI, Kaggle, OpenML\")\n        print(\"   ‚Ä¢ Simulation : Agent-based modeling\")\n        print(\"   ‚Ä¢ Real-world : EHR, IoT sensors\")\n        \n        n_patients = 200; records_per_patient = 50; total_records = n_patients * records_per_patient\n        \n        patient_profiles = pd.DataFrame({'patient_id': [f'P{i:03d}' for i in range(n_patients)], 'is_frail_profile': np.repeat([0, 1], n_patients // 2)})\n        np.random.shuffle(patient_profiles['is_frail_profile'].values)\n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X_list = []\n        for _, patient in patient_profiles.iterrows():\n            records = pd.DataFrame({'patient_id': [patient['patient_id']] * records_per_patient})\n            records['age'] = patient['age']\n            records['mobility_score'] = np.random.normal(patient['base_mobility'], 0.5, records_per_patient).clip(0, 10)\n            records['grip_strength'] = np.random.normal(patient['base_grip'], 2, records_per_patient).clip(0, 50)\n            X_list.append(records)\n        X = pd.concat(X_list, ignore_index=True)\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# PIPELINE COMPLET V14.2 - AVEC EXPLICABILIT√â (XAI) ET ALTERNATIVES SCIENTIFIQUES\n# =============================================================================\n\nprint(\"\\nüîç [Expert] Initialisation du pipeline V14.2 : Ajout de l'explicabilit√© et des alternatives scientifiques.\")\nprint(\"üë∂ [Enfant] On va demander √† notre machine de nous expliquer comment elle r√©fl√©chit et quelles autres m√©thodes existent !\")\n\n# 1. üì¶ IMPORTATIONS ET CONFIGURATION\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\")\nprint(\"=\"*80)\n\nprint(\"üî¨ [Science] Configuration choisie : Seed fixe pour la reproductibilit√©\")\nprint(\"üìö [Alternatives] D'autres approches existent :\")\nprint(\"   ‚Ä¢ Seed al√©atoire : Pour tester la robustesse statistique\")\nprint(\"   ‚Ä¢ Cross-validation stratifi√©e r√©p√©t√©e : Pour validation plus rigoureuse\")\nprint(\"   ‚Ä¢ Bootstrap sampling : Pour estimation d'intervalles de confiance\")\n\n# --- Imports standards ---\nimport numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import joblib; import os; import json; import traceback; from functools import partial; import warnings\n# --- Imports pour le Machine Learning ---\nimport optuna; from sklearn.ensemble import *; from sklearn.linear_model import *; from sklearn.svm import *; from sklearn.neighbors import *; from sklearn.naive_bayes import *; from sklearn.tree import *; from sklearn.discriminant_analysis import *; from sklearn.neural_network import *; import lightgbm as lgb; import xgboost as xgb; from catboost import CatBoostClassifier\nfrom sklearn.model_selection import *; from sklearn.pipeline import *; from sklearn.compose import *; from sklearn.preprocessing import *; from sklearn.impute import *; from sklearn.base import *; from sklearn.metrics import *; from sklearn.calibration import *\n# --- IMPORT POUR L'EXPLICABILIT√â ---\nimport shap\n\n# --- Configuration Globale ---\nRANDOM_STATE = 42; np.random.seed(RANDOM_STATE); warnings.filterwarnings(\"ignore\", category=FutureWarning)\ntry: plt.style.use('seaborn-v0_8-whitegrid')\nexcept: plt.style.use('default')\npd.set_option('display.max_columns', None); pd.set_option('display.precision', 4)\nprint(\"‚úÖ Environnement configur√©.\")\n\n\n# 2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES INDUSTRIELLES\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\")\nprint(\"=\"*80)\n\nprint(\"üî¨ [Science] M√©triques choisies : Recall (sensibilit√©) + AUC pour d√©s√©quilibre de classes\")\nprint(\"üìö [Alternatives] Autres m√©triques selon le contexte :\")\nprint(\"   ‚Ä¢ F1-Score : √âquilibre pr√©cision/rappel (quand co√ªts √©gaux)\")\nprint(\"   ‚Ä¢ Precision-Recall AUC : Mieux que ROC-AUC pour classes tr√®s d√©s√©quilibr√©es\")\nprint(\"   ‚Ä¢ Matthews Correlation Coefficient : Robuste aux d√©s√©quilibres extr√™mes\")\nprint(\"   ‚Ä¢ Balanced Accuracy : (Sensibilit√© + Sp√©cificit√©)/2\")\nprint(\"   ‚Ä¢ Cohen's Kappa : Accord inter-√©valuateurs ajust√© par le hasard\")\n\nclass EvaluationConfig:\n    def __init__(self):\n        self.alert_thresholds = {'recall': 0.85, 'min_auc': 0.90}\n        print(f\"Seuils d'alerte configur√©s : {self.alert_thresholds}\")\n        print(\"‚ö†Ô∏è [Standard] Seuils industriels typiques :\")\n        print(\"   ‚Ä¢ M√©dical/S√©curit√© : Recall > 0.95, AUC > 0.95\")\n        print(\"   ‚Ä¢ Finance/Fraude : Precision > 0.80, AUC > 0.85\")\n        print(\"   ‚Ä¢ Marketing : F1 > 0.70, AUC > 0.75\")\n        \neval_config = EvaluationConfig()\n\n# 3. üõ†Ô∏è FEATURE ENGINEERING (V14)\n# =============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V14 - INTERACTIONS)\")\nprint(\"=\"*80)\n\nprint(\"üî¨ [Science] Approche choisie : Feature engineering manuel avec interactions m√©dicales\")\nprint(\"üìö [Alternatives] Autres techniques de cr√©ation de features :\")\nprint(\"   ‚Ä¢ Automated Feature Engineering : Featuretools, tsfresh\")\nprint(\"   ‚Ä¢ Polynomial Features : sklearn.preprocessing.PolynomialFeatures\")\nprint(\"   ‚Ä¢ Feature Crosses : TensorFlow Feature Columns\")\nprint(\"   ‚Ä¢ Deep Feature Synthesis : Relations automatiques entre variables\")\nprint(\"   ‚Ä¢ Domain-Specific Transforms : Log, sqrt, reciprocal selon distribution\")\n\nclass FeatureEngineerV14(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None): \n        print(\"üß¨ [Medical] Features d√©riv√©es bas√©es sur la litt√©rature clinique :\")\n        print(\"   ‚Ä¢ Pulse Pressure = SBP - DBP (rigidit√© art√©rielle)\")\n        print(\"   ‚Ä¢ Shock Index = HR/SBP (√©tat h√©modynamique)\")\n        return self\n        \n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # Features m√©dicales valid√©es\n        X_copy['pulse_pressure'] = (X_copy['systolic_bp'] - X_copy['diastolic_bp']).clip(lower=0)\n        X_copy['shock_index'] = X_copy['heart_rate'] / X_copy['systolic_bp']\n        \n        # Interactions sp√©cifiques\n        if 'bmi' in X.columns: \n            X_copy['bmi_x_age'] = X_copy['bmi'] * X_copy['age']\n            print(\"üí° [Alternative] Autres interactions BMI possibles :\")\n            print(\"   ‚Ä¢ BMI categorization : Normal/Overweight/Obese\")\n            print(\"   ‚Ä¢ Waist-Hip ratio si disponible\")\n            \n        if 'comorbidities_count' in X.columns: \n            X_copy['age_x_comorbidities'] = X_copy['age'] * X_copy['comorbidities_count']\n            print(\"üí° [Alternative] Autres approches comorbidit√©s :\")\n            print(\"   ‚Ä¢ Charlson Comorbidity Index : Score pond√©r√©\")\n            print(\"   ‚Ä¢ Elixhauser Comorbidity Index : 30 conditions sp√©cifiques\")\n            \n        return X_copy\n\nprint(\"‚úÖ Classe FeatureEngineerV14 (avec interactions) pr√™te.\")\n\n# 4. üéØ FONCTIONS D'√âVALUATION\n# =============================================================================\nprint(\"\\nüî¨ [Science] Validation crois√©e stratifi√©e : Pr√©serve la distribution des classes\")\nprint(\"üìö [Alternatives] Autres strat√©gies de validation :\")\nprint(\"   ‚Ä¢ Time Series Split : Pour donn√©es temporelles\")\nprint(\"   ‚Ä¢ Group K-Fold : Pour donn√©es group√©es (patients, r√©gions)\")\nprint(\"   ‚Ä¢ Leave-One-Out : Pour tr√®s petits datasets\")\nprint(\"   ‚Ä¢ Nested CV : Validation + s√©lection hyperparam√®tres simultan√©es\")\n\ndef evaluate_model(pipeline, X, y, cv_folds=5):\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {'f2': make_scorer(fbeta_score, beta=2, zero_division=0), \n               'recall': make_scorer(recall_score, zero_division=0), 'roc_auc': 'roc_auc'}\n    return pd.DataFrame(cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, error_score='raise'))\n\n# 5. OPTIMISATION HYPERPARAM√àTRES\n# =============================================================================\nprint(\"\\nüî¨ [Science] Optimisation Bay√©sienne avec Optuna (Tree-structured Parzen Estimator)\")\nprint(\"üìö [Alternatives] Autres m√©thodes d'optimisation :\")\nprint(\"   ‚Ä¢ Grid Search : Exhaustif mais co√ªteux\")\nprint(\"   ‚Ä¢ Random Search : Plus efficace que Grid pour hautes dimensions\")\nprint(\"   ‚Ä¢ Hyperopt : Alternative √† Optuna (TPE + Adaptive)\")\nprint(\"   ‚Ä¢ BOHB : Bandit-based + Bayesian Optimization\")\nprint(\"   ‚Ä¢ Population-based Training : Pour deep learning\")\n\ndef create_study(study_name):\n    os.makedirs(\"optuna_checkpoints\", exist_ok=True)\n    return optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                               study_name=study_name, storage=f\"sqlite:///optuna_checkpoints/{study_name}.db\", load_if_exists=True)\n\ndef _generic_objective(trial, X, y, preprocessor, model_constructor, get_params_func):\n    params = get_params_func(trial, y)\n    classifier_obj = model_constructor(**params)\n    pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    cv_results = evaluate_model(pipeline, X, y)\n    mean_auc = cv_results['test_roc_auc'].mean()\n    if mean_auc < eval_config.alert_thresholds['min_auc']: raise optuna.TrialPruned(f\"AUC ({mean_auc:.2f}) < seuil.\")\n    mean_recall = cv_results['test_recall'].mean()\n    if mean_recall < eval_config.alert_thresholds['recall']: raise optuna.TrialPruned(f\"Recall ({mean_recall:.2f}) < seuil.\")\n    return cv_results['test_f2'].mean()\n\n# 6. D√âFINITION DES MOD√àLES\n# =============================================================================\ndef get_model_definitions():\n    print(\"\\nüî¨ [Science] S√©lection d'algorithmes pour classification binaire d√©s√©quilibr√©e\")\n    print(\"üìö [Alternatives] Autres familles d'algorithmes :\")\n    print(\"   ‚Ä¢ Deep Learning : Neural Networks, Transformers\")\n    print(\"   ‚Ä¢ Kernel Methods : SVM avec diff√©rents noyaux\")\n    print(\"   ‚Ä¢ Bayesian Methods : Gaussian Processes, Bayesian Neural Nets\")\n    print(\"   ‚Ä¢ Ensemble Methods : Voting, Stacking, Blending\")\n    print(\"   ‚Ä¢ Meta-learning : MAML, Few-shot learning\")\n    \n    models = {}\n    \n    def get_lgbm_params(t, y):\n        scale_pos_weight = (y==0).sum() / ((y==1).sum() + 1e-6)\n        print(\"üå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\")\n        print(\"üí° [Alternatives LightGBM] :\")\n        print(\"   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\")\n        print(\"   ‚Ä¢ dart mode : Dropout regularization\")\n        print(\"   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\")\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), \n                'learning_rate': t.suggest_float('learning_rate', 1e-3, 0.2, log=True), \n                'num_leaves': t.suggest_int('num_leaves', 10, 50), \n                'scale_pos_weight': t.suggest_float('scale_pos_weight', 1.0, scale_pos_weight * 1.5), \n                'random_state': RANDOM_STATE, 'verbosity': -1}\n    models['LGBM'] = {'constructor': lgb.LGBMClassifier, 'params': get_lgbm_params}\n    \n    def get_rf_params(t, y):\n        print(\"üå≤ [RandomForest] Ensemble de d√©cision avec bootstrap\")\n        print(\"üí° [Alternatives RandomForest] :\")\n        print(\"   ‚Ä¢ ExtraTreesClassifier : Plus al√©atoire, moins overfitting\")\n        print(\"   ‚Ä¢ IsolationForest : Pour d√©tection d'anomalies\")\n        print(\"   ‚Ä¢ Balanced RandomForest : SMOTE int√©gr√©\")\n        return {'n_estimators': t.suggest_int('n_estimators', 100, 1000), \n                'max_depth': t.suggest_int('max_depth', 5, 20), \n                'class_weight': t.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']), \n                'random_state': RANDOM_STATE, 'n_jobs': -1}\n    models['RandomForest'] = {'constructor': RandomForestClassifier, 'params': get_rf_params}\n    \n    def get_logreg_params(t,y):\n        print(\"üìà [LogisticRegression] Classification lin√©aire avec r√©gularisation\")\n        print(\"üí° [Alternatives LogisticRegression] :\")\n        print(\"   ‚Ä¢ ElasticNet : Combinaison L1 + L2\")\n        print(\"   ‚Ä¢ SGDClassifier : Pour tr√®s gros datasets\")\n        print(\"   ‚Ä¢ Multinomial regression : Pour multi-classes\")\n        return {'C': t.suggest_float('C', 1e-2, 1e2, log=True), \n                'solver': 'liblinear', 'class_weight': 'balanced', 'random_state': RANDOM_STATE}\n    models['LogisticRegression'] = {'constructor': LogisticRegression, 'params': get_logreg_params}\n    \n    return models\n\n# 7. OPTIMISATION GLOBALE\n# =============================================================================\ndef optimize_models(X, y, n_trials_per_model, version, preprocessor):\n    print(\"\\nüî¨ [Science] Multi-objective optimization avec early stopping\")\n    print(\"üìö [Alternatives] Autres strat√©gies d'optimisation :\")\n    print(\"   ‚Ä¢ Multi-objective : NSGA-II, MOEA/D\")\n    print(\"   ‚Ä¢ AutoML : Auto-sklearn, TPOT, H2O AutoML\")\n    print(\"   ‚Ä¢ Neural Architecture Search : DARTS, ENAS\")\n    \n    model_definitions = get_model_definitions()\n    best_trials = {}\n    for name, definition in model_definitions.items():\n        print(f\"\\n--- {f'Manche du tournoi : {name}':^35} ---\")\n        study = create_study(f\"frailty_{name}_v{version}\")\n        objective_with_args = partial(_generic_objective, X=X, y=y, preprocessor=preprocessor, \n                                    model_constructor=definition['constructor'], get_params_func=definition['params'])\n        study.optimize(objective_with_args, n_trials=n_trials_per_model, show_progress_bar=True)\n        try:\n            best_trials[name] = study.best_trial\n            print(f\"‚úÖ Meilleur F2-score pour {name}: {study.best_trial.value:.4f}\")\n        except ValueError:\n            print(f\"‚ö†Ô∏è Tous les essais pour {name} ont √©t√© √©lagu√©s.\")\n    if not best_trials: raise ValueError(\"Aucun mod√®le n'a pu √™tre optimis√© avec succ√®s.\")\n    champion_name = max(best_trials, key=lambda name: best_trials.get(name).value if best_trials.get(name) else -1)\n    print(\"\\n\" + \"=\"*40); print(f\"üèÜ CHAMPION : {champion_name} | F2-Score: {best_trials[champion_name].value:.4f}\"); print(\"=\"*40)\n    return champion_name, best_trials[champion_name].params\n\n# 8. TRAINING FINAL\n# =============================================================================\ndef train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor):\n    print(\"\\nüî¨ [Science] Training final avec validation et sauvegarde versionn√©e\")\n    print(\"üìö [Alternatives] Autres approches de training :\")\n    print(\"   ‚Ä¢ Online Learning : Apprentissage incr√©mental\")\n    print(\"   ‚Ä¢ Transfer Learning : Pr√©-training + fine-tuning\")\n    print(\"   ‚Ä¢ Meta-Learning : Learning to learn\")\n    print(\"   ‚Ä¢ Federated Learning : Training distribu√©\")\n    \n    MODEL_DIR = f\"model_v{version}\"; os.makedirs(MODEL_DIR, exist_ok=True)\n    constructor = get_model_definitions()[champion_name]['constructor']\n    classifier_obj = constructor(**best_params)\n    final_pipeline = Pipeline([('fe', FeatureEngineerV14()), ('pre', preprocessor), ('clf', classifier_obj)])\n    final_pipeline.fit(X_train, y_train)\n    joblib.dump(final_pipeline, os.path.join(MODEL_DIR, 'model.joblib'))\n    metadata = {\"model_version\": version, \"champion_model\": champion_name, \"best_params\": best_params}\n    with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f: json.dump(metadata, f, indent=4, default=str)\n    print(f\"üíæ Mod√®le et m√©tadonn√©es sauvegard√©s dans : {MODEL_DIR}\")\n    return final_pipeline, MODEL_DIR\n\n# 9. ANALYSE D'EXPLICABILIT√â\n# =============================================================================\ndef analyze_explicability(pipeline, X_data, champion_name):\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß† ANALYSE DE L'EXPLICABILIT√â (XAI) AVEC SHAP\")\n    print(\"=\"*80)\n    \n    print(\"üî¨ [Science] SHAP : Valeurs de Shapley pour attribution causale\")\n    print(\"üìö [Alternatives XAI] Autres m√©thodes d'explicabilit√© :\")\n    print(\"   ‚Ä¢ LIME : Local Interpretable Model-agnostic Explanations\")\n    print(\"   ‚Ä¢ Permutation Importance : Impact par permutation\")\n    print(\"   ‚Ä¢ Anchors : R√®gles locales suffisantes\")\n    print(\"   ‚Ä¢ Counterfactuals : Sc√©narios 'what-if'\")\n    print(\"   ‚Ä¢ Integrated Gradients : Pour deep learning\")\n    print(\"   ‚Ä¢ DeepLIFT : Attribution pour r√©seaux profonds\")\n    \n    try:\n        # Extraire les √©tapes du pipeline\n        feature_engineer = pipeline.named_steps['fe']\n        preprocessor = pipeline.named_steps['pre']\n        model = pipeline.named_steps['clf']\n        \n        # Transformer les donn√©es comme le ferait le pipeline\n        X_engineered = feature_engineer.transform(X_data)\n        X_processed = preprocessor.transform(X_engineered)\n        \n        # R√©cup√©rer les noms de features finaux\n        feature_names = preprocessor.get_feature_names_out()\n        \n        # Cr√©er l'explainer SHAP selon le type de mod√®le\n        if champion_name in [\"LGBM\", \"RandomForest\", \"XGB\", \"CatBoost\", \"DecisionTree\", \"ExtraTrees\"]:\n            print(\"üå≥ [SHAP] Utilisation de TreeExplainer (exact pour mod√®les √† base d'arbres)\")\n            explainer = shap.TreeExplainer(model)\n        else:\n            print(\"üîç [SHAP] Utilisation de KernelExplainer (approximation par √©chantillonnage)\")\n            print(\"üí° [Alternative] LinearExplainer pour mod√®les lin√©aires (plus rapide)\")\n            # KernelExplainer est plus lent, on utilise un sous-√©chantillon\n            X_summary = shap.sample(X_processed, 100)\n            explainer = shap.KernelExplainer(model.predict_proba, X_summary)\n            \n        print(\"‚öôÔ∏è Calcul des valeurs SHAP... (peut prendre un moment)\")\n        shap_values = explainer.shap_values(X_processed)\n        \n        # Pour les classifieurs binaires, shap_values peut √™tre une liste de 2 arrays\n        # On s'int√©resse √† l'impact sur la classe positive (classe 1)\n        if isinstance(shap_values, list):\n            shap_values = shap_values[1]\n            \n        # Cr√©er le DataFrame pour la visualisation\n        X_processed_df = pd.DataFrame(X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed, columns=feature_names)\n\n        print(\"üìä Affichage du SHAP Summary Plot...\")\n        print(\"üí° [Visualisation] Autres types de plots SHAP :\")\n        print(\"   ‚Ä¢ Waterfall : Contribution individuelle\")\n        print(\"   ‚Ä¢ Force : Impact local d√©taill√©\")\n        print(\"   ‚Ä¢ Dependence : Relation feature-target\")\n        print(\"   ‚Ä¢ Decision : Arbre de d√©cision\")\n        \n        shap.summary_plot(shap_values, X_processed_df, plot_type=\"bar\", show=False)\n        plt.title(f\"Importance Globale des Features (SHAP) - Mod√®le {champion_name}\")\n        plt.show()\n        \n        shap.summary_plot(shap_values, X_processed_df, show=False)\n        plt.title(f\"Distribution de l'Impact des Features (SHAP) - Mod√®le {champion_name}\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"‚ùå Erreur lors de l'analyse SHAP : {e}\")\n        print(\"üîß [Debug] Solutions alternatives :\")\n        print(\"   ‚Ä¢ R√©duire taille √©chantillon pour KernelExplainer\")\n        print(\"   ‚Ä¢ Utiliser explainer sp√©cifique au mod√®le\")\n        print(\"   ‚Ä¢ Fallback vers permutation importance\")\n        traceback.print_exc()\n\n# 10. PREPROCESSING AVANC√â\n# =============================================================================\ndef create_advanced_preprocessor(X_engineered):\n    print(\"\\nüî¨ [Science] Preprocessing choisi : StandardScaler + OneHotEncoder\")\n    print(\"üìö [Alternatives] Autres techniques de preprocessing :\")\n    print(\"   ‚Ä¢ Normalization : MinMaxScaler, RobustScaler, QuantileTransformer\")\n    print(\"   ‚Ä¢ Encoding : Target Encoding, Binary Encoding, Hash Encoding\")\n    print(\"   ‚Ä¢ Imputation : KNN, Iterative, Forward/Backward Fill\")\n    print(\"   ‚Ä¢ Feature Selection : SelectKBest, RFE, L1-based\")\n    print(\"   ‚Ä¢ Dimensionality Reduction : PCA, t-SNE, UMAP\")\n    \n    numeric_cols = [c for c in X_engineered.columns if pd.api.types.is_numeric_dtype(X_engineered[c]) and c not in ['patient_id', 'timestamp']]\n    categorical_cols = [c for c in X_engineered.select_dtypes(include=['object', 'category']).columns if c not in ['patient_id', 'timestamp']]\n    \n    print(f\"üìä Features num√©riques d√©tect√©es : {len(numeric_cols)}\")\n    print(f\"üìä Features cat√©gorielles d√©tect√©es : {len(categorical_cols)}\")\n    \n    return ColumnTransformer(\n        transformers=[\n            ('num', Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy='median')), \n                ('scaler', StandardScaler())\n            ]), numeric_cols),\n            ('cat', Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy='most_frequent')), \n                ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n            ]), categorical_cols)\n        ], \n        remainder='drop'\n    )\n\n# 11. üèÅ PIPELINE COMPLET ORCHESTR√â\n# =============================================================================\ndef run_full_pipeline(X, y, n_trials_per_model=10, version=\"1.0.0\"):\n    print(\"\\nüî¨ [Science] Architecture choisie : Pipeline end-to-end avec validation\")\n    print(\"üìö [Alternatives] Autres architectures ML :\")\n    print(\"   ‚Ä¢ MLOps : Kubeflow, MLflow, Airflow\")\n    print(\"   ‚Ä¢ AutoML : Auto-sklearn, TPOT, H2O\")\n    print(\"   ‚Ä¢ Streaming ML : Kafka + Spark MLlib\")\n    print(\"   ‚Ä¢ Edge ML : TensorFlow Lite, ONNX\")\n    \n    try:\n        print(\"\\n[√âtape 1/6] üõ°Ô∏è Configuration du pipeline dynamique\")\n        X = pd.DataFrame(X).copy(); y = pd.Series(y).copy()\n        if y.nunique() < 2: raise ValueError(\"La cible ne contient qu'une seule classe.\")\n        \n        fe = FeatureEngineerV14()\n        X_engineered = fe.fit_transform(X)\n        \n        preprocessor_dynamic = create_advanced_preprocessor(X_engineered)\n        \n        print(\"\\nüî¨ [Science] Split choisi : 80/20 stratifi√©\")\n        print(\"üìö [Alternatives] Autres strat√©gies de split :\")\n        print(\"   ‚Ä¢ Time-based split : Pour donn√©es temporelles\")\n        print(\"   ‚Ä¢ Group split : Par patient/r√©gion\")\n        print(\"   ‚Ä¢ Hold-out validation : 60/20/20 train/val/test\")\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n        print(f\"‚úÖ Donn√©es s√©par√©es (train: {len(X_train)}, test: {len(X_test)})\")\n\n        print(\"\\n[√âtape 2/6] üöÄ Optimisation et s√©lection du meilleur mod√®le\")\n        champion_name, best_params = optimize_models(X_train, y_train, n_trials_per_model, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 3/6] üéì Entra√Ænement du mod√®le final\")\n        model, model_dir = train_final_model(X_train, y_train, champion_name, best_params, version, preprocessor_dynamic)\n\n        print(\"\\n[√âtape 4/6] üìà √âvaluation compl√®te\")\n        print(\"üî¨ [Science] M√©triques choisies : Classification report + matrice confusion\")\n        print(\"üìö [Alternatives] Autres √©valuations :\")\n        print(\"   ‚Ä¢ Learning curves : Sur/sous-apprentissage\")\n        print(\"   ‚Ä¢ Calibration plots : Fiabilit√© probabilit√©s\")\n        print(\"   ‚Ä¢ Fairness metrics : √âquit√© entre groupes\")\n        \n        y_probs = model.predict_proba(X_test)[:, 1]; y_pred = model.predict(X_test)\n        print(\"\\nüìä Rapport de classification :\"); print(classification_report(y_test, y_pred, target_names=[\"Non fragile\", \"Fragile\"]))\n        \n        print(\"\\n[√âtape 5/6] üß† Analyse de l'explicabilit√© du mod√®le\")\n        analyze_explicability(model, X_test, champion_name)\n        \n        print(\"\\n[√âtape 6/6] üìã Validation et documentation finale\")\n        print(\"üî¨ [Science] Documentation automatique g√©n√©r√©e\")\n        print(\"üìö [Standards] Autres validations industrielles :\")\n        print(\"   ‚Ä¢ A/B Testing : Performance en production\")\n        print(\"   ‚Ä¢ Shadow Mode : Validation sans impact\")\n        print(\"   ‚Ä¢ Model Monitoring : Drift detection\")\n        print(\"   ‚Ä¢ Compliance : GDPR, FDA, SOX\")\n        \n        return model, {\"champion\": champion_name, \"params\": best_params}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå‚ùå ERREUR CRITIQUE DANS LE PIPELINE ‚ùå‚ùå: {e}\")\n        print(\"üîß [Debug] Solutions de r√©cup√©ration :\")\n        print(\"   ‚Ä¢ R√©duire complexit√© mod√®les\")\n        print(\"   ‚Ä¢ Augmenter donn√©es d'entra√Ænement\")\n        print(\"   ‚Ä¢ Simplifier feature engineering\")\n        traceback.print_exc()\n\n\n# =============================================================================\n# üöÄ EX√âCUTION DU SCRIPT\n# =============================================================================\nif __name__ == \"__main__\":\n    try:\n        print(\"\\n\" + \"=\"*80); print(\"üöÄ D√âMARRAGE DE L'EX√âCUTION V14.2 (AVEC ALTERNATIVES SCIENTIFIQUES)\"); print(\"=\"*80)\n        \n        print(\"\\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\")\n        print(\"üìö [Alternatives] Autres sources de donn√©es :\")\n        print(\"   ‚Ä¢ Synthetic Data : GAN, VAE pour augmentation\")\n        print(\"   ‚Ä¢ Public Datasets : UCI, Kaggle, OpenML\")\n        print(\"   ‚Ä¢ Simulation : Agent-based modeling\")\n        print(\"   ‚Ä¢ Real-world : EHR, IoT sensors\")\n        \n        n_patients = 200; records_per_patient = 50; total_records = n_patients * records_per_patient\n        \n        patient_profiles = pd.DataFrame({'patient_id': [f'P{i:03d}' for i in range(n_patients)], 'is_frail_profile': np.repeat([0, 1], n_patients // 2)})\n        np.random.shuffle(patient_profiles['is_frail_profile'].values)\n        patient_profiles['age'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.randint(65, 78, n_patients), np.random.randint(78, 95, n_patients))\n        patient_profiles['base_mobility'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.uniform(7, 10, n_patients), np.random.uniform(1, 5, n_patients))\n        patient_profiles['base_grip'] = np.where(patient_profiles['is_frail_profile'] == 0, np.random.normal(35, 5, n_patients), np.random.normal(20, 5, n_patients))\n\n        X_list = []\n        for _, patient in patient_profiles.iterrows():\n            records = pd.DataFrame({'patient_id': [patient['patient_id']] * records_per_patient})\n            records['age'] = patient['age']\n            records['mobility_score'] = np.random.normal(patient['base_mobility'], 0.5, records_per_patient).clip(0, 10)\n            records['grip_strength'] = np.random.normal(patient['base_grip'], 2, records_per_patient).clip(0, 50)\n            X_list.append(records)\n        X = pd.concat(X_list, ignore_index=True)\n        \n        X['systolic_bp'] = np.random.normal(130, 15, total_records)\n        X['diastolic_bp'] = X['systolic_bp'] - np.random.normal(50, 5, total_records)\n        X['heart_rate'] = np.random.normal(75, 10, total_records)\n        X['gender'] = np.random.choice(['M', 'F'], total_records)\n        X['bmi'] = np.random.normal(26, 4, total_records)\n        X['comorbidities_count'] = np.random.poisson(np.where(pd.merge(X[['patient_id']], patient_profiles, on='patient_id')['is_frail_profile']==1, 3, 1), total_records)\n        \n        print(\"üî¨ [Science] G√©n√©ration de donn√©es synth√©tiques avec distributions r√©alistes\")\n        print(\"üìö [Alternatives] Autres approches de g√©n√©ration :\")\n        print(\"   ‚Ä¢ Copulas : Pr√©servation des corr√©lations complexes\")\n        print(\"   ‚Ä¢ Bayesian Networks : Relations causales explicites\")\n        print(\"   ‚Ä¢ Markov Models : S√©quences temporelles\")\n        print(\"   ‚Ä¢ Monte Carlo : Sampling from prior distributions\")\n        \n        y = pd.merge(X[['patient_id']], patient_profiles[['patient_id', 'is_frail_profile']], on='patient_id')['is_frail_profile']\n        \n        print(f\"üìä Donn√©es g√©n√©r√©es : {len(X)} enregistrements pour {X['patient_id'].nunique()} patients.\")\n        print(f\"üéØ Distribution de la cible :\\n{y.value_counts(normalize=True).round(2)}\")\n        \n        print(\"üî¨ [Science] V√©rification de la qualit√© des donn√©es\")\n        print(\"üìö [Alternatives] Autres validations donn√©es :\")\n        print(\"   ‚Ä¢ Data Profiling : Great Expectations, Pandas Profiling\")\n        print(\"   ‚Ä¢ Outlier Detection : IQR, Z-score, Isolation Forest\")\n        print(\"   ‚Ä¢ Drift Detection : KS-test, Population Stability Index\")\n        print(\"   ‚Ä¢ Bias Detection : Fairness metrics par sous-groupes\")\n        \n        # V√©rifications de qualit√©\n        print(f\"‚úÖ Valeurs manquantes : {X.isnull().sum().sum()}\")\n        print(f\"‚úÖ Duplicats : {X.duplicated().sum()}\")\n        print(f\"‚úÖ Corr√©lation target-features : visible dans le design\")\n        \n        print(\"\\nüöÄ Lancement du pipeline complet...\")\n        model, report = run_full_pipeline(X.drop('patient_id', axis=1), y.values, n_trials_per_model=10, version=\"14.2-alternatives\")\n\n        if model and report:\n            print(\"\\nüéâüéâüéâ PIPELINE V14.2 AVEC ALTERNATIVES SCIENTIFIQUES EX√âCUT√â AVEC SUCC√àS! üéâüéâüéâ\")\n            print(\"\\nüìä R√âSUM√â EX√âCUTIF :\")\n            print(f\"   ‚Ä¢ Mod√®le champion : {report['champion']}\")\n            print(f\"   ‚Ä¢ Architecture : Pipeline end-to-end avec XAI\")\n            print(f\"   ‚Ä¢ Validation : Cross-validation stratifi√©e\")\n            print(f\"   ‚Ä¢ Explicabilit√© : SHAP values\")\n            \n            print(\"\\nüî¨ [Science] Recommandations pour la production :\")\n            print(\"   ‚Ä¢ Monitoring : Drift detection + performance tracking\")\n            print(\"   ‚Ä¢ Retraining : Scheduler automatique\")\n            print(\"   ‚Ä¢ A/B Testing : Validation impact business\")\n            print(\"   ‚Ä¢ Rollback : Strat√©gie de retour version pr√©c√©dente\")\n            \n            print(\"\\nüìö [Standards] Conformit√© industrielle :\")\n            print(\"   ‚Ä¢ Versioning : Mod√®les + donn√©es + code\")\n            print(\"   ‚Ä¢ Documentation : Automatique + m√©tadonn√©es\")\n            print(\"   ‚Ä¢ Auditabilit√© : Logs + reproductibilit√©\")\n            print(\"   ‚Ä¢ Governance : Approbation + risk management\")\n            \n        else:\n            print(\"\\nüíî Le pipeline V14.2 a √©chou√© ou n'a pas produit de mod√®le valide.\")\n            print(\"üîß [Troubleshooting] Actions recommand√©es :\")\n            print(\"   ‚Ä¢ V√©rifier distribution des classes\")\n            print(\"   ‚Ä¢ R√©duire complexit√© hyperparam√®tres\")\n            print(\"   ‚Ä¢ Augmenter donn√©es d'entra√Ænement\")\n            print(\"   ‚Ä¢ Simplifier feature engineering\")\n\n    except Exception as e:\n        print(f\"\\nüí• Le pipeline a √©chou√© avec erreur : {e}\")\n        print(\"üîß [Recovery] Strat√©gies de r√©cup√©ration :\")\n        print(\"   ‚Ä¢ Fallback vers mod√®le simple (LogisticRegression)\")\n        print(\"   ‚Ä¢ R√©duction dimensionnalit√© (PCA)\")\n        print(\"   ‚Ä¢ Sampling strat√©gique (SMOTE/ADASYN)\")\n        print(\"   ‚Ä¢ Validation manuelle des donn√©es\")\n        traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìö R√âF√âRENCES SCIENTIFIQUES ET INDUSTRIELLES\")\nprint(\"=\"*80)\nprint(\"üî¨ Feature Engineering :\")\nprint(\"   ‚Ä¢ Guyon & Elisseeff (2003) - Feature extraction\")\nprint(\"   ‚Ä¢ Kira & Rendell (1992) - RELIEF algorithm\")\nprint(\"üî¨ Hyperparameter Optimization :\")\nprint(\"   ‚Ä¢ Bergstra & Bengio (2012) - Random Search\")\nprint(\"   ‚Ä¢ Akiba et al. (2019) - Optuna framework\")\nprint(\"üî¨ Explicabilit√© :\")\nprint(\"   ‚Ä¢ Lundberg & Lee (2017) - SHAP values\")\nprint(\"   ‚Ä¢ Ribeiro et al. (2016) - LIME\")\nprint(\"üî¨ Class Imbalance :\")\nprint(\"   ‚Ä¢ Chawla et al. (2002) - SMOTE\")\nprint(\"   ‚Ä¢ He & Garcia (2009) - Learning from imbalanced data\")\nprint(\"üî¨ Model Validation :\")\nprint(\"   ‚Ä¢ Kohavi (1995) - Cross-validation\")\nprint(\"   ‚Ä¢ Dietterich (1998) - Approximate statistical tests\")\nprint(\"\\nüíº Standards Industriels :\")\nprint(\"   ‚Ä¢ ISO/IEC 23053:2022 - Framework for AI risk management\")\nprint(\"   ‚Ä¢ FDA Guidance - Software as Medical Device\")\nprint(\"   ‚Ä¢ EU GDPR - Right to explanation\")\nprint(\"   ‚Ä¢ IEEE 2857 - Privacy engineering\")\n\nprint(f\"\\nüèÅ Pipeline V14.2 termin√© - {pd.Timestamp.now()}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:49:40.683563Z","iopub.execute_input":"2025-06-26T07:49:40.683928Z"}},"outputs":[{"name":"stdout","text":"\nüîç [Expert] Initialisation du pipeline V14.2 : Ajout de l'explicabilit√© et des alternatives scientifiques.\nüë∂ [Enfant] On va demander √† notre machine de nous expliquer comment elle r√©fl√©chit et quelles autres m√©thodes existent !\n\n================================================================================\n1. üì¶ CONFIGURATION DE L'ENVIRONNEMENT\n================================================================================\nüî¨ [Science] Configuration choisie : Seed fixe pour la reproductibilit√©\nüìö [Alternatives] D'autres approches existent :\n   ‚Ä¢ Seed al√©atoire : Pour tester la robustesse statistique\n   ‚Ä¢ Cross-validation stratifi√©e r√©p√©t√©e : Pour validation plus rigoureuse\n   ‚Ä¢ Bootstrap sampling : Pour estimation d'intervalles de confiance\n‚úÖ Environnement configur√©.\n\n================================================================================\n2. üèóÔ∏è CONFIGURATION DES M√âTRIQUES CLINIQUES\n================================================================================\nüî¨ [Science] M√©triques choisies : Recall (sensibilit√©) + AUC pour d√©s√©quilibre de classes\nüìö [Alternatives] Autres m√©triques selon le contexte :\n   ‚Ä¢ F1-Score : √âquilibre pr√©cision/rappel (quand co√ªts √©gaux)\n   ‚Ä¢ Precision-Recall AUC : Mieux que ROC-AUC pour classes tr√®s d√©s√©quilibr√©es\n   ‚Ä¢ Matthews Correlation Coefficient : Robuste aux d√©s√©quilibres extr√™mes\n   ‚Ä¢ Balanced Accuracy : (Sensibilit√© + Sp√©cificit√©)/2\n   ‚Ä¢ Cohen's Kappa : Accord inter-√©valuateurs ajust√© par le hasard\nSeuils d'alerte configur√©s : {'recall': 0.85, 'min_auc': 0.9}\n‚ö†Ô∏è [Standard] Seuils industriels typiques :\n   ‚Ä¢ M√©dical/S√©curit√© : Recall > 0.95, AUC > 0.95\n   ‚Ä¢ Finance/Fraude : Precision > 0.80, AUC > 0.85\n   ‚Ä¢ Marketing : F1 > 0.70, AUC > 0.75\n\n================================================================================\n3. üõ†Ô∏è ING√âNIERIE DES CARACT√âRISTIQUES (V14 - INTERACTIONS)\n================================================================================\nüî¨ [Science] Approche choisie : Feature engineering manuel avec interactions m√©dicales\nüìö [Alternatives] Autres techniques de cr√©ation de features :\n   ‚Ä¢ Automated Feature Engineering : Featuretools, tsfresh\n   ‚Ä¢ Polynomial Features : sklearn.preprocessing.PolynomialFeatures\n   ‚Ä¢ Feature Crosses : TensorFlow Feature Columns\n   ‚Ä¢ Deep Feature Synthesis : Relations automatiques entre variables\n   ‚Ä¢ Domain-Specific Transforms : Log, sqrt, reciprocal selon distribution\n‚úÖ Classe FeatureEngineerV14 (avec interactions) pr√™te.\n\nüî¨ [Science] Validation crois√©e stratifi√©e : Pr√©serve la distribution des classes\nüìö [Alternatives] Autres strat√©gies de validation :\n   ‚Ä¢ Time Series Split : Pour donn√©es temporelles\n   ‚Ä¢ Group K-Fold : Pour donn√©es group√©es (patients, r√©gions)\n   ‚Ä¢ Leave-One-Out : Pour tr√®s petits datasets\n   ‚Ä¢ Nested CV : Validation + s√©lection hyperparam√®tres simultan√©es\n\nüî¨ [Science] Optimisation Bay√©sienne avec Optuna (Tree-structured Parzen Estimator)\nüìö [Alternatives] Autres m√©thodes d'optimisation :\n   ‚Ä¢ Grid Search : Exhaustif mais co√ªteux\n   ‚Ä¢ Random Search : Plus efficace que Grid pour hautes dimensions\n   ‚Ä¢ Hyperopt : Alternative √† Optuna (TPE + Adaptive)\n   ‚Ä¢ BOHB : Bandit-based + Bayesian Optimization\n   ‚Ä¢ Population-based Training : Pour deep learning\n\n================================================================================\nüöÄ D√âMARRAGE DE L'EX√âCUTION V14.2 (AVEC ALTERNATIVES SCIENTIFIQUES)\n================================================================================\n\nüî¨ Cr√©ation de donn√©es avec des profils de patients et un signal clairs...\nüìö [Alternatives] Autres sources de donn√©es :\n   ‚Ä¢ Synthetic Data : GAN, VAE pour augmentation\n   ‚Ä¢ Public Datasets : UCI, Kaggle, OpenML\n   ‚Ä¢ Simulation : Agent-based modeling\n   ‚Ä¢ Real-world : EHR, IoT sensors\nüî¨ [Science] G√©n√©ration de donn√©es synth√©tiques avec distributions r√©alistes\nüìö [Alternatives] Autres approches de g√©n√©ration :\n   ‚Ä¢ Copulas : Pr√©servation des corr√©lations complexes\n   ‚Ä¢ Bayesian Networks : Relations causales explicites\n   ‚Ä¢ Markov Models : S√©quences temporelles\n   ‚Ä¢ Monte Carlo : Sampling from prior distributions\nüìä Donn√©es g√©n√©r√©es : 10000 enregistrements pour 200 patients.\nüéØ Distribution de la cible :\nis_frail_profile\n0    0.5\n1    0.5\nName: proportion, dtype: float64\nüî¨ [Science] V√©rification de la qualit√© des donn√©es\nüìö [Alternatives] Autres validations donn√©es :\n   ‚Ä¢ Data Profiling : Great Expectations, Pandas Profiling\n   ‚Ä¢ Outlier Detection : IQR, Z-score, Isolation Forest\n   ‚Ä¢ Drift Detection : KS-test, Population Stability Index\n   ‚Ä¢ Bias Detection : Fairness metrics par sous-groupes\n‚úÖ Valeurs manquantes : 0\n‚úÖ Duplicats : 0\n‚úÖ Corr√©lation target-features : visible dans le design\n\nüöÄ Lancement du pipeline complet...\n\nüî¨ [Science] Architecture choisie : Pipeline end-to-end avec validation\nüìö [Alternatives] Autres architectures ML :\n   ‚Ä¢ MLOps : Kubeflow, MLflow, Airflow\n   ‚Ä¢ AutoML : Auto-sklearn, TPOT, H2O\n   ‚Ä¢ Streaming ML : Kafka + Spark MLlib\n   ‚Ä¢ Edge ML : TensorFlow Lite, ONNX\n\n[√âtape 1/6] üõ°Ô∏è Configuration du pipeline dynamique\nüß¨ [Medical] Features d√©riv√©es bas√©es sur la litt√©rature clinique :\n   ‚Ä¢ Pulse Pressure = SBP - DBP (rigidit√© art√©rielle)\n   ‚Ä¢ Shock Index = HR/SBP (√©tat h√©modynamique)\nüí° [Alternative] Autres interactions BMI possibles :\n   ‚Ä¢ BMI categorization : Normal/Overweight/Obese\n   ‚Ä¢ Waist-Hip ratio si disponible\nüí° [Alternative] Autres approches comorbidit√©s :\n   ‚Ä¢ Charlson Comorbidity Index : Score pond√©r√©\n   ‚Ä¢ Elixhauser Comorbidity Index : 30 conditions sp√©cifiques\n\nüî¨ [Science] Preprocessing choisi : StandardScaler + OneHotEncoder\nüìö [Alternatives] Autres techniques de preprocessing :\n   ‚Ä¢ Normalization : MinMaxScaler, RobustScaler, QuantileTransformer\n   ‚Ä¢ Encoding : Target Encoding, Binary Encoding, Hash Encoding\n   ‚Ä¢ Imputation : KNN, Iterative, Forward/Backward Fill\n   ‚Ä¢ Feature Selection : SelectKBest, RFE, L1-based\n   ‚Ä¢ Dimensionality Reduction : PCA, t-SNE, UMAP\nüìä Features num√©riques d√©tect√©es : 12\nüìä Features cat√©gorielles d√©tect√©es : 1\n\nüî¨ [Science] Split choisi : 80/20 stratifi√©\nüìö [Alternatives] Autres strat√©gies de split :\n   ‚Ä¢ Time-based split : Pour donn√©es temporelles\n   ‚Ä¢ Group split : Par patient/r√©gion\n   ‚Ä¢ Hold-out validation : 60/20/20 train/val/test\n‚úÖ Donn√©es s√©par√©es (train: 8000, test: 2000)\n\n[√âtape 2/6] üöÄ Optimisation et s√©lection du meilleur mod√®le\n\nüî¨ [Science] Multi-objective optimization avec early stopping\nüìö [Alternatives] Autres strat√©gies d'optimisation :\n   ‚Ä¢ Multi-objective : NSGA-II, MOEA/D\n   ‚Ä¢ AutoML : Auto-sklearn, TPOT, H2O AutoML\n   ‚Ä¢ Neural Architecture Search : DARTS, ENAS\n\nüî¨ [Science] S√©lection d'algorithmes pour classification binaire d√©s√©quilibr√©e\nüìö [Alternatives] Autres familles d'algorithmes :\n   ‚Ä¢ Deep Learning : Neural Networks, Transformers\n   ‚Ä¢ Kernel Methods : SVM avec diff√©rents noyaux\n   ‚Ä¢ Bayesian Methods : Gaussian Processes, Bayesian Neural Nets\n   ‚Ä¢ Ensemble Methods : Voting, Stacking, Blending\n   ‚Ä¢ Meta-learning : MAML, Few-shot learning\n\n---      Manche du tournoi : LGBM       ---\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-26 07:50:00,724] A new study created in RDB with name: frailty_LGBM_v14.2-alternatives\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89c5c05587f4643ba6584c5c8922ce2"}},"metadata":{}},{"name":"stdout","text":"üå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:07,455] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 437, 'learning_rate': 0.1540359659501924, 'num_leaves': 40, 'scale_pos_weight': 1.2993292418740214}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:08,904] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 240, 'learning_rate': 0.002285325525633921, 'num_leaves': 12, 'scale_pos_weight': 1.4330880725626516}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:12,376] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 641, 'learning_rate': 0.04258888210290081, 'num_leaves': 10, 'scale_pos_weight': 1.484954925717281}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:15,947] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 850, 'learning_rate': 0.0030803400529839683, 'num_leaves': 17, 'scale_pos_weight': 1.0917022548579403}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:17,881] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 374, 'learning_rate': 0.016124278458562614, 'num_leaves': 27, 'scale_pos_weight': 1.14561456998981}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:20,478] Trial 5 finished with value: 1.0 and parameters: {'n_estimators': 651, 'learning_rate': 0.002094013887393744, 'num_leaves': 21, 'scale_pos_weight': 1.1831809215094602}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:22,043] Trial 6 finished with value: 1.0 and parameters: {'n_estimators': 510, 'learning_rate': 0.06407866261851015, 'num_leaves': 18, 'scale_pos_weight': 1.257117219013968}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:24,942] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 633, 'learning_rate': 0.0012790390175145834, 'num_leaves': 34, 'scale_pos_weight': 1.0852620617796993}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:26,038] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 158, 'learning_rate': 0.15255065745117383, 'num_leaves': 49, 'scale_pos_weight': 1.4041986737550816}. Best is trial 0 with value: 1.0.\nüå≥ [LightGBM] Gradient Boosting optimis√© avec gestion d√©s√©quilibre\nüí° [Alternatives LightGBM] :\n   ‚Ä¢ focal_loss : Pour d√©s√©quilibres extr√™mes\n   ‚Ä¢ dart mode : Dropout regularization\n   ‚Ä¢ lambda_l1/l2 : Regularization Lasso/Ridge\n[I 2025-06-26 07:50:28,266] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 374, 'learning_rate': 0.001677824238407793, 'num_leaves': 38, 'scale_pos_weight': 1.2200762467047435}. Best is trial 0 with value: 1.0.\n‚úÖ Meilleur F2-score pour LGBM: 1.0000\n\n---  Manche du tournoi : RandomForest   ---\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-26 07:50:28,604] A new study created in RDB with name: frailty_RandomForest_v14.2-alternatives\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03975b482340410984cd32021ce7eb87"}},"metadata":{}},{"name":"stdout","text":"üå≤ [RandomForest] Ensemble de d√©cision avec bootstrap\nüí° [Alternatives RandomForest] :\n   ‚Ä¢ ExtraTreesClassifier : Plus al√©atoire, moins overfitting\n   ‚Ä¢ IsolationForest : Pour d√©tection d'anomalies\n   ‚Ä¢ Balanced RandomForest : SMOTE int√©gr√©\n[I 2025-06-26 07:50:34,855] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 437, 'max_depth': 20, 'class_weight': 'balanced'}. Best is trial 0 with value: 1.0.\nüå≤ [RandomForest] Ensemble de d√©cision avec bootstrap\nüí° [Alternatives RandomForest] :\n   ‚Ä¢ ExtraTreesClassifier : Plus al√©atoire, moins overfitting\n   ‚Ä¢ IsolationForest : Pour d√©tection d'anomalies\n   ‚Ä¢ Balanced RandomForest : SMOTE int√©gr√©\n[I 2025-06-26 07:50:39,843] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 240, 'max_depth': 7, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 1.0.\nüå≤ [RandomForest] Ensemble de d√©cision avec bootstrap\nüí° [Alternatives RandomForest] :\n   ‚Ä¢ ExtraTreesClassifier : Plus al√©atoire, moins overfitting\n   ‚Ä¢ IsolationForest : Pour d√©tection d'anomalies\n   ‚Ä¢ Balanced RandomForest : SMOTE int√©gr√©\n","output_type":"stream"}],"execution_count":null}]}