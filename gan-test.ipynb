{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:06:33.456635Z","iopub.execute_input":"2025-06-09T12:06:33.457160Z","iopub.status.idle":"2025-06-09T12:06:33.848121Z","shell.execute_reply.started":"2025-06-09T12:06:33.457137Z","shell.execute_reply":"2025-06-09T12:06:33.847346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# ==================================\n# SETUP: LIBRARIES AND HYPERPARAMETERS\n# ==================================\n#\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import make_grid, save_image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- 1. Hyperparameters ---\n# These are the key settings for our GAN.\n# We can tune these to improve results.\n\n# Training parameters\nepochs = 25\nlr = 0.0002 # Learning rate, a key parameter for GAN stability\nbatch_size = 128\n\n# Model parameters\nimage_size = 64  # We'll resize MNIST images to this size\nimage_channels = 1 # MNIST is grayscale, so 1 channel\nlatent_dim = 100 # Dimension of the random noise vector (z)\n\n# --- 2. Device Setup ---\n# We want to use the GPU if it's available, as it's much faster.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:58:01.072652Z","iopub.execute_input":"2025-06-09T12:58:01.073142Z","iopub.status.idle":"2025-06-09T12:58:01.078319Z","shell.execute_reply.started":"2025-06-09T12:58:01.073122Z","shell.execute_reply":"2025-06-09T12:58:01.077578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# ==================================\n# STEP 1: DATA PREPARATION\n# ==================================\n#\n# We define a series of transformations to apply to the images.\ntransform = transforms.Compose([\n    transforms.Resize(image_size),      # Resize to our desired image size\n    transforms.ToTensor(),              # Convert image to a PyTorch Tensor (values 0-1)\n    transforms.Normalize(\n        [0.5], [0.5]                    # Normalize to [-1, 1] range\n    )\n])\n\n# Download the MNIST dataset\nmnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# Create a DataLoader to handle batching\ndataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n\n# Let's visualize a batch of real images to see what we're working with\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"A Batch of Real MNIST Images\")\n# We need to un-normalize to display them correctly\ngrid = make_grid(real_batch[0][:64] * 0.5 + 0.5, padding=2, normalize=True)\nplt.imshow(np.transpose(grid.cpu(), (1, 2, 0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:58:06.692937Z","iopub.execute_input":"2025-06-09T12:58:06.693165Z","iopub.status.idle":"2025-06-09T12:58:06.990641Z","shell.execute_reply.started":"2025-06-09T12:58:06.693151Z","shell.execute_reply":"2025-06-09T12:58:06.989923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# ==================================\n# STEP 2: THE GENERATOR\n# ==================================\n#\n# The Generator's job is to create realistic images from random noise.\n# It uses ConvTranspose2d layers to \"upsample\" from a latent vector to a full image.\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, channels, img_size):\n        super(Generator, self).__init__()\n        self.img_size = img_size\n        self.channels = channels\n\n        # We build the network layer by layer.\n        # It's a series of blocks that progressively increase the image size.\n        self.model = nn.Sequential(\n            # Input is Z, going into a convolution\n            # Block 1: latent_dim -> 512 x 4x4\n            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n\n            # Block 2: 512 x 4x4 -> 256 x 8x8\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n\n            # Block 3: 256 x 8x8 -> 128 x 16x16\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n\n            # Block 4: 128 x 16x16 -> 64 x 32x32\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n\n            # Block 5: 64 x 32x32 -> 1 x 64x64\n            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n            nn.Tanh() # Tanh activation to scale output to [-1, 1]\n        )\n\n    def forward(self, z):\n        # z is the input noise vector\n        img = self.model(z)\n        return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:58:12.229609Z","iopub.execute_input":"2025-06-09T12:58:12.229988Z","iopub.status.idle":"2025-06-09T12:58:12.237025Z","shell.execute_reply.started":"2025-06-09T12:58:12.229962Z","shell.execute_reply":"2025-06-09T12:58:12.236362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# ==================================\n# STEP 3: THE DISCRIMINATOR\n# ==================================\n#\n# The Discriminator's job is to classify images as real or fake.\n# It's a standard CNN.\n\nclass Discriminator(nn.Module):\n    def __init__(self, channels, img_size):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            # Input: 1 x 64x64\n            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # State: 64 x 32x32\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # State: 128 x 16x16\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # State: 256 x 8x8\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # State: 512 x 4x4\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid() # Sigmoid to output a probability (0=Fake, 1=Real)\n        )\n\n    def forward(self, img):\n        # The input is an image\n        validity = self.model(img)\n        # We flatten the output to a single value per image in the batch\n        return validity.view(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:58:17.340969Z","iopub.execute_input":"2025-06-09T12:58:17.341228Z","iopub.status.idle":"2025-06-09T12:58:17.347684Z","shell.execute_reply.started":"2025-06-09T12:58:17.341210Z","shell.execute_reply":"2025-06-09T12:58:17.346879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #\n# # ==================================\n# # STEP 4: TRAINING THE GAN\n# # ==================================\n# #\n\n# # --- 1. Weight Initialization ---\n# # As per the DCGAN paper, it's good practice to initialize weights from a\n# # Normal distribution with mean=0, stdev=0.02.\n# def weights_init(m):\n#     classname = m.__class__.__name__\n#     if classname.find('Conv') != -1:\n#         nn.init.normal_(m.weight.data, 0.0, 0.02)\n#     elif classname.find('BatchNorm') != -1:\n#         nn.init.normal_(m.weight.data, 1.0, 0.02)\n#         nn.init.constant_(m.bias.data, 0)\n\n# # --- 2. Initialize Models & Components ---\n# generator = Generator(latent_dim, image_channels, image_size).to(device)\n# discriminator = Discriminator(image_channels, image_size).to(device)\n\n# generator.apply(weights_init)\n# discriminator.apply(weights_init)\n\n# # Loss function\n# criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n\n# # Optimizers (Adam is a good choice for GANs)\n# # We need separate optimizers for G and D\n# optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n# optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# # A fixed noise vector to see the generator's progress over time\n# fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n\n# # --- 3. The Training Loop ---\n# print(\"Starting Training Loop...\")\n# for epoch in range(epochs):\n#     for i, (real_images, _) in enumerate(dataloader):\n\n#         # ---------------------\n#         #  TRAIN DISCRIMINATOR\n#         # ---------------------\n\n#         # Send real images to the device\n#         real_images = real_images.to(device)\n\n#         # Create labels for real and fake images\n#         # Real images get label 1, fake images get label 0\n#         real_labels = torch.ones(real_images.size(0), device=device)\n#         fake_labels = torch.zeros(real_images.size(0), device=device)\n\n#         # --- Train with real images ---\n#         optimizer_D.zero_grad()\n\n#         # Pass real images through the discriminator\n#         d_output_real = discriminator(real_images)\n#         # Calculate loss on real images\n#         errD_real = criterion(d_output_real, real_labels)\n#         errD_real.backward()\n\n#         # --- Train with fake images ---\n#         # Generate a batch of fake images\n#         noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n#         fake_images = generator(noise)\n\n#         # Pass fake images through the discriminator\n#         # We use .detach() on fake_images because we don't want to calculate\n#         # gradients for the generator at this stage.\n#         d_output_fake = discriminator(fake_images.detach())\n#         # Calculate loss on fake images\n#         errD_fake = criterion(d_output_fake, fake_labels)\n#         errD_fake.backward()\n\n#         # Total discriminator loss is the sum of real and fake losses\n#         errD = errD_real + errD_fake\n#         optimizer_D.step()\n\n#         # -----------------\n#         #  TRAIN GENERATOR\n#         # -----------------\n#         optimizer_G.zero_grad()\n\n#         # We need to run the fake images through the discriminator again\n#         d_output_on_fake = discriminator(fake_images)\n\n#         # **The Generator's Goal**: To fool the discriminator.\n#         # It wants the discriminator to output 1 (real) for its fake images.\n#         # So, we calculate the generator's loss using REAL labels (1s) for the fake images.\n#         errG = criterion(d_output_on_fake, real_labels)\n\n#         # Calculate gradients for the generator and update its weights\n#         errG.backward()\n#         optimizer_G.step()\n\n#         # --- 4. Logging and Visualization ---\n#         if i % 100 == 0:\n#             print(\n#                 f'[{epoch}/{epochs}][{i}/{len(dataloader)}] '\n#                 f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}'\n#             )\n\n#     # At the end of each epoch, generate images with the fixed noise\n#     # so we can see how the generator is improving.\n#     with torch.no_grad():\n#         fake_samples = generator(fixed_noise).detach().cpu()\n\n#     # Create a grid of images\n#     img_grid = make_grid(fake_samples, padding=2, normalize=True)\n\n#     # Save the grid to a file\n#     save_image(img_grid, f\"mnist_fake_epoch_{epoch}.png\")\n\n#     # Display the grid (optional, but great for notebooks)\n#     plt.figure(figsize=(8,8))\n#     plt.axis(\"off\")\n#     plt.title(f\"Generated Images at Epoch {epoch}\")\n#     plt.imshow(np.transpose(img_grid, (1, 2, 0)))\n#     plt.show()\n\n# print(\"Training finished!\")\n#\n# ==================================\n# STEP 4: TRAINING THE GAN\n# ==================================\n#\n\n# --- 1. Weight Initialization ---\n# As per the DCGAN paper, it's good practice to initialize weights from a\n# Normal distribution with mean=0, stdev=0.02.\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# --- 2. Initialize Models & Components ---\ngenerator = Generator(latent_dim, image_channels, image_size).to(device)\ndiscriminator = Discriminator(image_channels, image_size).to(device)\n\ngenerator.apply(weights_init)\ndiscriminator.apply(weights_init)\n\n# Loss function\ncriterion = nn.BCELoss() # Binary Cross-Entropy Loss\n\n# Optimizers (Adam is a good choice for GANs)\n# We need separate optimizers for G and D\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# A fixed noise vector to see the generator's progress over time\nfixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n\n# --- 3. The Training Loop ---\nprint(\"Starting Training Loop...\")\nfor epoch in range(epochs):\n    for i, (real_images, _) in enumerate(dataloader):\n\n        # ---------------------\n        #  TRAIN DISCRIMINATOR\n        # ---------------------\n\n        # Send real images to the device\n        real_images = real_images.to(device)\n\n        # Create labels for real and fake images\n        # Real images get label 1, fake images get label 0\n        real_labels = torch.ones(real_images.size(0), device=device)\n        fake_labels = torch.zeros(real_images.size(0), device=device)\n\n        # --- Train with real images ---\n        optimizer_D.zero_grad()\n\n        # Pass real images through the discriminator\n        d_output_real = discriminator(real_images)\n        # Calculate loss on real images\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n\n        # --- Train with fake images ---\n        # Generate a batch of fake images\n        noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n        fake_images = generator(noise)\n\n        # Pass fake images through the discriminator\n        # We use .detach() on fake_images because we don't want to calculate\n        # gradients for the generator at this stage.\n        d_output_fake = discriminator(fake_images.detach())\n        # Calculate loss on fake images\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n\n        # Total discriminator loss is the sum of real and fake losses\n        errD = errD_real + errD_fake\n        optimizer_D.step()\n\n        # -----------------\n        #  TRAIN GENERATOR\n        # -----------------\n        optimizer_G.zero_grad()\n\n        # We need to run the fake images through the discriminator again\n        d_output_on_fake = discriminator(fake_images)\n\n        # **The Generator's Goal**: To fool the discriminator.\n        # It wants the discriminator to output 1 (real) for its fake images.\n        # So, we calculate the generator's loss using REAL labels (1s) for the fake images.\n        errG = criterion(d_output_on_fake, real_labels)\n\n        # Calculate gradients for the generator and update its weights\n        errG.backward()\n        optimizer_G.step()\n\n        # --- 4. Logging and Visualization ---\n        if i % 100 == 0:\n            print(\n                f'[{epoch}/{epochs}][{i}/{len(dataloader)}] '\n                f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}'\n            )\n\n    # At the end of each epoch, generate images with the fixed noise\n    # so we can see how the generator is improving.\n    with torch.no_grad():\n        fake_samples = generator(fixed_noise).detach().cpu()\n\n    # Create a grid of images\n    img_grid = make_grid(fake_samples, padding=2, normalize=True)\n\n    # Save the grid to a file\n    save_image(img_grid, f\"mnist_fake_epoch_{epoch}.png\")\n\n    # Save the models\n    torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n    torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch}.pth\")\n\n    # Display the grid (optional, but great for notebooks)\n    plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    plt.title(f\"Generated Images at Epoch {epoch}\")\n    plt.imshow(np.transpose(img_grid, (1, 2, 0)))\n    plt.show()\n\nprint(\"Training finished!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:58:27.278735Z","iopub.execute_input":"2025-06-09T12:58:27.279008Z","iopub.status.idle":"2025-06-09T13:13:17.218265Z","shell.execute_reply.started":"2025-06-09T12:58:27.278990Z","shell.execute_reply":"2025-06-09T13:13:17.217634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# ==================================\n# STEP 5: IMAGE GENERATION\n# ==================================\n#\n# Put the generator in evaluation mode\ngenerator.eval()\n\n# Generate a batch of new images\nwith torch.no_grad():\n    # Create random noise\n    new_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n    # Generate images\n    generated_images = generator(new_noise).detach().cpu()\n\n# Un-normalize the images to the [0, 1] range for visualization\ngenerated_images = generated_images * 0.5 + 0.5 \n\n# Display the final generated images\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"Final Generated Images\")\nplt.imshow(np.transpose(make_grid(generated_images, padding=2, normalize=True), (1, 2, 0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:14:51.260687Z","iopub.execute_input":"2025-06-09T13:14:51.260973Z","iopub.status.idle":"2025-06-09T13:14:51.528648Z","shell.execute_reply.started":"2025-06-09T13:14:51.260951Z","shell.execute_reply":"2025-06-09T13:14:51.527935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# ==================================\n# STEP 6: EXPORT AND RELOAD\n# ==================================\n#\n\n# --- 1. Export the Generator's State ---\n# This saves all the learned weights and buffers of the model.\ngenerator_path = \"/kaggle/working/generator_epoch_25.pth\"\ntorch.save(generator.state_dict(), generator_path)\nprint(f\"Generator model state saved to {generator_path}\")\n\n\n# --- 2. Reload the Generator for Inference ---\n# In a real application, you would do this in a separate script.\n#\n# IMPORTANT: You first need to have the model's class definition.\n# So you must have the 'Generator' class code available.\n\n# a. Create a new instance of the Generator model\nreloaded_generator = Generator(latent_dim, image_channels, image_size).to(device)\n\n# b. Load the saved state dictionary\nreloaded_generator.load_state_dict(torch.load(generator_path))\n\n# c. Set the model to evaluation mode\nreloaded_generator.eval()\n\nprint(\"Generator reloaded successfully!\")\n\n# --- 3. Test the Reloaded Generator ---\n# Let's prove it works by generating images again.\nwith torch.no_grad():\n    test_noise = torch.randn(16, latent_dim, 1, 1, device=device)\n    reloaded_images = reloaded_generator(test_noise).detach().cpu()\n    reloaded_images = reloaded_images * 0.5 + 0.5\n\nplt.figure(figsize=(4, 4))\nplt.axis(\"off\")\nplt.title(\"Images from Reloaded Generator\")\nplt.imshow(np.transpose(make_grid(reloaded_images, padding=2, normalize=True), (1, 2, 0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:15:10.861251Z","iopub.execute_input":"2025-06-09T13:15:10.861527Z","iopub.status.idle":"2025-06-09T13:15:10.999449Z","shell.execute_reply.started":"2025-06-09T13:15:10.861508Z","shell.execute_reply":"2025-06-09T13:15:10.998734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================\n# STEP 6: EXPORT AND RELOAD\n# ==================================\n\n# --- 1. Export the Generator's State ---\n# Cette section sauvegarde les poids appris du modèle générateur.\n\n# Spécifie le chemin où le modèle sera sauvegardé\ngenerator_path = \"/kaggle/working/generator_epoch_25.pth\"  # Alternative : utiliser pathlib pour plus de portabilité -> Path(\"generator_epoch_48.pth\")\n\n# Sauvegarde le dictionnaire d'état (poids et buffers) du générateur\ntorch.save(generator.state_dict(), generator_path)  # Alternative : torch.jit.save(torch.jit.script(generator), path) pour sauvegarder le modèle complet (structure + poids)\n\n# Affiche une confirmation que le modèle a bien été sauvegardé\nprint(f\"Generator model state saved to {generator_path}\")  # Alternative : logging.info(...) pour une gestion plus propre des logs\n\n# --- 2. Reload the Generator for Inference ---\n# Cette section montre comment recharger un générateur sauvegardé,\n# typiquement dans un autre script, pour générer des images.\n\n# IMPORTANT : la classe Generator doit être définie dans le script courant,\n# car on a besoin de connaître l'architecture exacte du modèle.\n\n# a. Crée une nouvelle instance du générateur (même architecture que l’original)\nreloaded_generator = Generator(latent_dim, image_channels, image_size).to(device)  \n# Alternative : sérialiser l’architecture avec torch.save(generator, path), puis torch.load(path), mais cela a moins de contrôle et peut poser problème si le code change.\n\n# b. Charge les poids sauvegardés dans cette nouvelle instance\nreloaded_generator.load_state_dict(torch.load(generator_path))  \n# Alternative : ajouter strict=False si les architectures diffèrent légèrement (mais attention à la cohérence des résultats)\n\n# c. Passe le modèle en mode évaluation (important pour désactiver le dropout, etc.)\nreloaded_generator.eval()  # Alternative : si tu veux générer avec dropout (ex: test-time augmentation), ne pas appeler eval()\n\n# Confirmation que le modèle a été rechargé avec succès\nprint(\"Generator reloaded successfully!\")  # Alternative : logging pour journaliser l'étape\n\n# --- 3. Test the Reloaded Generator ---\n# On vérifie que le modèle rechargé fonctionne bien en générant de nouvelles images.\n\n# Désactive la grad pour ne pas calculer de gradients (plus rapide et mémoire réduite)\nwith torch.no_grad():  # Alternative : torch.inference_mode() (plus restrictif et encore plus optimisé pour l'inférence)\n\n    # Génère un batch de bruit aléatoire\n    test_noise = torch.randn(16, latent_dim, 1, 1, device=device)  \n    # Alternative : torch.normal(mean, std, size=(...), device=device) pour plus de contrôle sur la distribution du bruit\n\n    # Génère des images à partir du générateur rechargé\n    reloaded_images = reloaded_generator(test_noise).detach().cpu()  \n    # Alternative : sans detach() si tu veux faire un backward ensuite (peu probable ici)\n\n    # Remet les pixels dans la plage [0, 1] (si normalisé en [-1, 1] avant)\n    reloaded_images = reloaded_images * 0.5 + 0.5  # Alternative : reloaded_images.add(1).div(2) (même effet mais syntaxe différente)\n\n# Affiche les images générées sous forme de grille\nplt.figure(figsize=(4, 4))  # Alternative : modifier figsize pour avoir une grille plus grande ou plus petite\nplt.axis(\"off\")  # Alternative : plt.axis(\"on\") si tu veux voir les axes\nplt.title(\"Images from Reloaded Generator\")  # Alternative : ajouter des infos dynamiques dans le titre (ex: date, epoch)\n\n# Affiche la grille dans le bon format pour matplotlib\nplt.imshow(np.transpose(make_grid(reloaded_images, padding=2, normalize=True), (1, 2, 0)))  \n# Alternative : torchvision.utils.save_image(...) pour enregistrer la grille sur disque au lieu de l'afficher\n\nplt.show()  # Alternative : plt.savefig(\"output.png\") pour sauvegarder directement l’image plutôt que de l’afficher\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:15:51.204630Z","iopub.execute_input":"2025-06-09T13:15:51.204912Z","iopub.status.idle":"2025-06-09T13:15:51.342253Z","shell.execute_reply.started":"2025-06-09T13:15:51.204892Z","shell.execute_reply":"2025-06-09T13:15:51.341623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================\n# STEP 6: EXPORT AND RELOAD\n# ==================================\n\n# --- 1. Export the Generator's State ---\n# Cette section sauvegarde les poids appris du modèle générateur.\n\n# Spécifie le chemin où le modèle sera sauvegardé\ngenerator_path = \"/kaggle/working/generator_epoch_48.pth\"  \n# Alternatives : \n# - Utiliser `pathlib.Path(\"generator_epoch_48.pth\")` pour une meilleure compatibilité multiplateforme\n# - Ajouter un timestamp dynamique au nom du fichier avec `datetime.now().strftime(...)` pour versionner automatiquement\n\n# Sauvegarde le dictionnaire d'état (poids et buffers) du générateur\ntorch.save(generator.state_dict(), generator_path)  \n# Alternatives :\n# - torch.save({'model': generator.state_dict(), 'epoch': current_epoch}, path) pour sauvegarder aussi des métadonnées (utile pour reprise d'entraînement)\n# - torch.jit.save(torch.jit.script(generator), path) pour exporter un modèle complet prêt pour la production (nécessite que le modèle soit scriptable)\n# - torch.save(generator.state_dict(), open(generator_path, 'wb')) pour plus de contrôle sur la méthode d’ouverture du fichier\n\n# Affiche une confirmation que le modèle a bien été sauvegardé\nprint(f\"Generator model state saved to {generator_path}\")  \n# Alternatives :\n# - logging.info(...) pour une gestion centralisée et configurable des logs\n# - Ajouter une vérification que le fichier existe via `os.path.exists(...)` pour valider la sauvegarde\n\n# --- 2. Reload the Generator for Inference ---\n# Cette section montre comment recharger un générateur sauvegardé,\n# typiquement dans un autre script, pour générer des images.\n\n# IMPORTANT : la classe Generator doit être définie dans le script courant,\n# car on a besoin de connaître l'architecture exacte du modèle.\n\n# a. Crée une nouvelle instance du générateur (même architecture que l’original)\nreloaded_generator = Generator(latent_dim, image_channels, image_size).to(device)  \n# Alternatives :\n# - Passer des hyperparamètres via un fichier de config (YAML, JSON) pour plus de flexibilité\n# - torch.load() si le modèle complet a été sérialisé (structure + poids), bien que cela soit moins recommandé\n\n# b. Charge les poids sauvegardés dans cette nouvelle instance\nreloaded_generator.load_state_dict(torch.load(generator_path))  \n# Alternatives :\n# - strict=False si vous avez modifié légèrement l'architecture (attention : cela peut provoquer un comportement inattendu)\n# - torch.load(..., map_location=torch.device('cpu')) pour recharger sur CPU même si le modèle a été sauvegardé depuis le GPU\n\n# c. Passe le modèle en mode évaluation (important pour désactiver le dropout, etc.)\nreloaded_generator.eval()  \n# Alternatives :\n# - Ne pas appeler `eval()` si vous voulez conserver des comportements stochastiques comme le Dropout (utile en test-time augmentation)\n# - Utiliser `model.train(False)` (équivalent à `.eval()`)\n\n# Confirmation que le modèle a été rechargé avec succès\nprint(\"Generator reloaded successfully!\")  \n# Alternatives :\n# - logging.info(\"Generator reloaded...\") pour un suivi plus rigoureux\n# - Afficher également la taille ou le résumé du modèle via `print(reloaded_generator)` ou `summary(...)`\n\n# --- 3. Test the Reloaded Generator ---\n# On vérifie que le modèle rechargé fonctionne bien en générant de nouvelles images.\n\n# Désactive la grad pour ne pas calculer de gradients (plus rapide et mémoire réduite)\nwith torch.no_grad():  \n# Alternatives :\n# - torch.inference_mode() pour une version encore plus optimisée (depuis PyTorch 1.9)\n# - Pas de contexte du tout si vous voulez analyser les gradients pour autre chose (ex : étude du comportement du modèle)\n\n    # Génère un batch de bruit aléatoire\n    test_noise = torch.randn(16, latent_dim, 1, 1, device=device)  \n    # Alternatives :\n    # - torch.normal(mean, std, size=(...), device=device) pour spécifier une moyenne et un écart type personnalisés\n    # - torch.empty(...).uniform_(-1, 1) pour générer un bruit uniforme\n    # - Utiliser une seed fixe via `torch.manual_seed(...)` pour résultats reproductibles\n\n    # Génère des images à partir du générateur rechargé\n    reloaded_images = reloaded_generator(test_noise).detach().cpu()  \n    # Alternatives :\n    # - Ne pas appeler detach() si vous souhaitez conserver la trace des gradients\n    # - Utiliser `.clone()` pour éviter d'altérer les données originales\n\n    # Remet les pixels dans la plage [0, 1] (si normalisé en [-1, 1] avant)\n    reloaded_images = reloaded_images * 0.5 + 0.5  \n    # Alternatives :\n    # - reloaded_images.add(1).div(2) (même effet, différente syntaxe)\n    # - torchvision.transforms.Normalize(...) avec des valeurs inverses pour \"dénormaliser\"\n\n# Affiche les images générées sous forme de grille\nplt.figure(figsize=(4, 4))  \n# Alternatives :\n# - figsize=(8, 8) ou plus pour affichage haute résolution\n# - fig = plt.subplots(...) si vous voulez un contrôle plus précis\n\nplt.axis(\"off\")  \n# Alternatives :\n# - plt.axis(\"on\") pour afficher les axes (utile pour debug)\n# - plt.grid(True) si vous voulez des repères\n\nplt.title(\"Images from Reloaded Generator\")  \n# Alternatives :\n# - Ajouter des infos contextuelles dynamiques : f\"Images - Epoch {epoch} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n# - Utiliser plusieurs sous-titres avec `plt.suptitle(...)` si vous avez plusieurs figures\n\n# Affiche la grille dans le bon format pour matplotlib\nplt.imshow(np.transpose(make_grid(reloaded_images, padding=2, normalize=True), (1, 2, 0)))  \n# Alternatives :\n# - torchvision.utils.save_image(reloaded_images, \"generated_grid.png\", nrow=4, normalize=True) pour sauvegarder au lieu d'afficher\n# - Utiliser PIL.Image.fromarray(...) pour manipuler l’image différemment\n\nplt.show()  \n# Alternatives :\n# - plt.savefig(\"reloaded_generator_output.png\") pour enregistrer l'image sur disque\n# - plt.pause(0.001) dans des environnements interactifs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:16:16.283362Z","iopub.execute_input":"2025-06-09T13:16:16.283814Z","iopub.status.idle":"2025-06-09T13:16:16.423968Z","shell.execute_reply.started":"2025-06-09T13:16:16.283792Z","shell.execute_reply":"2025-06-09T13:16:16.423280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:57:37.760198Z","iopub.execute_input":"2025-06-09T13:57:37.760892Z","iopub.status.idle":"2025-06-09T13:57:37.835430Z","shell.execute_reply.started":"2025-06-09T13:57:37.760856Z","shell.execute_reply":"2025-06-09T13:57:37.834536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch  # PyTorch pour opérations ML & GPU; alternative: TensorFlow (tf), JAX (jax.numpy)\nimport torch.nn as nn  # Modules NN (layers); alternative: torch.nn.functional pour fonctions stateless, ou frameworks comme Keras\nimport torch.optim as optim  # Optimiseurs; alternatives: SGD, RMSprop, AdamW, AdaGrad, LARS\nfrom torchvision import datasets, transforms, utils  # Datasets + transformations + utils; alternatives: custom datasets, albumentations, PIL.Image\nfrom torch.utils.data import DataLoader  # Chargement batch data; alternatives: DataLoader avec sampler personnalisé, torch.utils.data.DataLoader avec multiprocessing\nimport matplotlib.pyplot as plt  # Visualisation images; alternatives: seaborn, plotly, PIL.Image.show(), OpenCV (cv2.imshow)\nimport numpy as np  # Calcul numérique; alternatives: torch.Tensor pour tout, pandas pour tableaux, numba pour optimisation\n\nimport os  # Gestion fichiers/chemins; alternative: pathlib (plus moderne et orienté objets)\n\n# Choix device : GPU si disponible sinon CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n# Alternatives: forcer CPU avec torch.device(\"cpu\"), ou définir device spécifique cuda:0, cuda:1 si plusieurs GPUs\n\nprint(f\"Using device: {device}\")  \n# Alternative: logging.info(f\"Using device: {device}\") pour journalisation, ou print(colorama pour couleur)\n\n# Chemins fichiers pth (uploadés dans dossier Kaggle Input)\nGENERATOR_PATH = \"/kaggle/working/generator_epoch_25.pth\" \n# Alternative: \"./generator.pth\" pour chemin local, ou pathlib.Path(\"...\") pour manipulation objet chemin\n\nDISCRIMINATOR_PATH = \"/kaggle/working/discriminator_epoch_25.pth\"\n# Alternative: utiliser os.path.join pour concaténation robuste ou variables d'environnement\n\nBATCH_SIZE = 64  \n# Alternative: ajuster dynamiquement selon mémoire GPU (torch.cuda.get_device_properties), ou tester 32,128 selon vitesse/mémoire\n\nLATENT_DIM = 100  \n# Alternative: augmenter latent_dim pour plus de richesse, ou diminuer pour plus rapide/facile à entraîner\n\nIMG_SIZE = 32  \n# Alternative: dataset plus grand (64, 128), ou resize dynamique avec transforms.RandomResizedCrop\n\nIMG_CHANNELS = 3  \n# Alternative: 1 pour grayscale (ex: MNIST), 4 si RGBA ou image multispectrale\n\n# --- Generator ---\nclass Generator(nn.Module):\n    def __init__(self, latent_dim=LATENT_DIM, img_channels=IMG_CHANNELS, img_size=IMG_SIZE):\n        super(Generator, self).__init__()\n        self.init_size = img_size // 4  \n        # Alternative: diviser par 8 ou 16 pour architectures plus profondes; ou ne pas diviser pour taille originale\n\n        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))  \n        # Alternative: ajouter dropout, batchnorm, layernorm, ou plusieurs couches linéaires (MLP profond)\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),  \n            # Alternatives: InstanceNorm2d, GroupNorm, LayerNorm, ou pas de normalisation (parfois bénéfique)\n\n            nn.Upsample(scale_factor=2),  \n            # Alternatives: ConvTranspose2d pour upsampling appris, PixelShuffle, ou interpolation bilinéaire\n\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),  \n            # Alternatives: kernels 5x5, depthwise separable conv (MobileNet style), dilated convolutions\n\n            nn.BatchNorm2d(128, 0.8),  \n            # Alternative: momentum différent, ou remplacer batchnorm par dropout spatial\n\n            nn.ReLU(inplace=True),  \n            # Alternatives: LeakyReLU(negative_slope=0.2), PReLU (paramétrique), ELU, GELU, SELU pour self-normalizing\n\n            nn.Upsample(scale_factor=2),  \n            # Alternatives: ConvTranspose2d avec stride=2, ou resize + conv\n\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),  \n            # Alternative: groupes, profondeur separable, normalisation ou pas\n\n            nn.BatchNorm2d(64, 0.8),  \n            # Alternative: LayerNorm, InstanceNorm, ou sans normalisation\n\n            nn.ReLU(inplace=True),  \n            # Alternative: same as above activations\n\n            nn.Conv2d(64, img_channels, 3, stride=1, padding=1),  \n            # Alternative: 1 canal grayscale, 4 canaux RGBA, ou même channels pour style transfer\n\n            nn.Tanh()  \n            # Alternative: Sigmoid (sortie 0-1) si normalisation dataset différente, ou pas d’activation finale pour GAN Wasserstein\n        )\n\n    def forward(self, z):\n        out = self.l1(z)  \n        # Alternative: activation (ex: LeakyReLU) après linéaire pour mieux entraîner\n\n        out = out.view(out.size(0), 128, self.init_size, self.init_size)  \n        # Alternative: torch.reshape(out, (batch_size, 128, init_size, init_size)) ou .permute si réarrangement nécessaire\n\n        img = self.conv_blocks(out)  \n        # Alternative: plus de blocs, blocs résiduels, attention (self-attention GAN)\n\n        return img\n\n# --- Discriminator ---\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels=IMG_CHANNELS, img_size=IMG_SIZE):\n        super(Discriminator, self).__init__()\n\n        def disc_block(in_filters, out_filters, bn=True):\n            layers = [\n                nn.Conv2d(in_filters, out_filters, 3, 2, 1),  \n                # Alternative: stride=1 + MaxPool2d, dilated conv, spectral normalization\n\n                nn.LeakyReLU(0.2, inplace=True),  \n                # Alternative: ReLU, PReLU, ELU, SELU\n\n                nn.Dropout2d(0.25)  \n                # Alternative: SpatialDropout, Dropout classique, ou pas de dropout pour GAN moins régularisés\n            ]\n            if bn:\n                layers.append(nn.BatchNorm2d(out_filters, 0.8))  \n                # Alternative: InstanceNorm2d, LayerNorm, SpectralNorm (très utile en GANs pour stabilité)\n\n            return layers\n\n        self.model = nn.Sequential(\n            *disc_block(img_channels, 16, bn=False),  \n            # Alternative: mettre batchnorm aussi ici\n\n            *disc_block(16, 32),\n            *disc_block(32, 64),\n            *disc_block(64, 128),\n            # Alternative: plus ou moins de blocs selon complexité/dataset\n        )\n\n        ds_size = img_size // 2**4  \n        # Alternative: changer profondeur pour taille finale différente\n\n        self.adv_layer = nn.Sequential(\n            nn.Linear(128 * ds_size * ds_size, 1),  \n            # Alternative: convolution 1x1 + GAP (global average pooling)\n\n            nn.Sigmoid()  \n            # Alternative: pas d’activation + BCEWithLogitsLoss (plus stable), ou sorties logits pour Wasserstein GAN\n        )\n\n    def forward(self, img):\n        out = self.model(img)  \n        # Alternative: ajouter bruit gaussien, ou labels smoothing dans forward\n\n        out = out.view(out.size(0), -1)  \n        # Alternative: torch.flatten(out, start_dim=1)\n\n        validity = self.adv_layer(out)  \n        # Alternative: multiple sorties (multi-tâches), ou features pour losses additionnelles\n\n        return validity\n\n# --- Instanciation et chargement ---\ngenerator = Generator().to(device)  \n# Alternative: charger modèle scripté torch.jit.load(), ou DataParallel pour multi-GPU\n\ndiscriminator = Discriminator().to(device)  \n# Alternative: charger partiellement (strict=False), ou avec try-except pour gérer erreurs\n\nprint(f\"Loading generator from {GENERATOR_PATH}\")\ngenerator.load_state_dict(torch.load(GENERATOR_PATH, map_location=device))  \n# Alternative: torch.jit.load pour modèle complet (archi + poids)\n\nprint(f\"Loading discriminator from {DISCRIMINATOR_PATH}\")\ndiscriminator.load_state_dict(torch.load(DISCRIMINATOR_PATH, map_location=device))  \n# Alternative: charger avec strict=False si changement architecture léger\n\n# --- Dataset CIFAR-10 ---\ntransform = transforms.Compose([\n    transforms.Resize(IMG_SIZE),  \n    # Alternative: transforms.CenterCrop, RandomCrop, RandomResizedCrop pour augmentation\n\n    transforms.ToTensor(),  \n    # Alternative: transforms.ToPILImage inverse, custom transforms\n\n    transforms.Normalize([0.5]*3, [0.5]*3),  \n    # Alternative: normalisation selon stats CIFAR10 (mean/std), ou pas de normalisation\n])\n\ntrain_dataset = datasets.CIFAR10(root=\"/kaggle/working/data\", train=True, download=True, transform=transform)  \n# Alternative: STL10, CelebA, custom datasets, ou subset spécifique\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  \n# Alternative: shuffle=False, sampler personnalisé, num_workers>0 pour paralléliser le chargement\n\n# --- Loss & Optimizers ---\nadversarial_loss = nn.BCELoss()  \n# Alternative: nn.BCEWithLogitsLoss, hinge loss, Wasserstein loss (WGAN-GP)\n\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))  \n# Alternative: RMSprop, SGD avec momentum, AdamW, Adam avec lr scheduler\n\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))  \n# Alternative: lr différent du générateur, scheduler de lr\n\n# --- Training Loop ---\nEPOCHS = 10  \n# Alternative: early stopping, augmenter/diminuer epochs, ou scheduler pour lr\n\nfor epoch in range(EPOCHS):\n    generator.train()  \n    # Alternative: eval() pour freeze, ou mixed precision training (torch.cuda.amp.autocast)\n\n    discriminator.train()  \n    # Alternative: eval() si on veut figer discriminateur temporairement\n\n    for batch_idx, (imgs, _) in enumerate(train_loader):\n        batch_size = imgs.size(0)  \n        # Alternative: batch fixe ou batch dynamique\n\n        real_imgs = imgs.to(device)  \n        # Alternative: mixed precision, augmenter bruit (Gaussian noise)\n\n        valid = torch.ones(batch_size, 1, device=device)  \n        # Alternative: label smoothing (0.9), label flipping (invert labels)\n\n        fake = torch.zeros(batch_size, 1, device=device)  \n        # Alternative: same as valid, label noise\n\n        # Train Discriminator\n        optimizer_D.zero_grad()  \n        # Alternative: optimizer_D.zero_grad(set_to_none=True) pour performance mémoire\n\n        real_pred = discriminator(real_imgs)  \n        # Alternative: ajouter bruit ou augmentation\n\n        real_loss = adversarial_loss(real_pred, valid)  \n        # Alternative: Wasserstein loss ou hinge loss\n\n        z = torch.randn(batch_size, LATENT_DIM, device=device)  \n        # Alternative: uniform noise, bruit fixe, bruit conditionnel\n\n        fake_imgs = generator(z)  \n        # Alternative: ajouter bruit input ou layers dropout\n\n        fake_pred = discriminator(fake_imgs.detach())  \n        # Alternative: pas de detach() pour backprop conjoint\n\n        fake_loss = adversarial_loss(fake_pred, fake)  \n        # Alternative: weighted loss, hinge loss\n\n        d_loss = (real_loss + fake_loss) / 2  \n        # Alternative: pondération différente, ou loss plus complexe (feature matching)\n\n        d_loss.backward()  \n        # Alternative: gradient clipping\n\n        optimizer_D.step()  \n        # Alternative: scheduler.step(), ou optimizer alternatif\n\n        # Train Generator\n        optimizer_G.zero_grad()  \n        # Alternative: set_to_none=True, ou accumulate gradients\n\n        gen_pred = discriminator(fake_imgs)  \n        # Alternative: ajouter bruit pour régularisation\n\n        g_loss = adversarial_loss(gen_pred, valid)  \n        # Alternative: feature matching, perceptual loss, Wasserstein loss\n\n        g_loss.backward()  \n        # Alternative: gradient accumulation, mixed precision\n\n        optimizer_G.step()  \n        # Alternative: optimiser moins fréquemment (1x every n itérations)\n\n        if batch_idx % 200 == 0:\n            print(f\"[Epoch {epoch+1}/{EPOCHS}] Batch {batch_idx}/{len(train_loader)} D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")\n            # Alternative: log dans tensorboard, wandb, ou fichier\n\n# --- Génération finale ---\ngenerator.eval()  \n# Alternative: utiliser torch.no_grad() ou torch.inference_mode() pour meilleur perf\n\nwith torch.no_grad():\n    z = torch.randn(16, LATENT_DIM, device=device)  \n    # Alternative: seed fixé (torch.manual_seed) pour reproductibilité\n\n    gen_imgs = generator(z).cpu()  \n    # Alternative: garder sur GPU pour post-traitement\n\n    gen_imgs = (gen_imgs + 1) / 2  \n    # Alternative: min-max normalization, ou pas de normalisation si Tanh absente\n\ngrid = utils.make_grid(gen_imgs, nrow=4)  \n# Alternative: torchvision.utils.save_image pour sauvegarder, ou montage personnalisé\n\nplt.figure(figsize=(8,8))  \n# Alternative: plt.subplots() pour plus de contrôle\n\nplt.axis(\"off\")  \n# Alternative: plt.axis(\"on\") pour debug axes\n\nplt.title(\"Images générées après fine-tuning CIFAR-10\")  \n# Alternative: inclure timestamp, epoch, loss\n\nplt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))  \n# Alternative: plt.imshow(grid.permute(1,2,0)) si tensor, ou PIL.Image.fromarray\n\nplt.show()  \n# Alternative: plt.savefig(\"output.png\") pour sauvegarder\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-10T10:43:59.866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================\n# SETUP : LIBRAIRIES ET HYPERPARAMÈTRES GLOBAUX\n# ==================================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import make_grid, save_image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom copy import deepcopy\n\n# --- Création des dossiers pour sauvegarder les résultats ---\nos.makedirs(\"mnist_images\", exist_ok=True)\nos.makedirs(\"cifar10_images\", exist_ok=True)\nos.makedirs(\"models\", exist_ok=True)\n\n\n# --- Hyperparamètres ---\nbatch_size = 128\nimage_size = 64\nlatent_dim = 100\nlr_pretrain = 0.0002  # Taux d'apprentissage pour le pré-entraînement\nlr_finetune = 0.0001 # Taux d'apprentissage plus faible pour le fine-tuning\nbetas = (0.5, 0.999) # Bêtas pour l'optimiseur Adam, éprouvés pour les GANs\nepochs_pretrain = 15 # Nombre d'époques pour MNIST (pas besoin de beaucoup pour des features de base)\nepochs_finetune = 25 # Nombre d'époques pour le fine-tuning sur CIFAR-10\n\n# --- Configuration du device (GPU si disponible) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Utilisation du device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:02:54.336234Z","iopub.execute_input":"2025-06-10T12:02:54.336443Z","iopub.status.idle":"2025-06-10T12:03:03.930000Z","shell.execute_reply.started":"2025-06-10T12:02:54.336421Z","shell.execute_reply":"2025-06-10T12:03:03.929347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================================\n# PHASE 1 : PRÉ-ENTRAÎNEMENT DU GAN SUR MNIST (SOURCE)\n# =======================================================\n\nprint(\"\\n--- DÉBUT DE LA PHASE 1 : PRÉ-ENTRAÎNEMENT SUR MNIST ---\")\n\n# --- 1.1. Préparation des données MNIST ---\nimage_channels_mnist = 1\ntransform_mnist = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]) # Normalisation pour des images monochromes\n])\n\nmnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\ndataloader_mnist = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n\n\n# --- 1.2. Définition des modèles (Générateur et Discriminateur) ---\n# NOTE : Ces classes sont identiques à celles fournies, mais nous les redéfinissons ici pour la clarté.\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, channels):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False), nn.Tanh()\n        )\n    def forward(self, z): return self.model(z)\n\nclass Discriminator(nn.Module):\n    def __init__(self, channels):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(channels, 64, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False), nn.Sigmoid()\n        )\n    def forward(self, img): return self.model(img).view(-1)\n\n# Fonction d'initialisation des poids (pratique standard pour les DCGAN)\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1: nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1: nn.init.normal_(m.weight.data, 1.0, 0.02); nn.init.constant_(m.bias.data, 0)\n\n# --- 1.3. Initialisation et entraînement ---\ngenerator_mnist = Generator(latent_dim, image_channels_mnist).to(device)\ndiscriminator_mnist = Discriminator(image_channels_mnist).to(device)\ngenerator_mnist.apply(weights_init)\ndiscriminator_mnist.apply(weights_init)\n\ncriterion = nn.BCELoss()\noptimizer_G_mnist = optim.Adam(generator_mnist.parameters(), lr=lr_pretrain, betas=betas)\noptimizer_D_mnist = optim.Adam(discriminator_mnist.parameters(), lr=lr_pretrain, betas=betas)\n\nfixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n\nprint(\"Début de l'entraînement sur MNIST...\")\nfor epoch in range(epochs_pretrain):\n    for i, (real_images, _) in enumerate(dataloader_mnist):\n        real_images = real_images.to(device)\n        real_labels = torch.ones(real_images.size(0), device=device)\n        fake_labels = torch.zeros(real_images.size(0), device=device)\n\n        # --- Entraînement du Discriminateur ---\n        optimizer_D_mnist.zero_grad()\n        # Perte sur les images réelles\n        d_output_real = discriminator_mnist(real_images)\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n        # Perte sur les images générées\n        noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n        fake_images = generator_mnist(noise)\n        d_output_fake = discriminator_mnist(fake_images.detach())\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n        errD = errD_real + errD_fake\n        optimizer_D_mnist.step()\n\n        # --- Entraînement du Générateur ---\n        optimizer_G_mnist.zero_grad()\n        d_output_on_fake = discriminator_mnist(fake_images)\n        errG = criterion(d_output_on_fake, real_labels)\n        errG.backward()\n        optimizer_G_mnist.step()\n\n    print(f\"[Epoch MNIST {epoch+1}/{epochs_pretrain}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}\")\n    with torch.no_grad():\n        fake_samples = generator_mnist(fixed_noise).detach().cpu()\n    save_image(fake_samples, f\"mnist_images/mnist_fake_epoch_{epoch+1}.png\", normalize=True)\n\n# Sauvegarde des modèles pré-entraînés\ntorch.save(generator_mnist.state_dict(), \"models/generator_mnist.pth\")\ntorch.save(discriminator_mnist.state_dict(), \"models/discriminator_mnist.pth\")\nprint(\"--- FIN DE LA PHASE 1 : Modèles MNIST pré-entraînés et sauvegardés. ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:03:22.371141Z","iopub.execute_input":"2025-06-10T12:03:22.371409Z","iopub.status.idle":"2025-06-10T12:12:22.090539Z","shell.execute_reply.started":"2025-06-10T12:03:22.371389Z","shell.execute_reply":"2025-06-10T12:12:22.089889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# PHASE 2 : TRANSFERT D'APPRENTISSAGE ET FINE-TUNING SUR CIFAR-10 (CIBLE)\n# =========================================================================\n\nprint(\"\\n--- DÉBUT DE LA PHASE 2 : TRANSFERT ET FINE-TUNING SUR CIFAR-10 ---\")\n\n# --- 2.1. Préparation des données CIFAR-10 ---\nimage_channels_cifar = 3 # CIFAR-10 a 3 canaux de couleur (RGB)\ntransform_cifar = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # Normalisation pour 3 canaux\n])\n\ncifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\ndataloader_cifar = DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True)\n\n# Visualisation d'un batch d'images CIFAR-10\nreal_batch_cifar = next(iter(dataloader_cifar))\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"Batch d'images réelles de CIFAR-10\")\ngrid = make_grid(real_batch_cifar[0][:64] * 0.5 + 0.5, padding=2, normalize=True)\nplt.imshow(np.transpose(grid.cpu(), (1, 2, 0)))\nplt.show()\n\n\n# --- 2.2. Création des nouveaux modèles et transfert des poids ---\n\n# Nous créons de nouvelles instances de modèles pour CIFAR-10\n# Notez le changement du nombre de canaux\ngenerator_cifar = Generator(latent_dim, image_channels_cifar).to(device)\ndiscriminator_cifar = Discriminator(image_channels_cifar).to(device)\n\n# Chargement des poids pré-entraînés de MNIST\ngen_mnist_weights = torch.load(\"models/generator_mnist.pth\", map_location=device)\ndisc_mnist_weights = torch.load(\"models/discriminator_mnist.pth\", map_location=device)\n\n# *** ÉTAPE CRUCIALE : Transfert des poids compatibles ***\n# La dernière couche du générateur et la première du discriminateur ont des dimensions\n# différentes (à cause du nombre de canaux). Nous les ignorons lors du chargement.\n# Le reste des poids (les couches profondes) sera transféré.\n\n# Pour le Générateur\n# La couche finale \"model.8.weight\" (ConvTranspose2d) ne sera pas chargée\ngenerator_cifar.load_state_dict(gen_mnist_weights, strict=False)\nprint(\"Poids du générateur MNIST transférés (sauf la couche de sortie).\")\n\n# Pour le Discriminateur\n# La couche initiale \"model.0.weight\" (Conv2d) ne sera pas chargée\ndiscriminator_cifar.load_state_dict(disc_mnist_weights, strict=False)\nprint(\"Poids du discriminateur MNIST transférés (sauf la couche d'entrée).\")\n\n\n# --- 2.3. Gel (Freeze) des couches transférées ---\n# Nous gelons les 3 premiers blocs de chaque modèle. Seules les couches\n# plus profondes (spécifiques à la tâche) seront entraînées au début.\n\n# Fonction utilitaire pour vérifier l'état des couches\ndef print_trainable_status(model, model_name):\n    print(f\"\\nStatut des paramètres pour {model_name}:\")\n    for name, param in model.named_parameters():\n        print(f\"{name:<40} Entraînable: {param.requires_grad}\")\n\n# Geler les couches\nlayers_to_freeze_g = 6 # Les 3 premiers blocs (ConvT, BatchNorm)\nlayers_to_freeze_d = 6 # Les 3 premiers blocs (Conv, BatchNorm)\n\nfor i, (name, param) in enumerate(generator_cifar.named_parameters()):\n    if i < layers_to_freeze_g:\n        param.requires_grad = False\n\nfor i, (name, param) in enumerate(discriminator_cifar.named_parameters()):\n    if i < layers_to_freeze_d:\n        param.requires_grad = False\n\nprint_trainable_status(generator_cifar, \"Générateur CIFAR-10\")\nprint_trainable_status(discriminator_cifar, \"Discriminateur CIFAR-10\")\n\n# --- 2.4. Fine-Tuning sur CIFAR-10 ---\n# On crée des optimiseurs qui ne mettront à jour que les poids \"dégelés\" (requires_grad=True)\nparams_g_finetune = filter(lambda p: p.requires_grad, generator_cifar.parameters())\nparams_d_finetune = filter(lambda p: p.requires_grad, discriminator_cifar.parameters())\n\noptimizer_G_cifar = optim.Adam(params_g_finetune, lr=lr_finetune, betas=betas)\noptimizer_D_cifar = optim.Adam(params_d_finetune, lr=lr_finetune, betas=betas)\n\nprint(\"\\nDébut du Fine-Tuning sur CIFAR-10...\")\nfor epoch in range(epochs_finetune):\n    for i, (real_images, _) in enumerate(dataloader_cifar):\n        real_images = real_images.to(device)\n        real_labels = torch.ones(real_images.size(0), device=device)\n        fake_labels = torch.zeros(real_images.size(0), device=device)\n\n        # --- Entraînement du Discriminateur ---\n        optimizer_D_cifar.zero_grad()\n        d_output_real = discriminator_cifar(real_images)\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n\n        noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n        fake_images = generator_cifar(noise)\n        d_output_fake = discriminator_cifar(fake_images.detach())\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n        errD = errD_real + errD_fake\n        optimizer_D_cifar.step()\n\n        # --- Entraînement du Générateur ---\n        optimizer_G_cifar.zero_grad()\n        d_output_on_fake = discriminator_cifar(fake_images)\n        errG = criterion(d_output_on_fake, real_labels)\n        errG.backward()\n        optimizer_G_cifar.step()\n\n    print(f\"[Epoch CIFAR-10 {epoch+1}/{epochs_finetune}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}\")\n    with torch.no_grad():\n        fake_samples = generator_cifar(fixed_noise).detach().cpu()\n    save_image(fake_samples, f\"cifar10_images/cifar10_fake_epoch_{epoch+1}.png\", normalize=True)\n\n\nprint(\"--- FIN DE LA PHASE 2 : Fine-tuning terminé. ---\")\n\n\n# --- 2.5. Génération et Visualisation finale ---\ngenerator_cifar.eval()\nwith torch.no_grad():\n    final_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n    generated_images = generator_cifar(final_noise).detach().cpu()\n\nplt.figure(figsize=(10, 10))\nplt.axis(\"off\")\nplt.title(\"Images CIFAR-10 finales générées par Transfer Learning\")\nplt.imshow(np.transpose(make_grid(generated_images, padding=2, normalize=True), (1, 2, 0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:50:05.891155Z","iopub.execute_input":"2025-06-09T18:50:05.891403Z","iopub.status.idle":"2025-06-09T18:50:21.193740Z","shell.execute_reply.started":"2025-06-09T18:50:05.891385Z","shell.execute_reply":"2025-06-09T18:50:21.192792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# PHASE 2 : TRANSFERT D'APPRENTISSAGE ET FINE-TUNING SUR CIFAR-10 (CIBLE)\n# =========================================================================\n\nprint(\"\\n--- DÉBUT DE LA PHASE 2 : TRANSFERT ET FINE-TUNING SUR CIFAR-10 ---\")\n\n# --- 2.1. Préparation des données CIFAR-10 ---\nimage_channels_cifar = 3 # CIFAR-10 a 3 canaux de couleur (RGB)\ntransform_cifar = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # Normalisation pour 3 canaux\n])\n\ncifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\ndataloader_cifar = DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True)\n\n# Visualisation d'un batch d'images CIFAR-10\nreal_batch_cifar = next(iter(dataloader_cifar))\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"Batch d'images réelles de CIFAR-10\")\ngrid = make_grid(real_batch_cifar[0][:64] * 0.5 + 0.5, padding=2, normalize=True)\nplt.imshow(np.transpose(grid.cpu(), (1, 2, 0)))\nplt.show()\n\n# --- 2.2. (CORRIGÉ) Création des nouveaux modèles et transfert des poids ---\ngenerator_cifar = Generator(latent_dim, image_channels_cifar).to(device)\ndiscriminator_cifar = Discriminator(image_channels_cifar).to(device)\n\ngen_mnist_weights = torch.load(\"models/generator_mnist.pth\", map_location=device)\ndisc_mnist_weights = torch.load(\"models/discriminator_mnist.pth\", map_location=device)\n\n# Filtrage pour le Générateur\ngen_cifar_dict = generator_cifar.state_dict()\npretrained_gen_dict = {k: v for k, v in gen_mnist_weights.items() if k in gen_cifar_dict and gen_cifar_dict[k].size() == v.size()}\ngen_cifar_dict.update(pretrained_gen_dict)\ngenerator_cifar.load_state_dict(gen_cifar_dict)\nprint(f\"Transfert de {len(pretrained_gen_dict)}/{len(gen_cifar_dict)} couches du Générateur MNIST vers CIFAR-10.\")\n\n# Filtrage pour le Discriminateur\ndisc_cifar_dict = discriminator_cifar.state_dict()\npretrained_disc_dict = {k: v for k, v in disc_mnist_weights.items() if k in disc_cifar_dict and disc_cifar_dict[k].size() == v.size()}\ndisc_cifar_dict.update(pretrained_disc_dict)\ndiscriminator_cifar.load_state_dict(disc_cifar_dict)\nprint(f\"Transfert de {len(pretrained_disc_dict)}/{len(disc_cifar_dict)} couches du Discriminateur MNIST vers CIFAR-10.\")\n\n\n# --- 2.3. Gel (Freeze) des couches transférées ---\n# Cette partie est correcte et très importante pour la stabilité du fine-tuning.\ndef print_trainable_status(model, model_name):\n    print(f\"\\nStatut des paramètres pour {model_name}:\")\n    total_params = 0\n    trainable_params = 0\n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n        print(f\"{name:<40} Entraînable: {param.requires_grad}\")\n    print(f\"Paramètres entraînables : {trainable_params} / {total_params} ({100 * trainable_params / total_params:.2f}%)\")\n\n\n# Nous gelons les couches qui ont été chargées depuis MNIST pour ne pas les \"oublier\" trop vite.\n# On ne fine-tune que les dernières couches (celles qui n'ont pas été chargées + les plus profondes)\nlayers_to_freeze_g = 8 # Gelons les 4 premiers blocs\nlayers_to_freeze_d = 8 # Gelons les 4 premiers blocs\n\nfor i, (name, param) in enumerate(generator_cifar.named_parameters()):\n    if name in pretrained_gen_dict: # Une façon plus robuste de geler que de compter\n        param.requires_grad = False\n\nfor i, (name, param) in enumerate(discriminator_cifar.named_parameters()):\n     if name in pretrained_disc_dict:\n        param.requires_grad = False\n\nprint_trainable_status(generator_cifar, \"Générateur CIFAR-10\")\nprint_trainable_status(discriminator_cifar, \"Discriminateur CIFAR-10\")\n\n\n# --- 2.4. Fine-Tuning sur CIFAR-10 ---\n# Le reste de votre code est correct.\nparams_g_finetune = filter(lambda p: p.requires_grad, generator_cifar.parameters())\nparams_d_finetune = filter(lambda p: p.requires_grad, discriminator_cifar.parameters())\n\noptimizer_G_cifar = optim.Adam(params_g_finetune, lr=lr_finetune, betas=betas)\noptimizer_D_cifar = optim.Adam(params_d_finetune, lr=lr_finetune, betas=betas)\n\nprint(\"\\nDébut du Fine-Tuning sur CIFAR-10...\")\n# ... la boucle d'entraînement reste inchangée ...\nfor epoch in range(epochs_finetune):\n    for i, (real_images, _) in enumerate(dataloader_cifar):\n        real_images = real_images.to(device)\n        real_labels = torch.ones(real_images.size(0), device=device)\n        fake_labels = torch.zeros(real_images.size(0), device=device)\n\n        # Entraînement du Discriminateur\n        optimizer_D_cifar.zero_grad()\n        d_output_real = discriminator_cifar(real_images)\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n        noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n        fake_images = generator_cifar(noise)\n        d_output_fake = discriminator_cifar(fake_images.detach())\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n        errD = errD_real + errD_fake\n        optimizer_D_cifar.step()\n\n        # Entraînement du Générateur\n        optimizer_G_cifar.zero_grad()\n        d_output_on_fake = discriminator_cifar(fake_images)\n        errG = criterion(d_output_on_fake, real_labels)\n        errG.backward()\n        optimizer_G_cifar.step()\n\n    print(f\"[Epoch CIFAR-10 {epoch+1}/{epochs_finetune}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}\")\n    with torch.no_grad():\n        fake_samples = generator_cifar(fixed_noise).detach().cpu()\n    save_image(fake_samples, f\"cifar10_images/cifar10_fake_epoch_{epoch+1}.png\", normalize=True)\n\nprint(\"--- FIN DE LA PHASE 2 : Fine-tuning terminé. ---\")\n\n\n# --- 2.5. Génération et Visualisation finale ---\n# ... cette section reste inchangée ...\ngenerator_cifar.eval()\nwith torch.no_grad():\n    final_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n    generated_images = generator_cifar(final_noise).detach().cpu()\n\nplt.figure(figsize=(10, 10))\nplt.axis(\"off\")\nplt.title(\"Images CIFAR-10 finales générées par Transfer Learning\")\nplt.imshow(np.transpose(make_grid(generated_images, padding=2, normalize=True), (1, 2, 0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:00:26.472066Z","iopub.execute_input":"2025-06-09T19:00:26.472341Z","iopub.status.idle":"2025-06-09T19:08:52.529398Z","shell.execute_reply.started":"2025-06-09T19:00:26.472321Z","shell.execute_reply":"2025-06-09T19:08:52.528528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# PHASE 2 : TRANSFERT D'APPRENTISSAGE ET FINE-TUNING SUR CIFAR-10 (CIBLE)\n# =========================================================================\n# TECHNIQUE: Organisation modulaire du code avec séparateurs visuels\n# ALTERNATIVES: Utiliser des classes, des modules séparés, ou des notebooks Jupyter\n# ENFANT: C'est comme mettre une pancarte pour dire \"Ici commence la partie 2 de notre programme\"\n\nprint(\"\\n--- DÉBUT DE LA PHASE 2 : TRANSFERT ET FINE-TUNING SUR CIFAR-10 ---\")\n# TECHNIQUE: Logging informatif avec séparateurs visuels (\\n pour nouvelle ligne)\n# ALTERNATIVES: logging.info(), tqdm.write(), print avec timestamp, wandb.log()\n# ENFANT: On dit à l'ordinateur d'écrire un message pour nous dire où on en est\n\n# --- 2.1. Préparation des données CIFAR-10 ---\nimage_channels_cifar = 3 # CIFAR-10 a 3 canaux de couleur (RGB)\n# TECHNIQUE: Constante pour définir le nombre de canaux couleur (RGB = Rouge, Vert, Bleu)\n# ALTERNATIVES: Utiliser enum.Enum, dataclass, config.yaml, argparse\n# ENFANT: On dit à l'ordinateur que nos images ont 3 couleurs : rouge, vert et bleu (comme un arc-en-ciel)\n\ntransform_cifar = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # Normalisation pour 3 canaux\n])\n# TECHNIQUE: Pipeline de transformation avec Compose pour chaîner les opérations\n# ALTERNATIVES: transforms.v2, albumentations, kornia, transforms personnalisées, tf.data\n# ENFANT: C'est comme une recette : d'abord on redimensionne l'image, puis on la transforme en nombres, puis on l'ajuste\n\ncifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n# TECHNIQUE: Chargement automatique du dataset avec téléchargement et transformations\n# ALTERNATIVES: torchdata, datasets (HuggingFace), tf.data, chargement manuel avec PIL/cv2\n# ENFANT: On demande à l'ordinateur de télécharger des milliers d'images d'animaux et d'objets\n\ndataloader_cifar = DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True)\n# TECHNIQUE: DataLoader pour traitement par batch avec mélange aléatoire\n# ALTERNATIVES: torch.utils.data.distributed, tf.data.Dataset, ray.data, custom iterators\n# ENFANT: C'est comme prendre une poignée d'images au hasard dans un grand sac pour les montrer à notre IA\n\n# Visualisation d'un batch d'images CIFAR-10\nreal_batch_cifar = next(iter(dataloader_cifar))\n# TECHNIQUE: Extraction du premier batch avec next() et iter() pour inspection\n# ALTERNATIVES: enumerate(), for loop avec break, dataset[indices], random sampling\n# ENFANT: On prend la première poignée d'images pour les regarder\n\nplt.figure(figsize=(8, 8))\n# TECHNIQUE: Création d'une figure matplotlib avec taille spécifiée\n# ALTERNATIVES: plt.subplots(), fig.add_subplot(), seaborn, plotly, cv2.imshow()\n# ENFANT: On prépare une feuille de papier carrée pour dessiner dessus\n\nplt.axis(\"off\")\n# TECHNIQUE: Suppression des axes pour affichage propre des images\n# ALTERNATIVES: plt.xticks([]), plt.yticks([]), sns.despine(), ax.axis('off')\n# ENFANT: On enlève les lignes et les chiffres autour de notre dessin pour que ce soit plus joli\n\nplt.title(\"Batch d'images réelles de CIFAR-10\")\n# TECHNIQUE: Ajout d'un titre descriptif à la visualisation\n# ALTERNATIVES: ax.set_title(), fig.suptitle(), plt.text(), annotations personnalisées\n# ENFANT: On écrit un titre au-dessus de nos images comme dans un livre\n\ngrid = make_grid(real_batch_cifar[0][:64] * 0.5 + 0.5, padding=2, normalize=True)\n# TECHNIQUE: Création d'une grille d'images avec normalisation [-1,1] -> [0,1]\n# ALTERNATIVES: torchvision.utils.save_image(), custom grid, PIL.Image.thumbnail(), cv2.hconcat()\n# ENFANT: On arrange nos images en carré comme des cartes sur une table, et on les rend plus jolies\n\nplt.imshow(np.transpose(grid.cpu(), (1, 2, 0)))\n# TECHNIQUE: Transposition des dimensions (C,H,W) -> (H,W,C) pour matplotlib\n# ALTERNATIVES: einops.rearrange(), torch.permute(), PIL conversion, cv2.cvtColor()\n# ENFANT: On change l'ordre des couleurs pour que l'ordinateur puisse bien afficher l'image\n\nplt.show()\n# TECHNIQUE: Affichage de la figure dans l'interface\n# ALTERNATIVES: plt.savefig(), plt.display(), wandb.log(), tensorboard\n# ENFANT: On montre notre belle image à l'écran\n\n# --- 2.2. (CORRIGÉ) Création des nouveaux modèles et transfert des poids ---\ngenerator_cifar = Generator(latent_dim, image_channels_cifar).to(device)\n# TECHNIQUE: Instanciation du générateur pour CIFAR-10 avec transfert sur GPU/CPU\n# ALTERNATIVES: nn.DataParallel(), model.cuda(), lazy initialization, factory pattern\n# ENFANT: On crée un nouveau \"dessinateur\" spécialisé pour les images en couleur\n\ndiscriminator_cifar = Discriminator(image_channels_cifar).to(device)\n# TECHNIQUE: Instanciation du discriminateur pour CIFAR-10 avec transfert sur GPU/CPU\n# ALTERNATIVES: nn.DataParallel(), model.cuda(), lazy initialization, factory pattern\n# ENFANT: On crée un nouveau \"détective\" spécialisé pour reconnaître les vraies images en couleur\n\ngen_mnist_weights = torch.load(\"models/generator_mnist.pth\", map_location=device)\n# TECHNIQUE: Chargement des poids sauvegardés avec map_location pour compatibilité GPU/CPU\n# ALTERNATIVES: pickle.load(), joblib.load(), safetensors, checkpoint avec metadata\n# ENFANT: On récupère la mémoire de notre ancien \"dessinateur\" qui savait dessiner des chiffres\n\ndisc_mnist_weights = torch.load(\"models/discriminator_mnist.pth\", map_location=device)\n# TECHNIQUE: Chargement des poids sauvegardés avec map_location pour compatibilité GPU/CPU\n# ALTERNATIVES: pickle.load(), joblib.load(), safetensors, checkpoint avec metadata\n# ENFANT: On récupère la mémoire de notre ancien \"détective\" qui savait reconnaître des chiffres\n\n# Filtrage pour le Générateur\ngen_cifar_dict = generator_cifar.state_dict()\n# TECHNIQUE: Récupération du dictionnaire d'état du modèle (nom_paramètre -> tensor)\n# ALTERNATIVES: model.parameters(), model.named_parameters(), custom state management\n# ENFANT: On regarde la liste de tous les \"muscles\" (paramètres) de notre nouveau dessinateur\n\npretrained_gen_dict = {k: v for k, v in gen_mnist_weights.items() if k in gen_cifar_dict and gen_cifar_dict[k].size() == v.size()}\n# TECHNIQUE: Filtrage par compréhension de dictionnaire avec vérification de compatibilité des tailles\n# ALTERNATIVES: set intersection, try/except loading, manual layer matching, regex filtering\n# ENFANT: On regarde quels \"muscles\" de l'ancien dessinateur peuvent s'adapter au nouveau (même taille)\n\ngen_cifar_dict.update(pretrained_gen_dict)\n# TECHNIQUE: Mise à jour du dictionnaire d'état avec les poids compatibles\n# ALTERNATIVES: manual assignment, torch.nn.utils.prune, selective loading\n# ENFANT: On donne au nouveau dessinateur les \"muscles\" qu'on peut récupérer de l'ancien\n\ngenerator_cifar.load_state_dict(gen_cifar_dict)\n# TECHNIQUE: Chargement du state_dict modifié dans le modèle\n# ALTERNATIVES: load_state_dict(strict=False), manual parameter assignment, hooks\n# ENFANT: Le nouveau dessinateur apprend tout ce qu'il peut de l'ancien\n\nprint(f\"Transfert de {len(pretrained_gen_dict)}/{len(gen_cifar_dict)} couches du Générateur MNIST vers CIFAR-10.\")\n# TECHNIQUE: Logging informatif avec f-string pour le debugging\n# ALTERNATIVES: logging.info(), wandb.log(), tensorboard, print avec format()\n# ENFANT: On compte combien de \"muscles\" on a pu transférer et on l'écrit\n\n# Filtrage pour le Discriminateur\ndisc_cifar_dict = discriminator_cifar.state_dict()\n# TECHNIQUE: Récupération du dictionnaire d'état du discriminateur\n# ALTERNATIVES: model.parameters(), model.named_parameters(), custom state management\n# ENFANT: On regarde la liste de tous les \"muscles\" de notre nouveau détective\n\npretrained_disc_dict = {k: v for k, v in disc_mnist_weights.items() if k in disc_cifar_dict and disc_cifar_dict[k].size() == v.size()}\n# TECHNIQUE: Filtrage par compréhension avec vérification de compatibilité\n# ALTERNATIVES: set intersection, try/except loading, manual layer matching\n# ENFANT: On regarde quels \"muscles\" de l'ancien détective peuvent s'adapter au nouveau\n\ndisc_cifar_dict.update(pretrained_disc_dict)\n# TECHNIQUE: Mise à jour du dictionnaire avec les poids compatibles\n# ALTERNATIVES: manual assignment, selective loading, parameter surgery\n# ENFANT: On donne au nouveau détective les \"muscles\" récupérables de l'ancien\n\ndiscriminator_cifar.load_state_dict(disc_cifar_dict)\n# TECHNIQUE: Chargement du state_dict dans le modèle discriminateur\n# ALTERNATIVES: load_state_dict(strict=False), manual loading, progressive loading\n# ENFANT: Le nouveau détective apprend tout ce qu'il peut de l'ancien\n\nprint(f\"Transfert de {len(pretrained_disc_dict)}/{len(disc_cifar_dict)} couches du Discriminateur MNIST vers CIFAR-10.\")\n# TECHNIQUE: Logging informatif du transfert pour le discriminateur\n# ALTERNATIVES: logging with levels, structured logging, metrics tracking\n# ENFANT: On compte et on écrit combien de \"muscles\" du détective ont été transférés\n\n# --- 2.3. Gel (Freeze) des couches transférées ---\ndef print_trainable_status(model, model_name):\n    # TECHNIQUE: Fonction utilitaire pour inspection des paramètres entraînables\n    # ALTERNATIVES: torchinfo.summary(), model hooks, custom introspection, wandb.watch()\n    # ENFANT: On crée une fonction qui nous dit quels \"muscles\" peuvent encore apprendre\n    \n    print(f\"\\nStatut des paramètres pour {model_name}:\")\n    # TECHNIQUE: Logging avec formatage pour clarté\n    # ALTERNATIVES: tabulate, rich.table, pandas.DataFrame, structured output\n    # ENFANT: On écrit le nom du modèle qu'on va examiner\n    \n    total_params = 0\n    trainable_params = 0\n    # TECHNIQUE: Compteurs pour statistiques des paramètres\n    # ALTERNATIVES: sum() avec generator, collections.Counter, numpy operations\n    # ENFANT: On prépare deux compteurs : un pour tous les \"muscles\", un pour ceux qui apprennent\n    \n    for name, param in model.named_parameters():\n        # TECHNIQUE: Itération sur les paramètres nommés du modèle\n        # ALTERNATIVES: model.parameters(), recursive parameter extraction, hooks\n        # ENFANT: On regarde chaque \"muscle\" du modèle un par un\n        \n        total_params += param.numel()\n        # TECHNIQUE: Comptage du nombre d'éléments dans le tenseur\n        # ALTERNATIVES: param.size().numel(), torch.numel(), manual calculation\n        # ENFANT: On compte combien de petites parties a ce \"muscle\"\n        \n        if param.requires_grad:\n            trainable_params += param.numel()\n        # TECHNIQUE: Comptage conditionnel des paramètres entraînables\n        # ALTERNATIVES: filter() avec lambda, list comprehension, boolean indexing\n        # ENFANT: Si ce \"muscle\" peut encore apprendre, on l'ajoute à notre compteur spécial\n        \n        print(f\"{name:<40} Entraînable: {param.requires_grad}\")\n        # TECHNIQUE: Formatage aligné avec padding pour lisibilité\n        # ALTERNATIVES: f-string avec format specs, str.format(), tabulate\n        # ENFANT: On écrit le nom du \"muscle\" et si il peut apprendre (Oui/Non)\n    \n    print(f\"Paramètres entraînables : {trainable_params} / {total_params} ({100 * trainable_params / total_params:.2f}%)\")\n    # TECHNIQUE: Calcul et affichage de statistiques avec formatage décimal\n    # ALTERNATIVES: f-string formatting, round(), numpy.round(), percentage calculation libs\n    # ENFANT: On calcule et on écrit combien de \"muscles\" peuvent apprendre sur le total\n\nlayers_to_freeze_g = 8 # Gelons les 4 premiers blocs\n# TECHNIQUE: Configuration du nombre de couches à geler (commentaire incorrect : 8 ≠ 4)\n# ALTERNATIVES: config file, argparse, percentage-based freezing, layer name patterns\n# ENFANT: On décide combien de \"muscles\" du dessinateur on va \"endormir\" pour qu'ils n'apprennent plus\n\nlayers_to_freeze_d = 8 # Gelons les 4 premiers blocs\n# TECHNIQUE: Configuration pour le discriminateur (même problème de commentaire)\n# ALTERNATIVES: dynamic freezing, progressive unfreezing, layer-wise learning rates\n# ENFANT: On décide combien de \"muscles\" du détective on va \"endormir\"\n\nfor i, (name, param) in enumerate(generator_cifar.named_parameters()):\n    if name in pretrained_gen_dict: # Une façon plus robuste de geler que de compter\n        param.requires_grad = False\n# TECHNIQUE: Gel basé sur la présence dans le dictionnaire pré-entraîné (plus robuste)\n# ALTERNATIVES: regex matching, layer indexing, parameter grouping, hooks\n# ENFANT: Pour chaque \"muscle\" du dessinateur, si il vient de l'ancien, on l'endort\n\nfor i, (name, param) in enumerate(discriminator_cifar.named_parameters()):\n     if name in pretrained_disc_dict:\n        param.requires_grad = False\n# TECHNIQUE: Gel basé sur la présence dans le dictionnaire (même méthode)\n# ALTERNATIVES: functional approach with map(), parameter groups, selective freezing\n# ENFANT: Pour chaque \"muscle\" du détective, si il vient de l'ancien, on l'endort\n\nprint_trainable_status(generator_cifar, \"Générateur CIFAR-10\")\n# TECHNIQUE: Appel de fonction pour diagnostic du générateur\n# ALTERNATIVES: direct inspection, logging hooks, visualization tools\n# ENFANT: On demande à notre fonction de nous dire l'état des \"muscles\" du dessinateur\n\nprint_trainable_status(discriminator_cifar, \"Discriminateur CIFAR-10\")\n# TECHNIQUE: Appel de fonction pour diagnostic du discriminateur\n# ALTERNATIVES: combined reporting, comparative analysis, automated testing\n# ENFANT: On demande l'état des \"muscles\" du détective aussi\n\n# --- 2.4. Fine-Tuning sur CIFAR-10 ---\nparams_g_finetune = filter(lambda p: p.requires_grad, generator_cifar.parameters())\n# TECHNIQUE: Filtrage fonctionnel des paramètres entraînables avec lambda\n# ALTERNATIVES: list comprehension, [p for p in params if p.requires_grad], parameter groups\n# ENFANT: On fait une liste de tous les \"muscles\" du dessinateur qui peuvent encore apprendre\n\nparams_d_finetune = filter(lambda p: p.requires_grad, discriminator_cifar.parameters())\n# TECHNIQUE: Même filtrage pour le discriminateur\n# ALTERNATIVES: generator expression, manual parameter collection, grouped parameters\n# ENFANT: On fait une liste des \"muscles\" du détective qui peuvent encore apprendre\n\noptimizer_G_cifar = optim.Adam(params_g_finetune, lr=lr_finetune, betas=betas)\n# TECHNIQUE: Optimisateur Adam avec paramètres personnalisés et taux d'apprentissage spécifique\n# ALTERNATIVES: SGD, RMSprop, AdamW, RAdam, custom schedulers, parameter groups\n# ENFANT: On crée un \"professeur\" spécialisé pour enseigner au dessinateur avec une vitesse d'apprentissage lente\n\noptimizer_D_cifar = optim.Adam(params_d_finetune, lr=lr_finetune, betas=betas)\n# TECHNIQUE: Optimisateur séparé pour le discriminateur\n# ALTERNATIVES: shared optimizer, different optimizers (TTUR), parameter scheduling\n# ENFANT: On crée un \"professeur\" pour le détective aussi, avec la même vitesse lente\n\nprint(\"\\nDébut du Fine-Tuning sur CIFAR-10...\")\n# TECHNIQUE: Message informatif de début d'entraînement\n# ALTERNATIVES: logging with timestamps, progress bars, structured logging\n# ENFANT: On annonce qu'on commence l'école spécialisée pour nos deux IA\n\nfor epoch in range(epochs_finetune):\n    # TECHNIQUE: Boucle d'entraînement par époques\n    # ALTERNATIVES: while loops, custom iterators, early stopping, dynamic epochs\n    # ENFANT: On répète l'école plusieurs fois (comme plusieurs années scolaires)\n    \n    for i, (real_images, _) in enumerate(dataloader_cifar):\n        # TECHNIQUE: Boucle sur les batches avec énumération et destructuring\n        # ALTERNATIVES: while with iterator, batch generators, parallel loading\n        # ENFANT: Pour chaque poignée d'images, on va faire une leçon\n        \n        real_images = real_images.to(device)\n        # TECHNIQUE: Transfert des données sur le device (GPU/CPU)\n        # ALTERNATIVES: pin_memory, non_blocking transfers, automatic mixed precision\n        # ENFANT: On met les images sur le bon ordinateur (rapide ou lent)\n        \n        real_labels = torch.ones(real_images.size(0), device=device)\n        # TECHNIQUE: Création de labels \"vrai\" (1) pour les vraies images\n        # ALTERNATIVES: torch.full(), manual tensor creation, label smoothing\n        # ENFANT: On crée des étiquettes qui disent \"VRAI\" pour les vraies images\n        \n        fake_labels = torch.zeros(real_images.size(0), device=device)\n        # TECHNIQUE: Création de labels \"faux\" (0) pour les images générées\n        # ALTERNATIVES: torch.full(), -torch.ones(), soft labels\n        # ENFANT: On crée des étiquettes qui disent \"FAUX\" pour les fausses images\n\n        # Entraînement du Discriminateur\n        optimizer_D_cifar.zero_grad()\n        # TECHNIQUE: Remise à zéro des gradients accumulés\n        # ALTERNATIVES: set_to_none=True, gradient clipping, gradient accumulation\n        # ENFANT: On efface les anciennes leçons du détective pour qu'il puisse apprendre du nouveau\n        \n        d_output_real = discriminator_cifar(real_images)\n        # TECHNIQUE: Forward pass du discriminateur sur vraies images\n        # ALTERNATIVES: model.forward(), functional calls, hooks for intermediate outputs\n        # ENFANT: Le détective regarde les vraies images et dit ce qu'il pense\n        \n        errD_real = criterion(d_output_real, real_labels)\n        # TECHNIQUE: Calcul de la loss entre prédiction et labels vrais\n        # ALTERNATIVES: custom loss functions, weighted losses, focal loss\n        # ENFANT: On regarde à quel point le détective s'est trompé sur les vraies images\n        \n        errD_real.backward()\n        # TECHNIQUE: Rétropropagation pour calculer les gradients\n        # ALTERNATIVES: manual gradients, retain_graph=True, create_graph=True\n        # ENFANT: On calcule comment le détective doit changer pour mieux reconnaître les vraies images\n        \n        noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n        # TECHNIQUE: Génération de bruit aléatoire gaussien pour le générateur\n        # ALTERNATIVES: uniform noise, learned noise, conditional noise, different distributions\n        # ENFANT: On crée du \"bruit magique\" aléatoire pour que le dessinateur puisse créer des images\n        \n        fake_images = generator_cifar(noise)\n        # TECHNIQUE: Génération d'images à partir du bruit\n        # ALTERNATIVES: conditional generation, progressive generation, cached generation\n        # ENFANT: Le dessinateur transforme le bruit magique en fausses images\n        \n        d_output_fake = discriminator_cifar(fake_images.detach())\n        # TECHNIQUE: Évaluation des fausses images avec .detach() pour éviter les gradients du générateur\n        # ALTERNATIVES: torch.no_grad(), stop_gradient, separate forward passes\n        # ENFANT: Le détective regarde les fausses images, mais on ne veut pas que ça influence le dessinateur\n        \n        errD_fake = criterion(d_output_fake, fake_labels)\n        # TECHNIQUE: Calcul de la loss sur les fausses images\n        # ALTERNATIVES: different loss weighting, adversarial losses, custom metrics\n        # ENFANT: On regarde à quel point le détective s'est trompé sur les fausses images\n        \n        errD_fake.backward()\n        # TECHNIQUE: Rétropropagation pour les fausses images\n        # ALTERNATIVES: accumulated gradients, gradient penalty, spectral normalization\n        # ENFANT: On calcule comment le détective doit changer pour mieux reconnaître les fausses images\n        \n        errD = errD_real + errD_fake\n        # TECHNIQUE: Combinaison des deux losses du discriminateur\n        # ALTERNATIVES: weighted combination, separate optimization, alternating updates\n        # ENFANT: On additionne les deux erreurs du détective pour avoir son erreur totale\n        \n        optimizer_D_cifar.step()\n        # TECHNIQUE: Mise à jour des paramètres du discriminateur\n        # ALTERNATIVES: gradient clipping, learning rate scheduling, momentum updates\n        # ENFANT: Le \"professeur\" applique les corrections au détective\n\n        # Entraînement du Générateur\n        optimizer_G_cifar.zero_grad()\n        # TECHNIQUE: Reset des gradients pour le générateur\n        # ALTERNATIVES: shared optimizer management, gradient accumulation strategies\n        # ENFANT: On efface les anciennes leçons du dessinateur\n        \n        d_output_on_fake = discriminator_cifar(fake_images)\n        # TECHNIQUE: Évaluation des images générées par le discriminateur (sans detach)\n        # ALTERNATIVES: cached discriminator outputs, multiple discriminator calls\n        # ENFANT: Le détective regarde les nouvelles images du dessinateur\n        \n        errG = criterion(d_output_on_fake, real_labels)\n        # TECHNIQUE: Loss du générateur : il veut que ses fausses images soient classées comme vraies\n        # ALTERNATIVES: feature matching, perceptual loss, WGAN loss, custom adversarial objectives\n        # ENFANT: On regarde si le dessinateur arrive à tromper le détective (c'est le but !)\n        \n        errG.backward()\n        # TECHNIQUE: Rétropropagation pour le générateur\n        # ALTERNATIVES: gradient penalties, progressive growing, spectral normalization\n        # ENFANT: On calcule comment le dessinateur doit changer pour mieux tromper le détective\n        \n        optimizer_G_cifar.step()\n        # TECHNIQUE: Mise à jour des paramètres du générateur\n        # ALTERNATIVES: different learning rates, momentum, adaptive methods\n        # ENFANT: Le \"professeur\" applique les corrections au dessinateur\n\n    print(f\"[Epoch CIFAR-10 {epoch+1}/{epochs_finetune}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}\")\n    # TECHNIQUE: Logging des métriques avec formatage décimal\n    # ALTERNATIVES: wandb.log(), tensorboard, csv logging, structured metrics\n    # ENFANT: À la fin de chaque \"année scolaire\", on écrit les notes du détective et du dessinateur\n    \n    with torch.no_grad():\n        fake_samples = generator_cifar(fixed_noise).detach().cpu()\n    # TECHNIQUE: Génération d'échantillons sans calcul de gradients pour économiser la mémoire\n    # ALTERNATIVES: eval() mode, inference mode, separate inference function\n    # ENFANT: On demande au dessinateur de nous montrer ses progrès sans que ça compte pour ses notes\n    \n    save_image(fake_samples, f\"cifar10_images/cifar10_fake_epoch_{epoch+1}.png\", normalize=True)\n    # TECHNIQUE: Sauvegarde d'images avec normalisation automatique\n    # ALTERNATIVES: PIL.Image.save(), cv2.imwrite(), custom visualization, wandb.Image()\n    # ENFANT: On sauvegarde les dessins du dessinateur dans un dossier pour les regarder plus tard\n\nprint(\"--- FIN DE LA PHASE 2 : Fine-tuning terminé. ---\")\n# TECHNIQUE: Message de fin d'entraînement\n# ALTERNATIVES: logging with execution time, summary statistics, model checkpointing\n# ENFANT: On annonce que l'école spécialisée est finie !\n\n# --- 2.5. Génération et Visualisation finale ---\ngenerator_cifar.eval()\n# TECHNIQUE: Passage en mode évaluation (désactive dropout, batch norm en mode inference)\n# ALTERNATIVES: torch.inference_mode(), context managers, functional evaluation\n# ENFANT: On dit au dessinateur de passer en \"mode examen\" où il fait de son mieux\n\nwith torch.no_grad():\n    # TECHNIQUE: Context manager pour désactiver le calcul de gradients\n    # ALTERNATIVES: @torch.no_grad() decorator, torch.inference_mode(), manual gradient disabling\n    # ENFANT: On dit à l'ordinateur de ne pas prendre de notes pendant que le dessinateur travaille\n    \n    final_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n    # TECHNIQUE: Génération de bruit pour 64 images finales\n    # ALTERNATIVES: fixed seed noise, interpolated noise, conditional noise\n    # ENFANT: On prépare 64 pots de \"bruit magique\" différents\n    \n    generated_images = generator_cifar(final_noise).detach().cpu()\n    # TECHNIQUE: Génération finale avec transfert CPU et détachement du graphe\n    # ALTERNATIVES: batch processing, streaming generation, memory-efficient generation\n    # ENFANT: Le dessinateur transforme tous les pots de bruit en 64 belles images\n\nplt.figure(figsize=(10, 10))\n# TECHNIQUE: Création d'une grande figure pour la visualisation finale\n# ALTERNATIVES: subplots, interactive plots, save without display\n# ENFANT: On prépare une très grande feuille pour montrer tous les dessins\n\nplt.axis(\"off\")\n# TECHNIQUE: Suppression des axes pour un affichage propre\n# ALTERNATIVES: custom styling, seaborn styling, matplotlib themes\n# ENFANT: On enlève les lignes et chiffres pour que ce soit plus joli\n\nplt.title(\"Images CIFAR-10 finales générées par Transfer Learning\")\n# TECHNIQUE: Titre descriptif de la visualisation finale\n# ALTERNATIVES: custom text positioning, multi-line titles, styled titles\n# ENFANT: On écrit un beau titre qui explique ce qu'on regarde\n\nplt.imshow(np.transpose(make_grid(generated_images, padding=2, normalize=True), (1, 2, 0)))\n# TECHNIQUE: Affichage d'une grille d'images avec transposition des dimensions\n# ALTERNATIVES: custom grid layouts, subplots, interactive galleries\n# ENFANT: On arrange toutes les images en carré et on les montre sur notre grande feuille\n\nplt.show()\n# TECHNIQUE: Affichage final de la visualisation\n# ALTERNATIVES: plt.savefig(), interactive display, web-based visualization\n# ENFANT: On montre notre magnifique galerie d'art créée par notre IA !","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:02:49.934982Z","iopub.execute_input":"2025-06-09T17:02:49.935443Z","iopub.status.idle":"2025-06-09T17:11:18.536348Z","shell.execute_reply.started":"2025-06-09T17:02:49.935422Z","shell.execute_reply":"2025-06-09T17:11:18.535282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Installation des dépendances pour Kaggle\n!pip install pytorch-fid psutil\n\n# Variables à définir dans votre notebook\nlatent_dim = 100\nimage_size = 64\nlr_finetune = 0.0001\nepochs_finetune = 50\nbatch_size = 64\nbetas = (0.5, 0.999)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncriterion = nn.BCELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:22:12.898570Z","iopub.execute_input":"2025-06-10T12:22:12.898915Z","iopub.status.idle":"2025-06-10T12:23:23.874800Z","shell.execute_reply.started":"2025-06-10T12:22:12.898891Z","shell.execute_reply":"2025-06-10T12:23:23.873534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# PHASE 2 : TRANSFERT D'APPRENTISSAGE ET FINE-TUNING SUR CIFAR-10 (CIBLE)\n# Version Enterprise avec Model Selection et Monitoring Industriel\n# =========================================================================\n\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nimport gc\nimport psutil\nimport numpy as np\nfrom collections import defaultdict, deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration industrielle centralisée\nclass ModelConfig:\n    \"\"\"Configuration centralisée pour déploiement industriel\"\"\"\n    def __init__(self):\n        self.experiment_name = f\"cifar10_transfer_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.model_registry_path = Path(\"model_registry\")\n        self.metrics_path = Path(\"metrics\")\n        self.artifacts_path = Path(\"artifacts\")\n        \n        # Création des dossiers\n        for path in [self.model_registry_path, self.metrics_path, self.artifacts_path]:\n            path.mkdir(exist_ok=True)\n        \n        # Métriques de monitoring\n        self.metrics_to_track = ['fid_score', 'inception_score', 'lpips_distance', 'model_size', 'inference_time']\n        self.early_stopping_patience = 10\n        self.model_selection_metric = 'fid_score'  # Plus bas = meilleur\n\nconfig = ModelConfig()\n\n# Classe de métriques industrielles\nclass GANMetrics:\n    \"\"\"Calcul de métriques industrielles pour évaluation GAN\"\"\"\n    \n    def __init__(self, device):\n        self.device = device\n        self.fid_calculator = None\n        self.inception_model = None\n        self._init_metrics_models()\n    \n    def _init_metrics_models(self):\n        \"\"\"Initialisation des modèles pour calcul de métriques\"\"\"\n        try:\n            # FID Calculator (Fréchet Inception Distance)\n            from pytorch_fid import fid_score\n            from torchvision.models import inception_v3\n            \n            self.inception_model = inception_v3(pretrained=True, transform_input=False).to(self.device)\n            self.inception_model.eval()\n            print(\"✓ Inception model loaded for FID/IS calculation\")\n            \n        except ImportError:\n            print(\"⚠ pytorch-fid not available. Install with: pip install pytorch-fid\")\n            print(\"⚠ Using alternative metrics calculation\")\n    \n    def calculate_fid_score(self, real_images, fake_images):\n        \"\"\"Calcul du FID score (plus bas = meilleur)\"\"\"\n        try:\n            # Conversion en format approprié pour FID\n            real_imgs = (real_images * 255).clamp(0, 255).byte()\n            fake_imgs = (fake_images * 255).clamp(0, 255).byte()\n            \n            # Calcul simplifié du FID (approximation pour Kaggle)\n            real_features = self._extract_inception_features(real_imgs)\n            fake_features = self._extract_inception_features(fake_imgs)\n            \n            mu1, sigma1 = real_features.mean(0), np.cov(real_features.T)\n            mu2, sigma2 = fake_features.mean(0), np.cov(fake_features.T)\n            \n            fid = self._calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n            return float(fid)\n            \n        except Exception as e:\n            print(f\"⚠ FID calculation failed: {e}\")\n            return float('inf')\n    \n    def _extract_inception_features(self, images):\n        \"\"\"Extraction de features Inception\"\"\"\n        with torch.no_grad():\n            if len(images.shape) == 3:\n                images = images.unsqueeze(0)\n            \n            # Resize to 299x299 for Inception\n            images = torch.nn.functional.interpolate(images.float(), size=(299, 299), mode='bilinear')\n            images = images.to(self.device)\n            \n            features = self.inception_model(images)\n            return features.cpu().numpy()\n    \n    def _calculate_frechet_distance(self, mu1, sigma1, mu2, sigma2):\n        \"\"\"Calcul de la distance de Fréchet\"\"\"\n        diff = mu1 - mu2\n        covmean = np.sqrt(sigma1.dot(sigma2))\n        \n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n        \n        return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n    \n    def calculate_inception_score(self, fake_images, splits=10):\n        \"\"\"Calcul de l'Inception Score (plus haut = meilleur)\"\"\"\n        try:\n            with torch.no_grad():\n                fake_imgs = (fake_images * 255).clamp(0, 255).byte()\n                \n                # Resize pour Inception\n                fake_imgs = torch.nn.functional.interpolate(fake_imgs.float(), size=(299, 299), mode='bilinear')\n                fake_imgs = fake_imgs.to(self.device)\n                \n                probs = torch.nn.functional.softmax(self.inception_model(fake_imgs), dim=1)\n                \n                scores = []\n                for i in range(splits):\n                    part = probs[i * len(probs) // splits:(i + 1) * len(probs) // splits]\n                    kl_div = part * (torch.log(part) - torch.log(part.mean(0, keepdim=True)))\n                    kl_div = kl_div.sum(1).mean().exp()\n                    scores.append(kl_div.item())\n                \n                return np.mean(scores), np.std(scores)\n                \n        except Exception as e:\n            print(f\"⚠ IS calculation failed: {e}\")\n            return 1.0, 0.0\n\n# Classe de monitoring industriel\nclass ModelMonitor:\n    \"\"\"Monitoring industriel des performances et ressources\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.metrics_history = defaultdict(list)\n        self.resource_history = defaultdict(deque)\n        self.best_models = {}\n        self.early_stopping_counter = 0\n        self.best_score = float('inf')\n        \n    def log_metrics(self, epoch, metrics_dict):\n        \"\"\"Logging des métriques avec timestamp\"\"\"\n        timestamp = datetime.now().isoformat()\n        \n        log_entry = {\n            'timestamp': timestamp,\n            'epoch': epoch,\n            **metrics_dict\n        }\n        \n        # Sauvegarde des métriques\n        for key, value in metrics_dict.items():\n            self.metrics_history[key].append(value)\n        \n        # Sauvegarde JSON pour analyse\n        metrics_file = self.config.metrics_path / f\"{self.config.experiment_name}_metrics.jsonl\"\n        with open(metrics_file, 'a') as f:\n            f.write(json.dumps(log_entry) + '\\n')\n        \n        print(f\"📊 Epoch {epoch}: {metrics_dict}\")\n    \n    def monitor_resources(self):\n        \"\"\"Monitoring des ressources système\"\"\"\n        cpu_percent = psutil.cpu_percent()\n        memory = psutil.virtual_memory()\n        \n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB\n            gpu_cached = torch.cuda.memory_reserved() / 1024**3   # GB\n        else:\n            gpu_memory = gpu_cached = 0\n        \n        resources = {\n            'cpu_percent': cpu_percent,\n            'memory_used_gb': memory.used / 1024**3,\n            'memory_available_gb': memory.available / 1024**3,\n            'gpu_memory_gb': gpu_memory,\n            'gpu_cached_gb': gpu_cached\n        }\n        \n        # Garde seulement les 100 dernières mesures\n        for key, value in resources.items():\n            self.resource_history[key].append(value)\n            if len(self.resource_history[key]) > 100:\n                self.resource_history[key].popleft()\n        \n        return resources\n    \n    def check_early_stopping(self, current_score):\n        \"\"\"Early stopping basé sur la métrique principale\"\"\"\n        if current_score < self.best_score:\n            self.best_score = current_score\n            self.early_stopping_counter = 0\n            return False\n        else:\n            self.early_stopping_counter += 1\n            return self.early_stopping_counter >= self.config.early_stopping_patience\n    \n    def save_model_checkpoint(self, generator, discriminator, epoch, metrics, is_best=False):\n        \"\"\"Sauvegarde de checkpoint avec métadonnées\"\"\"\n        checkpoint_dir = self.config.model_registry_path / f\"epoch_{epoch}\"\n        checkpoint_dir.mkdir(exist_ok=True)\n        \n        # Métadonnées du modèle\n        metadata = {\n            'epoch': epoch,\n            'timestamp': datetime.now().isoformat(),\n            'metrics': metrics,\n            'model_size_mb': self._get_model_size(generator) + self._get_model_size(discriminator),\n            'config': {\n                'latent_dim': latent_dim,\n                'image_channels': image_channels_cifar,\n                'learning_rate': lr_finetune\n            }\n        }\n        \n        # Sauvegarde des modèles\n        torch.save(generator.state_dict(), checkpoint_dir / \"generator.pth\")\n        torch.save(discriminator.state_dict(), checkpoint_dir / \"discriminator.pth\")\n        \n        # Sauvegarde des métadonnées\n        with open(checkpoint_dir / \"metadata.json\", 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        # Marquage du meilleur modèle\n        if is_best:\n            best_model_path = self.config.model_registry_path / \"best_model\"\n            if best_model_path.exists():\n                import shutil\n                shutil.rmtree(best_model_path)\n            shutil.copytree(checkpoint_dir, best_model_path)\n            print(f\"🏆 New best model saved (FID: {metrics.get('fid_score', 'N/A')})\")\n    \n    def _get_model_size(self, model):\n        \"\"\"Calcul de la taille du modèle en MB\"\"\"\n        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n        return (param_size + buffer_size) / 1024**2\n\n# Classe d'évaluation comparative\nclass ModelComparator:\n    \"\"\"Comparaison et sélection des meilleurs modèles\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.model_scores = {}\n        \n    def evaluate_model_portfolio(self):\n        \"\"\"Évaluation de tous les modèles sauvegardés\"\"\"\n        model_registry = self.config.model_registry_path\n        evaluation_results = []\n        \n        for model_dir in model_registry.glob(\"epoch_*\"):\n            if not (model_dir / \"metadata.json\").exists():\n                continue\n                \n            with open(model_dir / \"metadata.json\", 'r') as f:\n                metadata = json.load(f)\n            \n            evaluation_results.append({\n                'model_path': str(model_dir),\n                'epoch': metadata['epoch'],\n                'fid_score': metadata['metrics'].get('fid_score', float('inf')),\n                'inception_score': metadata['metrics'].get('inception_score', 0),\n                'model_size_mb': metadata['model_size_mb'],\n                'inference_time': metadata['metrics'].get('inference_time', 0)\n            })\n        \n        # Tri par métrique principale\n        evaluation_results.sort(key=lambda x: x['fid_score'])\n        \n        # Sauvegarde du leaderboard\n        leaderboard_path = self.config.artifacts_path / \"model_leaderboard.json\"\n        with open(leaderboard_path, 'w') as f:\n            json.dump(evaluation_results, f, indent=2)\n        \n        print(\"🏅 Model Leaderboard (Top 5):\")\n        for i, result in enumerate(evaluation_results[:5]):\n            print(f\"  {i+1}. Epoch {result['epoch']}: FID={result['fid_score']:.3f}, \"\n                  f\"Size={result['model_size_mb']:.1f}MB\")\n        \n        return evaluation_results\n\n# Code principal amélioré\nprint(\"\\n--- DÉBUT DE LA PHASE 2 : TRANSFERT INDUSTRIEL SUR CIFAR-10 ---\")\n\n# Initialisation des composants industriels\nmonitor = ModelMonitor(config)\nmetrics_calculator = GANMetrics(device)\ncomparator = ModelComparator(config)\n\n# --- 2.1. Préparation des données CIFAR-10 (inchangé) ---\nimage_channels_cifar = 3\ntransform_cifar = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ncifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\ndataloader_cifar = DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True)\n\n# Dataset de validation pour métriques\ncifar_val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\nval_dataloader = DataLoader(cifar_val_dataset, batch_size=64, shuffle=False)\n\n# Échantillon de vraies images pour calcul FID\nreal_samples_for_metrics = next(iter(val_dataloader))[0][:100]\n\nprint(f\"🎯 Experiment: {config.experiment_name}\")\nprint(f\"📁 Artifacts will be saved to: {config.artifacts_path}\")\n\n# --- 2.2. Transfert des poids (code existant) ---\ngenerator_cifar = Generator(latent_dim, image_channels_cifar).to(device)\ndiscriminator_cifar = Discriminator(image_channels_cifar).to(device)\n\n# ... (code de transfert existant) ...\n\n# --- 2.3. Fine-Tuning avec monitoring industriel ---\nparams_g_finetune = filter(lambda p: p.requires_grad, generator_cifar.parameters())\nparams_d_finetune = filter(lambda p: p.requires_grad, discriminator_cifar.parameters())\n\noptimizer_G_cifar = optim.Adam(params_g_finetune, lr=lr_finetune, betas=betas)\noptimizer_D_cifar = optim.Adam(params_d_finetune, lr=lr_finetune, betas=betas)\n\n# Learning rate scheduler industriel\nscheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G_cifar, 'min', patience=5, factor=0.5)\nscheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D_cifar, 'min', patience=5, factor=0.5)\n\nprint(f\"\\n🚀 Starting industrial fine-tuning...\")\nprint(f\"📊 Tracking metrics: {config.metrics_to_track}\")\n\n# Variables de tracking\ntraining_start_time = time.time()\nepoch_times = []\n\nfor epoch in range(epochs_finetune):\n    epoch_start_time = time.time()\n    \n    # Métriques d'époque\n    epoch_metrics = {\n        'loss_d': 0.0,\n        'loss_g': 0.0,\n        'loss_d_real': 0.0,\n        'loss_d_fake': 0.0\n    }\n    \n    generator_cifar.train()\n    discriminator_cifar.train()\n    \n    for i, (real_images, _) in enumerate(dataloader_cifar):\n        real_images = real_images.to(device)\n        batch_size_current = real_images.size(0)\n        \n        real_labels = torch.ones(batch_size_current, device=device)\n        fake_labels = torch.zeros(batch_size_current, device=device)\n\n        # --- Entraînement Discriminateur ---\n        optimizer_D_cifar.zero_grad()\n        \n        d_output_real = discriminator_cifar(real_images)\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n        \n        noise = torch.randn(batch_size_current, latent_dim, 1, 1, device=device)\n        fake_images = generator_cifar(noise)\n        d_output_fake = discriminator_cifar(fake_images.detach())\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n        \n        errD = errD_real + errD_fake\n        optimizer_D_cifar.step()\n\n        # --- Entraînement Générateur ---\n        optimizer_G_cifar.zero_grad()\n        d_output_on_fake = discriminator_cifar(fake_images)\n        errG = criterion(d_output_on_fake, real_labels)\n        errG.backward()\n        optimizer_G_cifar.step()\n        \n        # Accumulation des métriques\n        epoch_metrics['loss_d'] += errD.item()\n        epoch_metrics['loss_g'] += errG.item()\n        epoch_metrics['loss_d_real'] += errD_real.item()\n        epoch_metrics['loss_d_fake'] += errD_fake.item()\n        \n        # Monitoring des ressources (toutes les 50 itérations)\n        if i % 50 == 0:\n            resources = monitor.monitor_resources()\n            \n        # Nettoyage mémoire périodique\n        if i % 100 == 0:\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            gc.collect()\n    \n    # Calcul des moyennes d'époque\n    num_batches = len(dataloader_cifar)\n    for key in epoch_metrics:\n        epoch_metrics[key] /= num_batches\n    \n    epoch_time = time.time() - epoch_start_time\n    epoch_times.append(epoch_time)\n    \n    # --- Évaluation industrielle ---\n    if epoch % 2 == 0 or epoch == epochs_finetune - 1:  # Évaluation tous les 2 époques\n        generator_cifar.eval()\n        \n        with torch.no_grad():\n            # Génération d'échantillons pour métriques\n            eval_noise = torch.randn(100, latent_dim, 1, 1, device=device)\n            fake_samples = generator_cifar(eval_noise)\n            \n            # Calcul des métriques industrielles\n            start_inference = time.time()\n            fid_score = metrics_calculator.calculate_fid_score(\n                real_samples_for_metrics.to(device), \n                fake_samples\n            )\n            inference_time = time.time() - start_inference\n            \n            inception_score, inception_std = metrics_calculator.calculate_inception_score(fake_samples)\n            \n            # Métriques complètes\n            complete_metrics = {\n                **epoch_metrics,\n                'fid_score': fid_score,\n                'inception_score': inception_score,\n                'inception_std': inception_std,\n                'inference_time': inference_time,\n                'epoch_time': epoch_time,\n                'lr_g': optimizer_G_cifar.param_groups[0]['lr'],\n                'lr_d': optimizer_D_cifar.param_groups[0]['lr']\n            }\n            \n            # Logging industriel\n            monitor.log_metrics(epoch, complete_metrics)\n            \n            # Sauvegarde de checkpoint\n            is_best = fid_score < monitor.best_score\n            monitor.save_model_checkpoint(\n                generator_cifar, discriminator_cifar, \n                epoch, complete_metrics, is_best\n            )\n            \n            # Mise à jour des schedulers\n            scheduler_G.step(fid_score)\n            scheduler_D.step(fid_score)\n            \n            # Early stopping check\n            if monitor.check_early_stopping(fid_score):\n                print(f\"🛑 Early stopping triggered at epoch {epoch}\")\n                break\n            \n            # Sauvegarde d'images avec métriques\n            save_image(\n                fake_samples[:64], \n                config.artifacts_path / f\"epoch_{epoch}_fid_{fid_score:.3f}.png\", \n                normalize=True, nrow=8\n            )\n    \n    # Affichage des métriques\n    print(f\"[Epoch {epoch+1}/{epochs_finetune}] \"\n          f\"Loss_D: {epoch_metrics['loss_d']:.4f} \"\n          f\"Loss_G: {epoch_metrics['loss_g']:.4f} \"\n          f\"Time: {epoch_time:.1f}s\")\n\n# --- Analyse finale et sélection du meilleur modèle ---\ntraining_time = time.time() - training_start_time\n\nprint(f\"\\n🎯 Training completed in {training_time/3600:.2f} hours\")\nprint(f\"⚡ Average epoch time: {np.mean(epoch_times):.1f}s\")\n\n# Évaluation comparative finale\nbest_models = comparator.evaluate_model_portfolio()\n\n# Génération du rapport final\nfinal_report = {\n    'experiment_name': config.experiment_name,\n    'total_training_time_hours': training_time / 3600,\n    'total_epochs_trained': epoch + 1,\n    'best_model': best_models[0] if best_models else None,\n    'avg_epoch_time_seconds': np.mean(epoch_times),\n    'final_metrics': monitor.metrics_history,\n    'early_stopped': monitor.early_stopping_counter >= config.early_stopping_patience\n}\n\n# Sauvegarde du rapport\nreport_path = config.artifacts_path / \"final_report.json\"\nwith open(report_path, 'w') as f:\n    json.dump(final_report, f, indent=2, default=str)\n\nprint(f\"📋 Final report saved to: {report_path}\")\nprint(f\"🏆 Best model FID score: {best_models[0]['fid_score']:.3f}\")\n\n# --- Code de test pour Kaggle ---\ndef kaggle_test_suite():\n    \"\"\"Suite de tests spécifiques pour environnement Kaggle\"\"\"\n    print(\"\\n🧪 Running Kaggle-specific tests...\")\n    \n    # Test 1: Vérification des chemins et permissions\n    test_paths = [config.model_registry_path, config.metrics_path, config.artifacts_path]\n    for path in test_paths:\n        assert path.exists(), f\"Path {path} not accessible\"\n        # Test d'écriture\n        test_file = path / \"test_write.tmp\"\n        test_file.write_text(\"test\")\n        test_file.unlink()\n    print(\"✅ File system access test passed\")\n    \n    # Test 2: Vérification mémoire GPU/CPU\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        print(f\"✅ GPU Memory available: {gpu_memory:.1f} GB\")\n        assert gpu_memory > 10, \"Insufficient GPU memory for training\"\n    \n    # Test 3: Génération rapide\n    with torch.no_grad():\n        test_noise = torch.randn(4, latent_dim, 1, 1, device=device)\n        if 'generator_cifar' in locals():\n            test_images = generator_cifar(test_noise)\n            assert test_images.shape == (4, 3, image_size, image_size), \"Wrong output shape\"\n    print(\"✅ Model generation test passed\")\n    \n    # Test 4: Métriques calculation\n    if len(monitor.metrics_history) > 0:\n        latest_fid = monitor.metrics_history['fid_score'][-1]\n        assert latest_fid < 300, f\"FID score too high: {latest_fid}\"\n    print(\"✅ Metrics validation test passed\")\n    \n    print(\"🎉 All Kaggle tests passed!\")\n\n# Exécution des tests Kaggle\ntry:\n    kaggle_test_suite()\nexcept Exception as e:\n    print(f\"⚠ Kaggle test failed: {e}\")\n\nprint(\"\\n--- FIN DE LA PHASE 2 INDUSTRIELLE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:17:00.422331Z","iopub.execute_input":"2025-06-09T17:17:00.423186Z","iopub.status.idle":"2025-06-09T17:36:57.022236Z","shell.execute_reply.started":"2025-06-09T17:17:00.423154Z","shell.execute_reply":"2025-06-09T17:36:57.021583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# PHASE 2 : TRANSFERT INDUSTRIEL CORRIGÉ - Métriques robustes pour Kaggle\n# =========================================================================\n\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nimport gc\nimport psutil\nimport numpy as np\nfrom collections import defaultdict, deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Installation des dépendances manquantes pour Kaggle\ntry:\n    import torch\n    import torchvision\n    from torchvision import transforms, datasets\n    from torch.utils.data import DataLoader\n    import torch.nn as nn\n    import torch.optim as optim\n    from torchvision.utils import save_image, make_grid\n    import matplotlib.pyplot as plt\n    from scipy import linalg\n    from sklearn.metrics import accuracy_score\nexcept ImportError as e:\n    print(f\"Installing missing dependencies: {e}\")\n\n# Classe de métriques robustes pour Kaggle\nclass RobustGANMetrics:\n    \"\"\"Métriques GAN robustes adaptées à l'environnement Kaggle\"\"\"\n    \n    def __init__(self, device):\n        self.device = device\n        self.inception_model = None\n        self.classifier_model = None\n        self._init_lightweight_models()\n    \n    def _init_lightweight_models(self):\n        \"\"\"Initialisation de modèles légers pour métriques\"\"\"\n        try:\n            # Modèle léger pour classification (alternative à Inception)\n            from torchvision.models import resnet18\n            self.classifier_model = resnet18(pretrained=True)\n            self.classifier_model.eval()\n            self.classifier_model = self.classifier_model.to(self.device)\n            print(\"✓ ResNet18 loaded for lightweight metrics\")\n            \n        except Exception as e:\n            print(f\"⚠ Could not load pretrained models: {e}\")\n            print(\"📊 Using basic statistical metrics instead\")\n    \n    def calculate_lightweight_fid(self, real_images, fake_images):\n        \"\"\"Calcul FID simplifié et robuste\"\"\"\n        try:\n            # Vérification des inputs\n            if real_images.numel() == 0 or fake_images.numel() == 0:\n                return float('inf')\n            \n            # Normalisation des images\n            real_imgs = self._normalize_images(real_images)\n            fake_imgs = self._normalize_images(fake_images)\n            \n            # Si pas de modèle préentraîné, utiliser des statistiques simples\n            if self.classifier_model is None:\n                return self._calculate_statistical_distance(real_imgs, fake_imgs)\n            \n            # Extraction de features avec modèle léger\n            real_features = self._extract_resnet_features(real_imgs)\n            fake_features = self._extract_resnet_features(fake_imgs)\n            \n            # Calcul FID robuste avec gestion des cas limites\n            fid_score = self._compute_robust_fid(real_features, fake_features)\n            \n            # Validation du résultat\n            if np.isnan(fid_score) or np.isinf(fid_score):\n                print(\"⚠ FID calculation resulted in nan/inf, using fallback metric\")\n                return self._calculate_statistical_distance(real_imgs, fake_imgs)\n                \n            return float(fid_score)\n            \n        except Exception as e:\n            print(f\"⚠ FID calculation error: {e}\")\n            return self._calculate_statistical_distance(real_images, fake_images)\n    \n    def _normalize_images(self, images):\n        \"\"\"Normalisation robuste des images\"\"\"\n        if images.dim() == 3:\n            images = images.unsqueeze(0)\n        \n        # Clamp et normalisation\n        images = torch.clamp(images, -1, 1)\n        images = (images + 1) / 2  # [-1,1] -> [0,1]\n        \n        return images\n    \n    def _extract_resnet_features(self, images):\n        \"\"\"Extraction de features avec ResNet18\"\"\"\n        with torch.no_grad():\n            # Resize pour ResNet\n            if images.shape[-1] != 224:\n                images = torch.nn.functional.interpolate(\n                    images, size=(224, 224), mode='bilinear', align_corners=False\n                )\n            \n            # Forward pass jusqu'aux features\n            x = self.classifier_model.conv1(images)\n            x = self.classifier_model.bn1(x)\n            x = self.classifier_model.relu(x)\n            x = self.classifier_model.maxpool(x)\n            \n            x = self.classifier_model.layer1(x)\n            x = self.classifier_model.layer2(x)\n            x = self.classifier_model.layer3(x)\n            x = self.classifier_model.layer4(x)\n            \n            x = self.classifier_model.avgpool(x)\n            features = torch.flatten(x, 1)\n            \n            return features.cpu().numpy()\n    \n    def _compute_robust_fid(self, real_features, fake_features):\n        \"\"\"Calcul FID robuste avec gestion d'erreurs\"\"\"\n        try:\n            # Calcul des statistiques\n            mu1 = np.mean(real_features, axis=0)\n            mu2 = np.mean(fake_features, axis=0)\n            \n            sigma1 = np.cov(real_features, rowvar=False)\n            sigma2 = np.cov(fake_features, rowvar=False)\n            \n            # Gestion des matrices singulières\n            if sigma1.ndim == 0:\n                sigma1 = np.array([[sigma1]])\n            if sigma2.ndim == 0:\n                sigma2 = np.array([[sigma2]])\n            \n            # Ajout de régularisation pour éviter les matrices singulières\n            eps = 1e-6\n            sigma1 += eps * np.eye(sigma1.shape[0])\n            sigma2 += eps * np.eye(sigma2.shape[0])\n            \n            # Calcul de la distance de Fréchet\n            diff = mu1 - mu2\n            \n            # Calcul de la racine carrée de la matrice\n            covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n            \n            # Gestion des nombres complexes\n            if np.iscomplexobj(covmean):\n                if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n                    print(\"⚠ Warning: Imaginary component in covariance mean\")\n                covmean = covmean.real\n            \n            # Calcul final du FID\n            fid = (diff.dot(diff) + np.trace(sigma1) + \n                   np.trace(sigma2) - 2 * np.trace(covmean))\n            \n            return fid\n            \n        except Exception as e:\n            print(f\"⚠ Robust FID calculation failed: {e}\")\n            return float('inf')\n    \n    def _calculate_statistical_distance(self, real_images, fake_images):\n        \"\"\"Métrique de fallback basée sur les statistiques d'images\"\"\"\n        try:\n            # Conversion en numpy\n            real_np = real_images.detach().cpu().numpy()\n            fake_np = fake_images.detach().cpu().numpy()\n            \n            # Statistiques de base\n            real_mean = np.mean(real_np)\n            fake_mean = np.mean(fake_np)\n            real_std = np.std(real_np)\n            fake_std = np.std(fake_np)\n            \n            # Distance statistique simple\n            mean_diff = abs(real_mean - fake_mean)\n            std_diff = abs(real_std - fake_std)\n            \n            # Score combiné (plus bas = meilleur)\n            stat_distance = mean_diff * 100 + std_diff * 50\n            \n            return float(stat_distance)\n            \n        except Exception as e:\n            print(f\"⚠ Statistical distance calculation failed: {e}\")\n            return 100.0  # Score par défaut\n    \n    def calculate_diversity_score(self, fake_images):\n        \"\"\"Score de diversité des images générées\"\"\"\n        try:\n            if fake_images.numel() == 0:\n                return 0.0\n            \n            # Calcul de la diversité basée sur la variance des pixels\n            fake_flat = fake_images.view(fake_images.size(0), -1)\n            \n            # Variance moyenne entre images\n            pairwise_distances = torch.cdist(fake_flat, fake_flat)\n            \n            # Exclusion de la diagonale (distance à soi-même)\n            mask = ~torch.eye(pairwise_distances.size(0), dtype=torch.bool)\n            avg_distance = pairwise_distances[mask].mean()\n            \n            return float(avg_distance)\n            \n        except Exception as e:\n            print(f\"⚠ Diversity score calculation failed: {e}\")\n            return 0.0\n    \n    def calculate_quality_score(self, images):\n        \"\"\"Score de qualité basé sur la netteté et le contraste\"\"\"\n        try:\n            if images.numel() == 0:\n                return 0.0\n            \n            # Conversion en format approprié\n            imgs = torch.clamp((images + 1) / 2, 0, 1)\n            \n            # Calcul du gradient (netteté)\n            grad_x = torch.abs(imgs[:, :, :, 1:] - imgs[:, :, :, :-1])\n            grad_y = torch.abs(imgs[:, :, 1:, :] - imgs[:, :, :-1, :])\n            \n            sharpness = (grad_x.mean() + grad_y.mean()) / 2\n            \n            # Calcul du contraste\n            contrast = torch.std(imgs)\n            \n            # Score combiné\n            quality = (sharpness * 10 + contrast * 5).item()\n            \n            return quality\n            \n        except Exception as e:\n            print(f\"⚠ Quality score calculation failed: {e}\")\n            return 0.0\n\n# Classe de monitoring améliorée\nclass ImprovedModelMonitor:\n    \"\"\"Monitoring amélioré avec métriques robustes\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.metrics_history = defaultdict(list)\n        self.resource_history = defaultdict(deque)\n        self.best_models = {}\n        self.early_stopping_counter = 0\n        self.best_score = float('inf')\n        self.metrics_calculator = RobustGANMetrics(device)\n        \n        # Métriques alternatives pour Kaggle\n        self.alternative_metrics = [\n            'statistical_distance', 'diversity_score', 'quality_score', \n            'loss_stability', 'gradient_norm'\n        ]\n    \n    def comprehensive_evaluation(self, generator, real_samples, epoch):\n        \"\"\"Évaluation complète avec métriques multiples\"\"\"\n        try:\n            generator.eval()\n            metrics = {}\n            \n            with torch.no_grad():\n                # Génération d'échantillons\n                eval_noise = torch.randn(min(100, len(real_samples)), latent_dim, 1, 1, device=device)\n                fake_samples = generator(eval_noise)\n                \n                # Limitation du nombre d'échantillons pour éviter les problèmes mémoire\n                n_samples = min(50, len(real_samples), len(fake_samples))\n                real_subset = real_samples[:n_samples]\n                fake_subset = fake_samples[:n_samples]\n                \n                # Métriques principales\n                start_time = time.time()\n                \n                # FID robuste\n                fid_score = self.metrics_calculator.calculate_lightweight_fid(\n                    real_subset, fake_subset\n                )\n                metrics['fid_score'] = fid_score\n                \n                # Métriques alternatives\n                diversity = self.metrics_calculator.calculate_diversity_score(fake_subset)\n                quality = self.metrics_calculator.calculate_quality_score(fake_subset)\n                \n                metrics.update({\n                    'diversity_score': diversity,\n                    'quality_score': quality,\n                    'inference_time': time.time() - start_time,\n                    'n_samples_evaluated': n_samples\n                })\n                \n                # Score composite pour sélection de modèle\n                if not np.isnan(fid_score) and not np.isinf(fid_score):\n                    composite_score = fid_score\n                else:\n                    # Fallback: utiliser le score de qualité inversé\n                    composite_score = max(0, 100 - quality * 10)\n                \n                metrics['composite_score'] = composite_score\n                \n                print(f\"📊 Epoch {epoch} Metrics:\")\n                print(f\"   FID: {fid_score:.3f}\")\n                print(f\"   Diversity: {diversity:.3f}\")\n                print(f\"   Quality: {quality:.3f}\")\n                print(f\"   Composite: {composite_score:.3f}\")\n                \n                return metrics\n                \n        except Exception as e:\n            print(f\"⚠ Comprehensive evaluation failed: {e}\")\n            # Métriques par défaut en cas d'erreur\n            return {\n                'fid_score': 100.0,\n                'diversity_score': 0.0,\n                'quality_score': 0.0,\n                'composite_score': 100.0,\n                'inference_time': 0.0,\n                'evaluation_error': str(e)\n            }\n    \n    def check_early_stopping(self, current_score):\n        \"\"\"Early stopping basé sur le score composite\"\"\"\n        if np.isnan(current_score) or np.isinf(current_score):\n            current_score = float('inf')\n            \n        if current_score < self.best_score:\n            self.best_score = current_score\n            self.early_stopping_counter = 0\n            return False\n        else:\n            self.early_stopping_counter += 1\n            return self.early_stopping_counter >= self.config.early_stopping_patience\n    \n    def log_metrics(self, epoch, metrics_dict):\n        \"\"\"Logging robuste des métriques\"\"\"\n        timestamp = datetime.now().isoformat()\n        \n        # Nettoyage des valeurs NaN/Inf\n        clean_metrics = {}\n        for key, value in metrics_dict.items():\n            if isinstance(value, (int, float)):\n                if np.isnan(value) or np.isinf(value):\n                    clean_metrics[key] = None\n                else:\n                    clean_metrics[key] = float(value)\n            else:\n                clean_metrics[key] = value\n        \n        log_entry = {\n            'timestamp': timestamp,\n            'epoch': epoch,\n            **clean_metrics\n        }\n        \n        # Sauvegarde des métriques\n        for key, value in clean_metrics.items():\n            if value is not None:\n                self.metrics_history[key].append(value)\n        \n        # Sauvegarde JSON\n        metrics_file = self.config.metrics_path / f\"{self.config.experiment_name}_metrics.jsonl\"\n        with open(metrics_file, 'a') as f:\n            f.write(json.dumps(log_entry, default=str) + '\\n')\n        \n        print(f\"📊 Epoch {epoch}: {clean_metrics}\")\n    \n    def save_model_checkpoint(self, generator, discriminator, epoch, metrics, is_best=False):\n        \"\"\"Sauvegarde robuste avec validation\"\"\"\n        try:\n            checkpoint_dir = self.config.model_registry_path / f\"epoch_{epoch}\"\n            checkpoint_dir.mkdir(exist_ok=True)\n            \n            # Validation des modèles avant sauvegarde\n            if not hasattr(generator, 'state_dict') or not hasattr(discriminator, 'state_dict'):\n                raise ValueError(\"Invalid model objects for saving\")\n            \n            # Métadonnées avec validation\n            clean_metrics = {k: v for k, v in metrics.items() \n                           if not (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))}\n            \n            metadata = {\n                'epoch': epoch,\n                'timestamp': datetime.now().isoformat(),\n                'metrics': clean_metrics,\n                'model_size_mb': self._get_model_size(generator) + self._get_model_size(discriminator),\n                'is_best': is_best,\n                'config': {\n                    'latent_dim': latent_dim,\n                    'image_channels': image_channels_cifar,\n                    'learning_rate': lr_finetune\n                }\n            }\n            \n            # Sauvegarde des modèles\n            torch.save(generator.state_dict(), checkpoint_dir / \"generator.pth\")\n            torch.save(discriminator.state_dict(), checkpoint_dir / \"discriminator.pth\")\n            \n            # Sauvegarde des métadonnées\n            with open(checkpoint_dir / \"metadata.json\", 'w') as f:\n                json.dump(metadata, f, indent=2, default=str)\n            \n            # Marquage du meilleur modèle\n            if is_best:\n                best_model_path = self.config.model_registry_path / \"best_model\"\n                if best_model_path.exists():\n                    import shutil\n                    shutil.rmtree(best_model_path)\n                \n                import shutil\n                shutil.copytree(checkpoint_dir, best_model_path)\n                \n                composite_score = clean_metrics.get('composite_score', 'N/A')\n                print(f\"🏆 New best model saved (Composite Score: {composite_score})\")\n                \n        except Exception as e:\n            print(f\"⚠ Model checkpoint saving failed: {e}\")\n    \n    def _get_model_size(self, model):\n        \"\"\"Calcul robuste de la taille du modèle\"\"\"\n        try:\n            param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n            buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n            return (param_size + buffer_size) / 1024**2\n        except Exception as e:\n            print(f\"⚠ Model size calculation failed: {e}\")\n            return 0.0\n\n# Configuration mise à jour\nclass ImprovedModelConfig:\n    def __init__(self):\n        self.experiment_name = f\"cifar10_robust_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.model_registry_path = Path(\"model_registry\")\n        self.metrics_path = Path(\"metrics\")\n        self.artifacts_path = Path(\"artifacts\")\n        \n        for path in [self.model_registry_path, self.metrics_path, self.artifacts_path]:\n            path.mkdir(exist_ok=True)\n        \n        self.early_stopping_patience = 8  # Plus conservateur\n        self.model_selection_metric = 'composite_score'\n        self.evaluation_frequency = 3  # Évaluation tous les 3 epochs\n\n# Tests Kaggle améliorés\ndef improved_kaggle_test_suite():\n    \"\"\"Suite de tests robuste pour Kaggle\"\"\"\n    print(\"\\n🧪 Running improved Kaggle tests...\")\n    \n    try:\n        # Test 1: Vérification système\n        print(f\"🖥 Python version: {sys.version.split()[0]}\")\n        print(f\"🔥 PyTorch version: {torch.__version__}\")\n        print(f\"🖼 Torchvision version: {torchvision.__version__}\")\n        \n        # Test 2: Mémoire disponible\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n            print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n            print(f\"💾 GPU Memory: {gpu_memory:.1f} GB\")\n        \n        # Test 3: Test des métriques\n        test_metrics = RobustGANMetrics(device)\n        test_real = torch.randn(4, 3, 64, 64).to(device)\n        test_fake = torch.randn(4, 3, 64, 64).to(device)\n        \n        fid_test = test_metrics.calculate_lightweight_fid(test_real, test_fake)\n        diversity_test = test_metrics.calculate_diversity_score(test_fake)\n        quality_test = test_metrics.calculate_quality_score(test_fake)\n        \n        print(f\"🧮 Metrics test - FID: {fid_test:.3f}, Diversity: {diversity_test:.3f}, Quality: {quality_test:.3f}\")\n        \n        # Validation des métriques\n        assert not np.isnan(fid_test), f\"FID calculation returned NaN\"\n        assert not np.isnan(diversity_test), f\"Diversity calculation returned NaN\"\n        assert not np.isnan(quality_test), f\"Quality calculation returned NaN\"\n        \n        print(\"✅ All improved Kaggle tests passed!\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ Kaggle test failed: {e}\")\n        return False\n\n# Application du code corrigé\nprint(\"\\n--- DÉBUT DE LA PHASE 2 INDUSTRIELLE CORRIGÉE ---\")\n\n# Initialisation des composants améliorés\nconfig = ImprovedModelConfig()\nmonitor = ImprovedModelMonitor(config)\n\nprint(f\"🎯 Experiment: {config.experiment_name}\")\nprint(f\"🔧 Using robust metrics with fallback strategies\")\n\n# Test initial\ntest_passed = improved_kaggle_test_suite()\nif not test_passed:\n    print(\"⚠ Some tests failed, but continuing with robust fallbacks...\")\n\n# Le reste du code d'entraînement reste similaire mais avec monitor amélioré\n# Remplacer l'appel d'évaluation par:\n\"\"\"\n# Dans la boucle d'entraînement, remplacer la section d'évaluation par:\nif epoch % config.evaluation_frequency == 0 or epoch == epochs_finetune - 1:\n    # Évaluation complète avec métriques robustes\n    evaluation_metrics = monitor.comprehensive_evaluation(\n        generator_cifar, real_samples_for_metrics, epoch\n    )\n    \n    # Métriques d'entraînement\n    complete_metrics = {\n        **epoch_metrics,\n        **evaluation_metrics,\n        'epoch_time': epoch_time,\n        'lr_g': optimizer_G_cifar.param_groups[0]['lr'],\n        'lr_d': optimizer_D_cifar.param_groups[0]['lr']\n    }\n    \n    # Logging et sauvegarde\n    monitor.log_metrics(epoch, complete_metrics)\n    \n    # Sélection basée sur le score composite\n    composite_score = evaluation_metrics.get('composite_score', float('inf'))\n    is_best = composite_score < monitor.best_score\n    \n    monitor.save_model_checkpoint(\n        generator_cifar, discriminator_cifar, \n        epoch, complete_metrics, is_best\n    )\n    \n    # Early stopping\n    if monitor.check_early_stopping(composite_score):\n        print(f\"🛑 Early stopping at epoch {epoch}\")\n        break\n\"\"\"\n\n# Code principal amélioré\n# print(\"\\n--- DÉBUT DE LA PHASE 2 : TRANSFERT INDUSTRIEL SUR CIFAR-10 ---\")\n\n# Initialisation des composants industriels\nmonitor = ModelMonitor(config)\nmetrics_calculator = GANMetrics(device)\ncomparator = ModelComparator(config)\n\n# --- 2.1. Préparation des données CIFAR-10 (inchangé) ---\nimage_channels_cifar = 3\ntransform_cifar = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ncifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\ndataloader_cifar = DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True)\n\n# Dataset de validation pour métriques\ncifar_val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\nval_dataloader = DataLoader(cifar_val_dataset, batch_size=64, shuffle=False)\n\n# Échantillon de vraies images pour calcul FID\nreal_samples_for_metrics = next(iter(val_dataloader))[0][:100]\n\nprint(f\"🎯 Experiment: {config.experiment_name}\")\nprint(f\"📁 Artifacts will be saved to: {config.artifacts_path}\")\n\n# --- 2.2. Transfert des poids (code existant) ---\ngenerator_cifar = Generator(latent_dim, image_channels_cifar).to(device)\ndiscriminator_cifar = Discriminator(image_channels_cifar).to(device)\n\n# ... (code de transfert existant) ...\n\n# --- 2.3. Fine-Tuning avec monitoring industriel ---\nparams_g_finetune = filter(lambda p: p.requires_grad, generator_cifar.parameters())\nparams_d_finetune = filter(lambda p: p.requires_grad, discriminator_cifar.parameters())\n\noptimizer_G_cifar = optim.Adam(params_g_finetune, lr=lr_finetune, betas=betas)\noptimizer_D_cifar = optim.Adam(params_d_finetune, lr=lr_finetune, betas=betas)\n\n# Learning rate scheduler industriel\nscheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G_cifar, 'min', patience=5, factor=0.5)\nscheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D_cifar, 'min', patience=5, factor=0.5)\n\nprint(f\"\\n🚀 Starting industrial fine-tuning...\")\nprint(f\"📊 Tracking metrics: {config.metrics_to_track}\")\n\n# Variables de tracking\ntraining_start_time = time.time()\nepoch_times = []\n\nfor epoch in range(epochs_finetune):\n    epoch_start_time = time.time()\n    \n    # Métriques d'époque\n    epoch_metrics = {\n        'loss_d': 0.0,\n        'loss_g': 0.0,\n        'loss_d_real': 0.0,\n        'loss_d_fake': 0.0\n    }\n    \n    generator_cifar.train()\n    discriminator_cifar.train()\n    \n    for i, (real_images, _) in enumerate(dataloader_cifar):\n        real_images = real_images.to(device)\n        batch_size_current = real_images.size(0)\n        \n        real_labels = torch.ones(batch_size_current, device=device)\n        fake_labels = torch.zeros(batch_size_current, device=device)\n\n        # --- Entraînement Discriminateur ---\n        optimizer_D_cifar.zero_grad()\n        \n        d_output_real = discriminator_cifar(real_images)\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n        \n        noise = torch.randn(batch_size_current, latent_dim, 1, 1, device=device)\n        fake_images = generator_cifar(noise)\n        d_output_fake = discriminator_cifar(fake_images.detach())\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n        \n        errD = errD_real + errD_fake\n        optimizer_D_cifar.step()\n\n        # --- Entraînement Générateur ---\n        optimizer_G_cifar.zero_grad()\n        d_output_on_fake = discriminator_cifar(fake_images)\n        errG = criterion(d_output_on_fake, real_labels)\n        errG.backward()\n        optimizer_G_cifar.step()\n        \n        # Accumulation des métriques\n        epoch_metrics['loss_d'] += errD.item()\n        epoch_metrics['loss_g'] += errG.item()\n        epoch_metrics['loss_d_real'] += errD_real.item()\n        epoch_metrics['loss_d_fake'] += errD_fake.item()\n        \n        # Monitoring des ressources (toutes les 50 itérations)\n        if i % 50 == 0:\n            resources = monitor.monitor_resources()\n            \n        # Nettoyage mémoire périodique\n        if i % 100 == 0:\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            gc.collect()\n    \n    # Calcul des moyennes d'époque\n    num_batches = len(dataloader_cifar)\n    for key in epoch_metrics:\n        epoch_metrics[key] /= num_batches\n    \n    epoch_time = time.time() - epoch_start_time\n    epoch_times.append(epoch_time)\n    # Dans la boucle d'entraînement, remplacer la section d'évaluation par:\n    if epoch % config.evaluation_frequency == 0 or epoch == epochs_finetune - 1:\n        # Évaluation complète avec métriques robustes\n        evaluation_metrics = monitor.comprehensive_evaluation(\n            generator_cifar, real_samples_for_metrics, epoch\n        )\n        \n        # Métriques d'entraînement\n        complete_metrics = {\n            **epoch_metrics,\n            **evaluation_metrics,\n            'epoch_time': epoch_time,\n            'lr_g': optimizer_G_cifar.param_groups[0]['lr'],\n            'lr_d': optimizer_D_cifar.param_groups[0]['lr']\n        }\n        \n        # Logging et sauvegarde\n        monitor.log_metrics(epoch, complete_metrics)\n        \n        # Sélection basée sur le score composite\n        composite_score = evaluation_metrics.get('composite_score', float('inf'))\n        is_best = composite_score < monitor.best_score\n        \n        monitor.save_model_checkpoint(\n            generator_cifar, discriminator_cifar, \n            epoch, complete_metrics, is_best\n        )\n        \n        # Early stopping\n        if monitor.check_early_stopping(composite_score):\n            print(f\"🛑 Early stopping at epoch {epoch}\")\n            break\n\n    # # --- Évaluation industrielle ---\n    # if epoch % 2 == 0 or epoch == epochs_finetune - 1:  # Évaluation tous les 2 époques\n    #     generator_cifar.eval()\n        \n    #     with torch.no_grad():\n    #         # Génération d'échantillons pour métriques\n    #         eval_noise = torch.randn(100, latent_dim, 1, 1, device=device)\n    #         fake_samples = generator_cifar(eval_noise)\n            \n    #         # Calcul des métriques industrielles\n    #         start_inference = time.time()\n    #         fid_score = metrics_calculator.calculate_fid_score(\n    #             real_samples_for_metrics.to(device), \n    #             fake_samples\n    #         )\n    #         inference_time = time.time() - start_inference\n            \n    #         inception_score, inception_std = metrics_calculator.calculate_inception_score(fake_samples)\n            \n    #         # Métriques complètes\n    #         complete_metrics = {\n    #             **epoch_metrics,\n    #             'fid_score': fid_score,\n    #             'inception_score': inception_score,\n    #             'inception_std': inception_std,\n    #             'inference_time': inference_time,\n    #             'epoch_time': epoch_time,\n    #             'lr_g': optimizer_G_cifar.param_groups[0]['lr'],\n    #             'lr_d': optimizer_D_cifar.param_groups[0]['lr']\n    #         }\n            \n    #         # Logging industriel\n    #         monitor.log_metrics(epoch, complete_metrics)\n            \n    #         # Sauvegarde de checkpoint\n    #         is_best = fid_score < monitor.best_score\n    #         monitor.save_model_checkpoint(\n    #             generator_cifar, discriminator_cifar, \n    #             epoch, complete_metrics, is_best\n    #         )\n            \n    #         # Mise à jour des schedulers\n    #         scheduler_G.step(fid_score)\n    #         scheduler_D.step(fid_score)\n            \n    #         # Early stopping check\n    #         if monitor.check_early_stopping(fid_score):\n    #             print(f\"🛑 Early stopping triggered at epoch {epoch}\")\n    #             break\n            \n            # Sauvegarde d'images avec métriques\n            save_image(\n                fake_samples[:64], \n                config.artifacts_path / f\"epoch_{epoch}_fid_{fid_score:.3f}.png\", \n                normalize=True, nrow=8\n            )\n    \n    # Affichage des métriques\n    print(f\"[Epoch {epoch+1}/{epochs_finetune}] \"\n          f\"Loss_D: {epoch_metrics['loss_d']:.4f} \"\n          f\"Loss_G: {epoch_metrics['loss_g']:.4f} \"\n          f\"Time: {epoch_time:.1f}s\")\n\n# --- Analyse finale et sélection du meilleur modèle ---\ntraining_time = time.time() - training_start_time\n\nprint(f\"\\n🎯 Training completed in {training_time/3600:.2f} hours\")\nprint(f\"⚡ Average epoch time: {np.mean(epoch_times):.1f}s\")\n\n# Évaluation comparative finale\nbest_models = comparator.evaluate_model_portfolio()\n\n# Génération du rapport final\nfinal_report = {\n    'experiment_name': config.experiment_name,\n    'total_training_time_hours': training_time / 3600,\n    'total_epochs_trained': epoch + 1,\n    'best_model': best_models[0] if best_models else None,\n    'avg_epoch_time_seconds': np.mean(epoch_times),\n    'final_metrics': monitor.metrics_history,\n    'early_stopped': monitor.early_stopping_counter >= config.early_stopping_patience\n}\n\n# Sauvegarde du rapport\nreport_path = config.artifacts_path / \"final_report.json\"\nwith open(report_path, 'w') as f:\n    json.dump(final_report, f, indent=2, default=str)\n\nprint(f\"📋 Final report saved to: {report_path}\")\nprint(f\"🏆 Best model FID score: {best_models[0]['fid_score']:.3f}\")\n\n# --- Code de test pour Kaggle ---\ndef kaggle_test_suite():\n    \"\"\"Suite de tests spécifiques pour environnement Kaggle\"\"\"\n    print(\"\\n🧪 Running Kaggle-specific tests...\")\n    \n    # Test 1: Vérification des chemins et permissions\n    test_paths = [config.model_registry_path, config.metrics_path, config.artifacts_path]\n    for path in test_paths:\n        assert path.exists(), f\"Path {path} not accessible\"\n        # Test d'écriture\n        test_file = path / \"test_write.tmp\"\n        test_file.write_text(\"test\")\n        test_file.unlink()\n    print(\"✅ File system access test passed\")\n    \n    # Test 2: Vérification mémoire GPU/CPU\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        print(f\"✅ GPU Memory available: {gpu_memory:.1f} GB\")\n        assert gpu_memory > 10, \"Insufficient GPU memory for training\"\n    \n    # Test 3: Génération rapide\n    with torch.no_grad():\n        test_noise = torch.randn(4, latent_dim, 1, 1, device=device)\n        if 'generator_cifar' in locals():\n            test_images = generator_cifar(test_noise)\n            assert test_images.shape == (4, 3, image_size, image_size), \"Wrong output shape\"\n    print(\"✅ Model generation test passed\")\n    \n    # Test 4: Métriques calculation\n    if len(monitor.metrics_history) > 0:\n        latest_fid = monitor.metrics_history['fid_score'][-1]\n        assert latest_fid < 300, f\"FID score too high: {latest_fid}\"\n    print(\"✅ Metrics validation test passed\")\n    \n    print(\"🎉 All Kaggle tests passed!\")\n\n# Exécution des tests Kaggle\ntry:\n    kaggle_test_suite()\nexcept Exception as e:\n    print(f\"⚠ Kaggle test failed: {e}\")\n\nprint(\"\\n--- FIN DE LA PHASE 2 INDUSTRIELLE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:23:50.253420Z","iopub.execute_input":"2025-06-10T12:23:50.253756Z","iopub.status.idle":"2025-06-10T12:23:52.364540Z","shell.execute_reply.started":"2025-06-10T12:23:50.253726Z","shell.execute_reply":"2025-06-10T12:23:52.363633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# PHASE 2 : TRANSFERT INDUSTRIEL CORRIGÉ - Métriques robustes pour Kaggle\n# =========================================================================\n\nimport json\nimport time\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nimport gc\nimport psutil\nimport numpy as np\nfrom collections import defaultdict, deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Installation des dépendances manquantes pour Kaggle\ntry:\n    import torch\n    import torchvision\n    from torchvision import transforms, datasets\n    from torch.utils.data import DataLoader\n    import torch.nn as nn\n    import torch.optim as optim\n    from torchvision.utils import save_image, make_grid\n    import matplotlib.pyplot as plt\n    from scipy import linalg\n    from sklearn.metrics import accuracy_score\nexcept ImportError as e:\n    print(f\"Installing missing dependencies: {e}\")\n\n# Configuration des paramètres de base\nimage_size = 64\nbatch_size = 128\nlatent_dim = 100\nlr_finetune = 0.0002\nbetas = (0.5, 0.999)\nepochs_finetune = 100\n\n# Initialisation du device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Classe de métriques robustes pour Kaggle\nclass RobustGANMetrics:\n    \"\"\"Métriques GAN robustes adaptées à l'environnement Kaggle\"\"\"\n    \n    def __init__(self, device):\n        self.device = device\n        self.inception_model = None\n        self.classifier_model = None\n        self._init_lightweight_models()\n    \n    def _init_lightweight_models(self):\n        \"\"\"Initialisation de modèles légers pour métriques\"\"\"\n        try:\n            # Modèle léger pour classification (alternative à Inception)\n            from torchvision.models import resnet18\n            self.classifier_model = resnet18(pretrained=True)\n            self.classifier_model.eval()\n            self.classifier_model = self.classifier_model.to(self.device)\n            print(\"✓ ResNet18 loaded for lightweight metrics\")\n            \n        except Exception as e:\n            print(f\"⚠ Could not load pretrained models: {e}\")\n            print(\"📊 Using basic statistical metrics instead\")\n    \n    def calculate_lightweight_fid(self, real_images, fake_images):\n        \"\"\"Calcul FID simplifié et robuste\"\"\"\n        try:\n            # Vérification des inputs\n            if real_images.numel() == 0 or fake_images.numel() == 0:\n                return float('inf')\n            \n            # Normalisation des images\n            real_imgs = self._normalize_images(real_images)\n            fake_imgs = self._normalize_images(fake_images)\n            \n            # Si pas de modèle préentraîné, utiliser des statistiques simples\n            if self.classifier_model is None:\n                return self._calculate_statistical_distance(real_imgs, fake_imgs)\n            \n            # Extraction de features avec modèle léger\n            real_features = self._extract_resnet_features(real_imgs)\n            fake_features = self._extract_resnet_features(fake_imgs)\n            \n            # Calcul FID robuste avec gestion des cas limites\n            fid_score = self._compute_robust_fid(real_features, fake_features)\n            \n            # Validation du résultat\n            if np.isnan(fid_score) or np.isinf(fid_score):\n                print(\"⚠ FID calculation resulted in nan/inf, using fallback metric\")\n                return self._calculate_statistical_distance(real_imgs, fake_imgs)\n                \n            return float(fid_score)\n            \n        except Exception as e:\n            print(f\"⚠ FID calculation error: {e}\")\n            return self._calculate_statistical_distance(real_images, fake_images)\n    \n    def _normalize_images(self, images):\n        \"\"\"Normalisation robuste des images\"\"\"\n        if images.dim() == 3:\n            images = images.unsqueeze(0)\n        \n        # Clamp et normalisation\n        images = torch.clamp(images, -1, 1)\n        images = (images + 1) / 2  # [-1,1] -> [0,1]\n        \n        return images\n    \n    def _extract_resnet_features(self, images):\n        \"\"\"Extraction de features avec ResNet18\"\"\"\n        with torch.no_grad():\n            # Resize pour ResNet\n            if images.shape[-1] != 224:\n                images = torch.nn.functional.interpolate(\n                    images, size=(224, 224), mode='bilinear', align_corners=False\n                )\n            \n            # Forward pass jusqu'aux features\n            x = self.classifier_model.conv1(images)\n            x = self.classifier_model.bn1(x)\n            x = self.classifier_model.relu(x)\n            x = self.classifier_model.maxpool(x)\n            \n            x = self.classifier_model.layer1(x)\n            x = self.classifier_model.layer2(x)\n            x = self.classifier_model.layer3(x)\n            x = self.classifier_model.layer4(x)\n            \n            x = self.classifier_model.avgpool(x)\n            features = torch.flatten(x, 1)\n            \n            return features.cpu().numpy()\n    \n    def _compute_robust_fid(self, real_features, fake_features):\n        \"\"\"Calcul FID robuste avec gestion d'erreurs\"\"\"\n        try:\n            # Calcul des statistiques\n            mu1 = np.mean(real_features, axis=0)\n            mu2 = np.mean(fake_features, axis=0)\n            \n            sigma1 = np.cov(real_features, rowvar=False)\n            sigma2 = np.cov(fake_features, rowvar=False)\n            \n            # Gestion des matrices singulières\n            if sigma1.ndim == 0:\n                sigma1 = np.array([[sigma1]])\n            if sigma2.ndim == 0:\n                sigma2 = np.array([[sigma2]])\n            \n            # Ajout de régularisation pour éviter les matrices singulières\n            eps = 1e-6\n            sigma1 += eps * np.eye(sigma1.shape[0])\n            sigma2 += eps * np.eye(sigma2.shape[0])\n            \n            # Calcul de la distance de Fréchet\n            diff = mu1 - mu2\n            \n            # Calcul de la racine carrée de la matrice\n            covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n            \n            # Gestion des nombres complexes\n            if np.iscomplexobj(covmean):\n                if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n                    print(\"⚠ Warning: Imaginary component in covariance mean\")\n                covmean = covmean.real\n            \n            # Calcul final du FID\n            fid = (diff.dot(diff) + np.trace(sigma1) + \n                   np.trace(sigma2) - 2 * np.trace(covmean))\n            \n            return fid\n            \n        except Exception as e:\n            print(f\"⚠ Robust FID calculation failed: {e}\")\n            return float('inf')\n    \n    def _calculate_statistical_distance(self, real_images, fake_images):\n        \"\"\"Métrique de fallback basée sur les statistiques d'images\"\"\"\n        try:\n            # Conversion en numpy\n            real_np = real_images.detach().cpu().numpy()\n            fake_np = fake_images.detach().cpu().numpy()\n            \n            # Statistiques de base\n            real_mean = np.mean(real_np)\n            fake_mean = np.mean(fake_np)\n            real_std = np.std(real_np)\n            fake_std = np.std(fake_np)\n            \n            # Distance statistique simple\n            mean_diff = abs(real_mean - fake_mean)\n            std_diff = abs(real_std - fake_std)\n            \n            # Score combiné (plus bas = meilleur)\n            stat_distance = mean_diff * 100 + std_diff * 50\n            \n            return float(stat_distance)\n            \n        except Exception as e:\n            print(f\"⚠ Statistical distance calculation failed: {e}\")\n            return 100.0  # Score par défaut\n    \n    def calculate_diversity_score(self, fake_images):\n        \"\"\"Score de diversité des images générées\"\"\"\n        try:\n            if fake_images.numel() == 0:\n                return 0.0\n            \n            # Calcul de la diversité basée sur la variance des pixels\n            fake_flat = fake_images.view(fake_images.size(0), -1)\n            \n            # Variance moyenne entre images\n            pairwise_distances = torch.cdist(fake_flat, fake_flat)\n            \n            # Exclusion de la diagonale (distance à soi-même)\n            mask = ~torch.eye(pairwise_distances.size(0), dtype=torch.bool)\n            avg_distance = pairwise_distances[mask].mean()\n            \n            return float(avg_distance)\n            \n        except Exception as e:\n            print(f\"⚠ Diversity score calculation failed: {e}\")\n            return 0.0\n    \n    def calculate_quality_score(self, images):\n        \"\"\"Score de qualité basé sur la netteté et le contraste\"\"\"\n        try:\n            if images.numel() == 0:\n                return 0.0\n            \n            # Conversion en format approprié\n            imgs = torch.clamp((images + 1) / 2, 0, 1)\n            \n            # Calcul du gradient (netteté)\n            grad_x = torch.abs(imgs[:, :, :, 1:] - imgs[:, :, :, :-1])\n            grad_y = torch.abs(imgs[:, :, 1:, :] - imgs[:, :, :-1, :])\n            \n            sharpness = (grad_x.mean() + grad_y.mean()) / 2\n            \n            # Calcul du contraste\n            contrast = torch.std(imgs)\n            \n            # Score combiné\n            quality = (sharpness * 10 + contrast * 5).item()\n            \n            return quality\n            \n        except Exception as e:\n            print(f\"⚠ Quality score calculation failed: {e}\")\n            return 0.0\n\n# Configuration mise à jour\nclass ImprovedModelConfig:\n    def __init__(self):\n        self.experiment_name = f\"cifar10_robust_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.model_registry_path = Path(\"model_registry\")\n        self.metrics_path = Path(\"metrics\")\n        self.artifacts_path = Path(\"artifacts\")\n        \n        for path in [self.model_registry_path, self.metrics_path, self.artifacts_path]:\n            path.mkdir(exist_ok=True)\n        \n        self.early_stopping_patience = 8  # Plus conservateur\n        self.model_selection_metric = 'composite_score'\n        self.evaluation_frequency = 3  # Évaluation tous les 3 epochs\n        self.metrics_to_track = ['fid_score', 'diversity_score', 'quality_score', 'composite_score']\n\n# Classe de monitoring améliorée\nclass ImprovedModelMonitor:\n    \"\"\"Monitoring amélioré avec métriques robustes\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.metrics_history = defaultdict(list)\n        self.resource_history = defaultdict(deque)\n        self.best_models = {}\n        self.early_stopping_counter = 0\n        self.best_score = float('inf')\n        self.metrics_calculator = RobustGANMetrics(device)\n        \n        # Métriques alternatives pour Kaggle\n        self.alternative_metrics = [\n            'statistical_distance', 'diversity_score', 'quality_score', \n            'loss_stability', 'gradient_norm'\n        ]\n    \n    def comprehensive_evaluation(self, generator, real_samples, epoch):\n        \"\"\"Évaluation complète avec métriques multiples\"\"\"\n        try:\n            generator.eval()\n            metrics = {}\n            \n            with torch.no_grad():\n                # Génération d'échantillons\n                eval_noise = torch.randn(min(100, len(real_samples)), latent_dim, 1, 1, device=device)\n                fake_samples = generator(eval_noise)\n                \n                # Limitation du nombre d'échantillons pour éviter les problèmes mémoire\n                n_samples = min(50, len(real_samples), len(fake_samples))\n                real_subset = real_samples[:n_samples]\n                fake_subset = fake_samples[:n_samples]\n                \n                # Métriques principales\n                start_time = time.time()\n                \n                # FID robuste\n                fid_score = self.metrics_calculator.calculate_lightweight_fid(\n                    real_subset, fake_subset\n                )\n                metrics['fid_score'] = fid_score\n                \n                # Métriques alternatives\n                diversity = self.metrics_calculator.calculate_diversity_score(fake_subset)\n                quality = self.metrics_calculator.calculate_quality_score(fake_subset)\n                \n                metrics.update({\n                    'diversity_score': diversity,\n                    'quality_score': quality,\n                    'inference_time': time.time() - start_time,\n                    'n_samples_evaluated': n_samples\n                })\n                \n                # Score composite pour sélection de modèle\n                if not np.isnan(fid_score) and not np.isinf(fid_score):\n                    composite_score = fid_score\n                else:\n                    # Fallback: utiliser le score de qualité inversé\n                    composite_score = max(0, 100 - quality * 10)\n                \n                metrics['composite_score'] = composite_score\n                \n                print(f\"📊 Epoch {epoch} Metrics:\")\n                print(f\"   FID: {fid_score:.3f}\")\n                print(f\"   Diversity: {diversity:.3f}\")\n                print(f\"   Quality: {quality:.3f}\")\n                print(f\"   Composite: {composite_score:.3f}\")\n                \n                return metrics\n                \n        except Exception as e:\n            print(f\"⚠ Comprehensive evaluation failed: {e}\")\n            # Métriques par défaut en cas d'erreur\n            return {\n                'fid_score': 100.0,\n                'diversity_score': 0.0,\n                'quality_score': 0.0,\n                'composite_score': 100.0,\n                'inference_time': 0.0,\n                'evaluation_error': str(e)\n            }\n    \n    def check_early_stopping(self, current_score):\n        \"\"\"Early stopping basé sur le score composite\"\"\"\n        if np.isnan(current_score) or np.isinf(current_score):\n            current_score = float('inf')\n            \n        if current_score < self.best_score:\n            self.best_score = current_score\n            self.early_stopping_counter = 0\n            return False\n        else:\n            self.early_stopping_counter += 1\n            return self.early_stopping_counter >= self.config.early_stopping_patience\n    \n    def log_metrics(self, epoch, metrics_dict):\n        \"\"\"Logging robuste des métriques\"\"\"\n        timestamp = datetime.now().isoformat()\n        \n        # Nettoyage des valeurs NaN/Inf\n        clean_metrics = {}\n        for key, value in metrics_dict.items():\n            if isinstance(value, (int, float)):\n                if np.isnan(value) or np.isinf(value):\n                    clean_metrics[key] = None\n                else:\n                    clean_metrics[key] = float(value)\n            else:\n                clean_metrics[key] = value\n        \n        log_entry = {\n            'timestamp': timestamp,\n            'epoch': epoch,\n            **clean_metrics\n        }\n        \n        # Sauvegarde des métriques\n        for key, value in clean_metrics.items():\n            if value is not None:\n                self.metrics_history[key].append(value)\n        \n        # Sauvegarde JSON\n        metrics_file = self.config.metrics_path / f\"{self.config.experiment_name}_metrics.jsonl\"\n        with open(metrics_file, 'a') as f:\n            f.write(json.dumps(log_entry, default=str) + '\\n')\n        \n        print(f\"📊 Epoch {epoch}: {clean_metrics}\")\n    \n    def save_model_checkpoint(self, generator, discriminator, epoch, metrics, is_best=False):\n        \"\"\"Sauvegarde robuste avec validation\"\"\"\n        try:\n            checkpoint_dir = self.config.model_registry_path / f\"epoch_{epoch}\"\n            checkpoint_dir.mkdir(exist_ok=True)\n            \n            # Validation des modèles avant sauvegarde\n            if not hasattr(generator, 'state_dict') or not hasattr(discriminator, 'state_dict'):\n                raise ValueError(\"Invalid model objects for saving\")\n            \n            # Métadonnées avec validation\n            clean_metrics = {k: v for k, v in metrics.items() \n                           if not (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))}\n            \n            metadata = {\n                'epoch': epoch,\n                'timestamp': datetime.now().isoformat(),\n                'metrics': clean_metrics,\n                'model_size_mb': self._get_model_size(generator) + self._get_model_size(discriminator),\n                'is_best': is_best,\n                'config': {\n                    'latent_dim': latent_dim,\n                    'image_channels': 3,  # For CIFAR-10\n                    'learning_rate': lr_finetune\n                }\n            }\n            \n            # Sauvegarde des modèles\n            torch.save(generator.state_dict(), checkpoint_dir / \"generator.pth\")\n            torch.save(discriminator.state_dict(), checkpoint_dir / \"discriminator.pth\")\n            \n            # Sauvegarde des métadonnées\n            with open(checkpoint_dir / \"metadata.json\", 'w') as f:\n                json.dump(metadata, f, indent=2, default=str)\n            \n            # Marquage du meilleur modèle\n            if is_best:\n                best_model_path = self.config.model_registry_path / \"best_model\"\n                if best_model_path.exists():\n                    import shutil\n                    shutil.rmtree(best_model_path)\n                \n                import shutil\n                shutil.copytree(checkpoint_dir, best_model_path)\n                \n                composite_score = clean_metrics.get('composite_score', 'N/A')\n                print(f\"🏆 New best model saved (Composite Score: {composite_score})\")\n                \n        except Exception as e:\n            print(f\"⚠ Model checkpoint saving failed: {e}\")\n    \n    def _get_model_size(self, model):\n        \"\"\"Calcul robuste de la taille du modèle\"\"\"\n        try:\n            param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n            buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n            return (param_size + buffer_size) / 1024**2\n        except Exception as e:\n            print(f\"⚠ Model size calculation failed: {e}\")\n            return 0.0\n    \n    def monitor_resources(self):\n        \"\"\"Surveillance des ressources système\"\"\"\n        try:\n            resources = {\n                'cpu_usage': psutil.cpu_percent(),\n                'memory_usage': psutil.virtual_memory().percent,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            if torch.cuda.is_available():\n                resources.update({\n                    'gpu_memory_used': torch.cuda.memory_allocated() / 1024**3,\n                    'gpu_memory_total': torch.cuda.get_device_properties(0).total_memory / 1024**3\n                })\n            \n            return resources\n            \n        except Exception as e:\n            print(f\"⚠ Resource monitoring failed: {e}\")\n            return {}\n\n# Tests Kaggle améliorés\ndef improved_kaggle_test_suite():\n    \"\"\"Suite de tests robuste pour Kaggle\"\"\"\n    print(\"\\n🧪 Running improved Kaggle tests...\")\n    \n    try:\n        # Test 1: Vérification système\n        print(f\"🖥 Python version: {sys.version.split()[0]}\")\n        print(f\"🔥 PyTorch version: {torch.__version__}\")\n        print(f\"🖼 Torchvision version: {torchvision.__version__}\")\n        \n        # Test 2: Mémoire disponible\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n            print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n            print(f\"💾 GPU Memory: {gpu_memory:.1f} GB\")\n        \n        # Test 3: Test des métriques\n        test_metrics = RobustGANMetrics(device)\n        test_real = torch.randn(4, 3, 64, 64).to(device)\n        test_fake = torch.randn(4, 3, 64, 64).to(device)\n        \n        fid_test = test_metrics.calculate_lightweight_fid(test_real, test_fake)\n        diversity_test = test_metrics.calculate_diversity_score(test_fake)\n        quality_test = test_metrics.calculate_quality_score(test_fake)\n        \n        print(f\"🧮 Metrics test - FID: {fid_test:.3f}, Diversity: {diversity_test:.3f}, Quality: {quality_test:.3f}\")\n        \n        # Validation des métriques\n        assert not np.isnan(fid_test), f\"FID calculation returned NaN\"\n        assert not np.isnan(diversity_test), f\"Diversity calculation returned NaN\"\n        assert not np.isnan(quality_test), f\"Quality calculation returned NaN\"\n        \n        print(\"✅ All improved Kaggle tests passed!\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ Kaggle test failed: {e}\")\n        return False\n\n# Définition des modèles de base (Generator et Discriminator)\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            # Input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            # state size. 512 x 4 x 4\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            # state size. 256 x 8 x 8\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            # state size. 128 x 16 x 16\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            # state size. 64 x 32 x 32\n            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. img_channels x 64 x 64\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # input is (img_channels) x 64 x 64\n            nn.Conv2d(img_channels, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. 64 x 32 x 32\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. 128 x 16 x 16\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. 256 x 8 x 8\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. 512 x 4 x 4\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Initialisation des composants\nprint(\"\\n--- DÉBUT DE LA PHASE 2 INDUSTRIELLE CORRIGÉE ---\")\n\n# Initialisation des composants améliorés\nconfig = ImprovedModelConfig()\nmonitor = ImprovedModelMonitor(config)\nmetrics_calculator = RobustGANMetrics(device)\n\nprint(f\"🎯 Experiment: {config.experiment_name}\")\nprint(f\"🔧 Using robust metrics with fallback strategies\")\n\n# Test initial\ntest_passed = improved_kaggle_test_suite()\nif not test_passed:\n    print(\"⚠ Some tests failed, but continuing with robust fallbacks...\")\n\n# --- 2.1. Préparation des données CIFAR-10 ---\nimage_channels_cifar = 3\ntransform_cifar = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ncifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\ndataloader_cifar = DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True)\n\n# Dataset de validation pour métriques\ncifar_val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\nval_dataloader = DataLoader(cifar_val_dataset, batch_size=64, shuffle=False)\n\n# Échantillon de vraies images pour calcul FID\nreal_samples_for_metrics = next(iter(val_dataloader))[0][:100].to(device)\n\n# Initialisation des modèles\ngenerator_cifar = Generator(latent_dim, image_channels_cifar).to(device)\ndiscriminator_cifar = Discriminator(image_channels_cifar).to(device)\n\n# Initialisation des optimiseurs\noptimizer_G_cifar = optim.Adam(generator_cifar.parameters(), lr=lr_finetune, betas=betas)\noptimizer_D_cifar = optim.Adam(discriminator_cifar.parameters(), lr=lr_finetune, betas=betas)\n\n# Fonction de perte\ncriterion = nn.BCELoss()\n\n# --- 2.3. Fine-Tuning avec monitoring industriel ---\nprint(f\"\\n🚀 Starting industrial fine-tuning...\")\nprint(f\"📊 Tracking metrics: {config.metrics_to_track}\")\n\n# Variables de tracking\ntraining_start_time = time.time()\nepoch_times = []\n\nfor epoch in range(epochs_finetune):\n    epoch_start_time = time.time()\n    \n    # Métriques d'époque\n    epoch_metrics = {\n        'loss_d': 0.0,\n        'loss_g': 0.0,\n        'loss_d_real': 0.0,\n        'loss_d_fake': 0.0\n    }\n    \n    generator_cifar.train()\n    discriminator_cifar.train()\n    \n    for i, (real_images, _) in enumerate(dataloader_cifar):\n        real_images = real_images.to(device)\n        batch_size_current = real_images.size(0)\n        \n        real_labels = torch.ones(batch_size_current, device=device)\n        fake_labels = torch.zeros(batch_size_current, device=device)\n\n        # --- Entraînement Discriminateur ---\n        optimizer_D_cifar.zero_grad()\n        \n        d_output_real = discriminator_cifar(real_images)\n        # Reshape output to match target shape\n        d_output_real = d_output_real.view(-1)  # [batch_size, 1, 1, 1] -> [batch_size]\n        errD_real = criterion(d_output_real, real_labels)\n        errD_real.backward()\n        \n        noise = torch.randn(batch_size_current, latent_dim, 1, 1, device=device)\n        fake_images = generator_cifar(noise)\n        d_output_fake = discriminator_cifar(fake_images.detach())\n        # Reshape output to match target shape\n        d_output_fake = d_output_fake.view(-1)  # [batch_size, 1, 1, 1] -> [batch_size]\n        errD_fake = criterion(d_output_fake, fake_labels)\n        errD_fake.backward()\n        \n        errD = errD_real + errD_fake\n        optimizer_D_cifar.step()\n\n        # --- Entraînement Générateur ---\n        optimizer_G_cifar.zero_grad()\n        d_output_on_fake = discriminator_cifar(fake_images)\n        # Reshape output to match target shape\n        d_output_on_fake = d_output_on_fake.view(-1)  # [batch_size, 1, 1, 1] -> [batch_size]\n        errG = criterion(d_output_on_fake, real_labels)\n        errG.backward()\n        optimizer_G_cifar.step()\n        \n        # Accumulation des métriques\n        epoch_metrics['loss_d'] += errD.item()\n        epoch_metrics['loss_g'] += errG.item()\n        epoch_metrics['loss_d_real'] += errD_real.item()\n        epoch_metrics['loss_d_fake'] += errD_fake.item()\n        \n        # Monitoring des ressources (toutes les 50 itérations)\n        if i % 50 == 0:\n            resources = monitor.monitor_resources()\n            \n        # Nettoyage mémoire périodique\n        if i % 100 == 0:\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            gc.collect()\n    \n    # Calcul des moyennes d'époque\n    num_batches = len(dataloader_cifar)\n    for key in epoch_metrics:\n        epoch_metrics[key] /= num_batches\n    \n    epoch_time = time.time() - epoch_start_time\n    epoch_times.append(epoch_time)\n    \n    # Évaluation périodique\n    if epoch % config.evaluation_frequency == 0 or epoch == epochs_finetune - 1:\n        # Évaluation complète avec métriques robustes\n        evaluation_metrics = monitor.comprehensive_evaluation(\n            generator_cifar, real_samples_for_metrics, epoch\n        )\n        \n        # Métriques d'entraînement\n        complete_metrics = {\n            **epoch_metrics,\n            **evaluation_metrics,\n            'epoch_time': epoch_time,\n            'lr_g': optimizer_G_cifar.param_groups[0]['lr'],\n            'lr_d': optimizer_D_cifar.param_groups[0]['lr']\n        }\n        \n        # Logging et sauvegarde\n        monitor.log_metrics(epoch, complete_metrics)\n        \n        # Sélection basée sur le score composite\n        composite_score = evaluation_metrics.get('composite_score', float('inf'))\n        is_best = composite_score < monitor.best_score\n        \n        monitor.save_model_checkpoint(\n            generator_cifar, discriminator_cifar, \n            epoch, complete_metrics, is_best\n        )\n        \n        # Early stopping\n        if monitor.check_early_stopping(composite_score):\n            print(f\"🛑 Early stopping at epoch {epoch}\")\n            break\n    \n    # Affichage des métriques\n    print(f\"[Epoch {epoch+1}/{epochs_finetune}] \"\n          f\"Loss_D: {epoch_metrics['loss_d']:.4f} \"\n          f\"Loss_G: {epoch_metrics['loss_g']:.4f} \"\n          f\"Time: {epoch_time:.1f}s\")\n\n# --- Analyse finale ---\ntraining_time = time.time() - training_start_time\n\nprint(f\"\\n🎯 Training completed in {training_time/3600:.2f} hours\")\nprint(f\"⚡ Average epoch time: {np.mean(epoch_times):.1f}s\")\n\n# Génération du rapport final\nfinal_report = {\n    'experiment_name': config.experiment_name,\n    'total_training_time_hours': training_time / 3600,\n    'total_epochs_trained': epoch + 1,\n    'best_composite_score': monitor.best_score,\n    'avg_epoch_time_seconds': np.mean(epoch_times),\n    'final_metrics': {k: v[-1] for k, v in monitor.metrics_history.items()},\n    'early_stopped': monitor.early_stopping_counter >= config.early_stopping_patience\n}\n\n# Sauvegarde du rapport\nreport_path = config.artifacts_path / \"final_report.json\"\nwith open(report_path, 'w') as f:\n    json.dump(final_report, f, indent=2, default=str)\n\nprint(f\"📋 Final report saved to: {report_path}\")\nprint(f\"🏆 Best model composite score: {monitor.best_score:.3f}\")\n\nprint(\"\\n--- FIN DE LA PHASE 2 INDUSTRIELLE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:24:12.248183Z","iopub.execute_input":"2025-06-10T12:24:12.248494Z","iopub.status.idle":"2025-06-10T13:20:13.245495Z","shell.execute_reply.started":"2025-06-10T12:24:12.248471Z","shell.execute_reply":"2025-06-10T13:20:13.244747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.utils import save_image, make_grid\nimport matplotlib.pyplot as plt\nimport json\nfrom pathlib import Path\n\n# Définition des classes (identiques à votre code original)\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(img_channels, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Fonction pour charger le meilleur modèle\ndef load_best_model(model_registry_path=\"model_registry\"):\n    \"\"\"Charge le meilleur modèle sauvegardé\"\"\"\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Chemin vers le meilleur modèle\n    #best_model_path = Path(model_registry_path) / \"best_model\"\n    best_model_path = Path(\"/kaggle/working/model_registry/best_model\")\n    if not best_model_path.exists():\n        raise FileNotFoundError(f\"Meilleur modèle non trouvé dans {best_model_path}\")\n    \n    # Chargement des métadonnées\n    with open(best_model_path / \"metadata.json\", 'r') as f:\n        metadata = json.load(f)\n    \n    print(f\"📋 Chargement du meilleur modèle:\")\n    print(f\"   Epoch: {metadata['epoch']}\")\n    print(f\"   Composite Score: {metadata['metrics']['composite_score']:.3f}\")\n    print(f\"   FID Score: {metadata['metrics']['fid_score']:.3f}\")\n    \n    # Paramètres du modèle\n    latent_dim = metadata['config']['latent_dim']\n    img_channels = metadata['config']['image_channels']\n    \n    # Initialisation des modèles\n    generator = Generator(latent_dim, img_channels).to(device)\n    discriminator = Discriminator(img_channels).to(device)\n    \n    # Chargement des poids\n    generator.load_state_dict(torch.load(best_model_path / \"generator.pth\", map_location=device))\n    discriminator.load_state_dict(torch.load(best_model_path / \"discriminator.pth\", map_location=device))\n    \n    # Mode évaluation\n    generator.eval()\n    discriminator.eval()\n    \n    print(\"✅ Meilleur modèle chargé avec succès!\")\n    \n    return generator, discriminator, metadata\n\n# Fonction pour générer des images\ndef generate_images(generator, num_images=16, latent_dim=100, save_path=\"generated_samples.png\"):\n    \"\"\"Génère des images avec le meilleur modèle\"\"\"\n    \n    device = next(generator.parameters()).device\n    \n    with torch.no_grad():\n        # Génération de bruit aléatoire\n        noise = torch.randn(num_images, latent_dim, 1, 1, device=device)\n        \n        # Génération d'images\n        fake_images = generator(noise)\n        \n        # Normalisation pour affichage [0,1]\n        fake_images = (fake_images + 1) / 2\n        \n        # Sauvegarde\n        save_image(fake_images, save_path, nrow=4, normalize=True)\n        \n        print(f\"💾 {num_images} images générées et sauvegardées dans {save_path}\")\n        \n        return fake_images\n\n# Exemple d'utilisation complète\ndef main():\n    \"\"\"Exemple d'utilisation du meilleur modèle\"\"\"\n    \n    try:\n        # 1. Chargement du meilleur modèle\n        generator, discriminator, metadata = load_best_model()\n        \n        # 2. Génération d'images\n        print(\"\\n🎨 Génération d'images...\")\n        generated_images = generate_images(\n            generator, \n            num_images=16, \n            latent_dim=100,\n            save_path=\"best_model_samples.png\"\n        )\n        \n        # 3. Affichage des métriques du modèle\n        print(f\"\\n📊 Métriques du meilleur modèle:\")\n        for metric, value in metadata['metrics'].items():\n            if isinstance(value, float):\n                print(f\"   {metric}: {value:.3f}\")\n        \n        # 4. Génération d'une grille personnalisée\n        print(\"\\n🖼 Création d'une grille personnalisée...\")\n        with torch.no_grad():\n            device = next(generator.parameters()).device\n            noise = torch.randn(25, 100, 1, 1, device=device)  # 5x5 grille\n            samples = generator(noise)\n            samples = (samples + 1) / 2  # Normalisation\n            \n            # Sauvegarde de la grille\n            grid = make_grid(samples, nrow=5, normalize=True, padding=2)\n            save_image(grid, \"custom_grid_5x5.png\")\n            print(\"💾 Grille 5x5 sauvegardée dans custom_grid_5x5.png\")\n        \n        # 5. Évaluation d'images réelles vs générées\n        print(\"\\n🔍 Test de discrimination...\")\n        with torch.no_grad():\n            # Images générées\n            fake_batch = generator(torch.randn(4, 100, 1, 1, device=device))\n            fake_scores = discriminator(fake_batch)\n            \n            print(f\"   Scores de discrimination (fake): {fake_scores.mean().item():.3f}\")\n            print(f\"   (Plus proche de 0.5 = meilleur équilibre G/D)\")\n        \n        return generator, discriminator\n        \n    except Exception as e:\n        print(f\"❌ Erreur lors du chargement: {e}\")\n        return None, None\n\n# Fonction pour comparer différents checkpoints\ndef compare_checkpoints(model_registry_path=\"model_registry\"):\n    \"\"\"Compare les métriques de différents checkpoints\"\"\"\n    \n    registry_path = Path(model_registry_path)\n    \n    if not registry_path.exists():\n        print(\"❌ Dossier model_registry non trouvé\")\n        return\n    \n    print(\"📊 Comparaison des checkpoints:\")\n    print(\"-\" * 60)\n    \n    checkpoints = []\n    \n    # Parcours des dossiers d'epochs\n    for epoch_dir in sorted(registry_path.glob(\"epoch_*\")):\n        metadata_file = epoch_dir / \"metadata.json\"\n        \n        if metadata_file.exists():\n            with open(metadata_file, 'r') as f:\n                metadata = json.load(f)\n            \n            epoch = metadata['epoch']\n            composite_score = metadata['metrics'].get('composite_score', 'N/A')\n            fid_score = metadata['metrics'].get('fid_score', 'N/A')\n            \n            checkpoints.append({\n                'epoch': epoch,\n                'composite_score': composite_score,\n                'fid_score': fid_score,\n                'path': epoch_dir\n            })\n    \n    # Affichage trié par composite score\n    checkpoints.sort(key=lambda x: x['composite_score'] if isinstance(x['composite_score'], float) else float('inf'))\n    \n    for i, cp in enumerate(checkpoints[:10]):  # Top 10\n        marker = \"🏆\" if i == 0 else f\"{i+1:2d}.\"\n        print(f\"{marker} Epoch {cp['epoch']:2d} | Composite: {cp['composite_score']:6.3f} | FID: {cp['fid_score']:6.3f}\")\n\nif __name__ == \"__main__\":\n    # Chargement et utilisation du meilleur modèle\n    generator, discriminator = main()\n    \n    # Comparaison des checkpoints\n    print(\"\\n\" + \"=\"*60)\n    compare_checkpoints()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:08:06.970925Z","iopub.execute_input":"2025-06-10T15:08:06.971457Z","iopub.status.idle":"2025-06-10T15:08:07.151165Z","shell.execute_reply.started":"2025-06-10T15:08:06.971431Z","shell.execute_reply":"2025-06-10T15:08:07.150451Z"}},"outputs":[{"name":"stdout","text":"📋 Chargement du meilleur modèle:\n   Epoch: 45\n   Composite Score: 71.944\n   FID Score: 71.944\n✅ Meilleur modèle chargé avec succès!\n\n🎨 Génération d'images...\n💾 16 images générées et sauvegardées dans best_model_samples.png\n\n📊 Métriques du meilleur modèle:\n   loss_d: 0.253\n   loss_g: 4.853\n   loss_d_real: 0.122\n   loss_d_fake: 0.131\n   fid_score: 71.944\n   diversity_score: 54.729\n   quality_score: 1.133\n   inference_time: 0.429\n   composite_score: 71.944\n   epoch_time: 47.342\n   lr_g: 0.000\n   lr_d: 0.000\n\n🖼 Création d'une grille personnalisée...\n💾 Grille 5x5 sauvegardée dans custom_grid_5x5.png\n\n🔍 Test de discrimination...\n   Scores de discrimination (fake): 0.026\n   (Plus proche de 0.5 = meilleur équilibre G/D)\n\n============================================================\n📊 Comparaison des checkpoints:\n------------------------------------------------------------\n🏆 Epoch 45 | Composite: 71.944 | FID: 71.944\n 2. Epoch 39 | Composite: 73.558 | FID: 73.558\n 3. Epoch 54 | Composite: 74.668 | FID: 74.668\n 4. Epoch 66 | Composite: 75.651 | FID: 75.651\n 5. Epoch 36 | Composite: 77.048 | FID: 77.048\n 6. Epoch 69 | Composite: 80.244 | FID: 80.244\n 7. Epoch 60 | Composite: 80.375 | FID: 80.375\n 8. Epoch 63 | Composite: 81.463 | FID: 81.463\n 9. Epoch 48 | Composite: 83.223 | FID: 83.223\n10. Epoch 30 | Composite: 83.663 | FID: 83.663\n","output_type":"stream"}],"execution_count":7}]}